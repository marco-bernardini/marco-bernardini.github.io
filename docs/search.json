[
  {
    "objectID": "notebooks/User Cohorts.html",
    "href": "notebooks/User Cohorts.html",
    "title": "User Cohorts Analysis",
    "section": "",
    "text": "Understanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention, engagement, and growth is through cohort analysis.\nA user cohort table segments users based on shared characteristics—such as sign-up month or first purchase—and tracks their activity across time. This approach helps businesses measure customer retention, assess the impact of product changes, and optimize marketing strategies.\nIn this notebook, we’ll walk through the process of analyzing a cohort table step by step. Using Python, we’ll transform raw data into actionable insights, visualizing retention curves and uncovering trends that can inform strategic decisions. Whether you’re working in SaaS, e-commerce, or mobile apps, this guide will equip you with the tools to extract meaningful patterns from user data.\nLet’s dive in.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\ncmap = plt.get_cmap('viridis')\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/User Cohorts.html#right-aligned-table",
    "href": "notebooks/User Cohorts.html#right-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Right-aligned table",
    "text": "Right-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active in the given month\nFor example, the number of users who made their first purchase in March and are still engaging with the platform in June are 9,468\n\nnc_cohorts_right = pd.read_csv('user_cohorts.csv')\nnc_cohorts_right.rename(columns={'Num Users': 'month'}, inplace=True)\nind = nc_cohorts_right['month']\nnc_cohorts_right.set_index('month', inplace=True)\n\nnc_cohorts_right_filled = nc_cohorts_right.fillna(0)\n\nstyled_df = nc_cohorts_right.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\nnan\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\n\n\nMar\nnan\nnan\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\n\n\nApr\nnan\nnan\nnan\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\n\n\nMay\nnan\nnan\nnan\nnan\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\n\n\nJun\nnan\nnan\nnan\nnan\nnan\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\n\n\nJul\nnan\nnan\nnan\nnan\nnan\nnan\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\n\n\nAug\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n109,117\n43,172\n35,336\n33,469\n30,535\n\n\nSep\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n116,187\n40,402\n35,154\n31,603\n\n\nOct\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n117,185\n34,462\n29,055\n\n\nNov\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n107,597\n32,920\n\n\nDec\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n97,164\n\n\n\n\n\n\nretained_clients = nc_cohorts_right.Dec.sum()\nprint(f\"Total reatined clients: {int(retained_clients)}\")\n\nTotal reatined clients: 288895\n\n\nWe can more intuitively visualize the table with a simple chart.\n\ncolors = cmap(np.linspace(0, 1, nc_cohorts_right.shape[1]))\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.stackplot(nc_cohorts_right_filled.index, nc_cohorts_right_filled, labels=nc_cohorts_right_filled.columns, colors=colors, alpha=0.8)\n\nax.legend(loc='upper left', title='Cohort')\nax.set_title('Users Per Month', loc='left')\nax.set_xlabel('Month')\nax.set_ylabel('Number of Users')\nax.grid(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, I noticed that the growth is slowing down. Remember, you always want to confirm your visual intuition with data: in this case, i approximated the growth curve with a known (continuous) function. As the function approximates well the curve, I can use its first derivative to draw conclusions about the instantaneous growth.\n\ntrend = nc_cohorts_right.sum()\nt = np.arange(1, len(trend) + 1)\n\ndef logistic_function(t, L, k, t0):\n    return L / (1 + np.exp(-k * (t - t0)))\n\n[L, k, t0], _ = curve_fit(logistic_function, t, trend, p0=[trend.max(), 1, 0])\n\napprox = logistic_function(t, L, k, t0)\nintensity = k * L * np.exp(-k * (t - t0)) / (1 + np.exp(-k * (t - t0)))**2\n\nR2 = 1 - np.sum((trend - approx) ** 2) / np.sum((trend - np.mean(trend)) ** 2)\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 6))\nax[0].plot(trend, label=\"Trend\")\nax[0].plot(approx, label=\"Logistic Fit\")\nax[0].set_title(f'Approximation of Growth with Logistic Function: R² = {round(R2, 3)}')\nax[0].legend()\n\nax[1].plot(intensity, color=\"red\", label=\"Derivative\")\nax[1].set_title(\"Derivative of Logistic Function (Growth Intensity)\")\nax[1].legend()\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/User Cohorts.html#left-aligned-table",
    "href": "notebooks/User Cohorts.html#left-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Left-aligned table",
    "text": "Left-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active after n months, where n is the column header. n=0 corresponds to the month in which the users made their first purchase\nThis table is very useful, as it allows to analyze the behaviour of different cohorts during their lifetime\n\nnc = nc_cohorts_right.copy().to_numpy()\nfor i, row in enumerate(nc):\n    a = np.argwhere(np.isnan(row))\n    if a.size != 0:\n        start = a.max() + 1\n        temp = row[start:]\n        if temp.size &lt; 12:\n            temp = np.append(temp, np.full(shape=(12 - temp.size), fill_value=np.nan))\n            nc[i] = temp\n    else:\n        pass\n    \nnc_cohorts_left = pd.DataFrame(nc, index=ind)\n\nstyled_df = nc_cohorts_left.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\nnan\n\n\nMar\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\nnan\nnan\n\n\nApr\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\nnan\nnan\nnan\n\n\nMay\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\nnan\nnan\nnan\nnan\n\n\nJun\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\nnan\nnan\nnan\nnan\nnan\n\n\nJul\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nAug\n109,117\n43,172\n35,336\n33,469\n30,535\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nSep\n116,187\n40,402\n35,154\n31,603\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nOct\n117,185\n34,462\n29,055\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nNov\n107,597\n32,920\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nDec\n97,164\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\nprint(f\"Total initial users: {int(nc_cohorts_left[0].sum())}\")\n\nTotal initial users: 765395\n\n\n\nAssessing retention rates\nWith a left-aligned table, we can analyse retention rates for every cohort. In this case, retention rates are calculated as: \\[\n    \\frac{Active\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nretention = nc_cohorts_left.copy()\n\nfor col in retention.columns:\n    if col == 0:\n        retention[col] = 1\n    else:\n        retention[col] = nc_cohorts_left[col]/nc_cohorts_left[0]\n\n\nstyled_df = retention.style.background_gradient(cmap=cmap, axis=1, vmax=.60, vmin=.30).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n100.00%\n60.14%\n55.07%\n49.64%\n50.36%\n52.54%\n50.00%\n50.72%\n50.36%\n49.28%\n49.64%\n48.91%\n\n\nFeb\n100.00%\n54.33%\n47.06%\n45.85%\n43.04%\n42.28%\n41.44%\n37.69%\n38.72%\n36.66%\n36.22%\nnan%\n\n\nMar\n100.00%\n55.07%\n49.26%\n47.24%\n43.94%\n43.32%\n40.04%\n38.86%\n37.44%\n35.21%\nnan%\nnan%\n\n\nApr\n100.00%\n52.29%\n47.72%\n44.09%\n41.57%\n39.16%\n36.92%\n36.29%\n33.03%\nnan%\nnan%\nnan%\n\n\nMay\n100.00%\n47.50%\n41.45%\n38.84%\n36.39%\n34.41%\n34.01%\n31.10%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n100.00%\n45.60%\n39.84%\n36.10%\n33.28%\n32.96%\n29.70%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n100.00%\n44.34%\n37.49%\n33.25%\n32.17%\n29.35%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n100.00%\n39.56%\n32.38%\n30.67%\n27.98%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n100.00%\n34.77%\n30.26%\n27.20%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n100.00%\n29.41%\n24.79%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n100.00%\n30.60%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n100.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nWe can then easily visualize the retention trend across different cohorts with a line chart. Ideally, retention rates should improve with time, although this is not the case.\n\nfig, ax = plt.subplots(1, 1)\nretention.T.plot(marker='s', ax=ax, color=colors, alpha=0.67)\n\nax.set_title('Hanging Ribbons', loc='left')\nax.set_ylabel('% Of Cohort Retained')\nax.set_xlabel('Time')\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\nax.legend(\n    title='Cohort',\n    loc='upper center',         \n    ncol=5,\n    frameon=True\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can easily check that retention is negatively correlated with time (and size of the cohorts)\n\nfrom sklearn.preprocessing import minmax_scale\nscaled_nc_cohorts_left = minmax_scale(range(1, len(nc_cohorts_left.index)+1), axis=0)\nscaled_retention = minmax_scale(retention.iloc[:, 1:4], axis=0)\n\ndf_scaled = pd.DataFrame(\n    scaled_nc_cohorts_left, columns=[\"nc_cohorts_left\"]\n).join(pd.DataFrame(scaled_retention, columns=[\"retention_1\", \"retention_2\", \"retention_3\"]))\n\ncorr_matrix = df_scaled.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=False)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nAssessing churn rates\nWith a left-aligned table, we can also analyse churn rates for every cohort. Churn rates in the following table are calculated as: \\[\n    \\frac{Churned\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nchurn = nc_cohorts_left.copy()\n\nfor col in churn.columns:\n    if col == 0:\n        churn[col] = 0\n    else:\n        churn[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])/nc_cohorts_left[col-1]\n\nstyled_df = churn.style.background_gradient(cmap=cmap, axis=1, vmax=.2, vmin=0).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n0.00%\n39.86%\n8.43%\n9.87%\n-1.46%\n-4.32%\n4.83%\n-1.45%\n0.71%\n2.16%\n-0.74%\n1.46%\n\n\nFeb\n0.00%\n45.67%\n13.38%\n2.56%\n6.13%\n1.76%\n2.00%\n9.04%\n-2.72%\n5.30%\n1.22%\nnan%\n\n\nMar\n0.00%\n44.93%\n10.56%\n4.10%\n6.98%\n1.42%\n7.56%\n2.95%\n3.65%\n5.95%\nnan%\nnan%\n\n\nApr\n0.00%\n47.71%\n8.73%\n7.60%\n5.72%\n5.81%\n5.70%\n1.72%\n8.99%\nnan%\nnan%\nnan%\n\n\nMay\n0.00%\n52.50%\n12.74%\n6.29%\n6.31%\n5.45%\n1.16%\n8.55%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n0.00%\n54.40%\n12.62%\n9.40%\n7.79%\n0.96%\n9.92%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n0.00%\n55.66%\n15.46%\n11.31%\n3.24%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n0.00%\n60.44%\n18.15%\n5.28%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n0.00%\n65.23%\n12.99%\n10.10%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n0.00%\n70.59%\n15.69%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n0.00%\n69.40%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n0.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nIt’s interesting to calculate and plot the weighted average churn rate for every month. In this case, most of the users disengage after the first month: this is because they were attracted with offers and promotions, and left the platforms as soon as they used those incentives.\n\nchurned = nc_cohorts_left.copy()\nretained = nc_cohorts_left.copy()\n\nfor col in churned.columns:\n    if col == 0:\n        churned[col] = 0\n    else:\n        churned[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])\n        \navg_churn = np.zeros(11)\n\nretained_sum = retained.iloc[:-1,:].sum()\nchurned_sum = churned.iloc[:-1,1:].sum()\n\nfor i in range(len(avg_churn)):\n    avg_churn[i] = churned_sum[i+1] / retained_sum[(i)]\n    \navg_churn\n\narray([0.61894928, 0.12467046, 0.0687431 , 0.05115784, 0.03639244,\n       0.04167227, 0.03471718, 0.0376834 , 0.02842538, 0.00237718,\n       0.00210748])\n\n\n\nstd = churn.iloc[:-1,1:].std()\n\nfig, ax = plt.subplots(1, 1,figsize=(8,5))\nax.plot(range(1, 12, 1), avg_churn, label='Mean', color='#00A082')\n\nplt.fill_between(range(1, 12, 1), avg_churn - std, avg_churn + std, alpha=0.2, label='±1 Std Dev', color='#00A082')\n\nplt.xlabel('Month')\nplt.ylabel('% of churned customers (relative to prev. month)')\nplt.title('Churned customers in lifetime month', loc='left')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\nI then wanted to see how many additional clients could’ve been retained at the end of the year by reducing the churn rate by 5 percentage points in the first month\n\navg_churn_proj = avg_churn.copy()\navg_churn_proj[0] = avg_churn_proj[0]-.05\nstarting_users = nc_cohorts_left[0].values\nprojection = np.zeros([12,12])\nprojection[:,0] = starting_users\nfor i in range(11):\n    projection[:(11-i),i+1] = projection[:(11-i),i] * (1-avg_churn_proj[i])\n    \ntotal_clients = 0\nfor i in range(len(projection)):\n    total_clients += projection[i, 11-i]\n    \nprint(f\"Additional clients: {int(total_clients - retained_clients)}\")\n\nAdditional clients: 42441.379632713506\n\n\nTo gauge the average time clients who get past the first month get engaged with the service, it is userful to calculate the (truncated) lifespan of the users. This is simply calculated as: \\[\n\\frac{1}{avg\\, churn\\, rate}\n\\]\n\n1/((nc_cohorts_left.sum()[1:-1]*avg_churn[1:]).sum()/nc_cohorts_left.sum()[1:-1].sum())\n\n14.111089070031147"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html",
    "href": "notebooks/La curva di Phillips.html",
    "title": "La curva di Phillips",
    "section": "",
    "text": "Nel finire degli anni 50 l’economista della LSE A.W.Phillips osservò una relazione inversa fra disoccupazione e inflazione salariale, ovvero il tasso di crescita dei salari nominali, nell’economia britannica. Pochi anni dopo, Samuelson e Solow mostrarono che una simile relazione esisteva anche nell’economia americana, legando non più la disoccupazione solamente all’inflazione salariale, ma al tasso di crescita generale dei prezzi. La curva divenne così uno strumento utilizzato dai policy makers che, dopo aver valutato il trade-off tra inflazione e disoccupazione, mettevano in atto strategie atte a raggiungere un punto specifico della curva. In questa sezione, replicheremo l’articolo originale di Phillips, utilizzando i dati forniti dalla Bank Of England (https://www.bankofengland.co.uk/statistics/research-datasets), e cercheremo di analizzare il fenomeno all’iterno dell’economia Americana, per la quale i dati sono purtroppo frammentari (https://data.bls.gov/pdq/SurveyOutputServlet e https://fred.stlouisfed.org/series/M08142USM055NNBR)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport warnings\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n#     \"axes.grid\": False,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "href": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "title": "La curva di Phillips",
    "section": "Paper di Phillips",
    "text": "Paper di Phillips\n\n# Importa i dati sulla disoccupazione dal file Excel contenente dati macroeconomici del Regno Unito\nunemployment = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                             sheet_name='A50. Employment & unemployment', \n                             skiprows=2, usecols='A,J')\n\n# Rimuove le prime due righe che contengono informazioni non necessarie\nunemployment = unemployment[2:]\n\n# Rinomina la colonna non etichettata in 'Year' per maggiore chiarezza\nunemployment.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n\n# Imposta l'anno come indice del DataFrame\nunemployment.set_index('Year', inplace=True)\n\n# Converte la colonna del tasso di disoccupazione in formato numerico (float)\nunemployment['Unemployment rate'] = unemployment['Unemployment rate'].astype(float)\n\n# Mostra le prime righe del DataFrame per verificare la corretta importazione dei dati\nunemployment.head()\n\n\n\n\n\n\n\n\nUnemployment rate\n\n\nYear\n\n\n\n\n\n1855\n3.731877\n\n\n1856\n3.518180\n\n\n1857\n3.945176\n\n\n1858\n5.215345\n\n\n1859\n3.276380\n\n\n\n\n\n\n\n\n# Importa i dati sui salari e l'inflazione dal file Excel contenente dati macroeconomici del Regno Unito\nwages_inflation = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                                sheet_name='A47. Wages and prices', \n                                usecols='A,BD')\n\n# Rinomina le colonne per una maggiore chiarezza\nwages_inflation.columns = ['Year', 'Wage index']\n\n# Rimuove le prime sei righe non necessarie\nwages_inflation = wages_inflation[6:]\n\n# Converte la colonna 'Year' in formato intero\nwages_inflation['Year'] = wages_inflation['Year'].astype(int)\n\n# Converte la colonna 'Wage index' in formato numerico (float)\nwages_inflation['Wage index'] = wages_inflation['Wage index'].astype(float)\n\n# Filtra i dati per includere solo gli anni a partire dal 1855\nwages_inflation = wages_inflation[wages_inflation['Year'] &gt;= 1855]\n\n# Imposta l'anno come indice del DataFrame\nwages_inflation.set_index('Year', inplace=True)\n\n# Calcola la differenza centrale prima (approssimazione numerica della derivata prima) e la esprime in percentuale.\n# Questa è la misura utilizzata nell'articolo originale, qui utilizzeremo invece la variazione percentuale\nwages_inflation['First central difference'] = (\n    (wages_inflation['Wage index'].shift(-1) - wages_inflation['Wage index'].shift(1)) / \n    (2 * wages_inflation['Wage index'])\n) * 100\n\n# Calcola la variazione percentuale anno su anno del Wage index\nwages_inflation['Percent change'] = wages_inflation['Wage index'].pct_change() * 100\n\n# Mostra le prime righe del DataFrame per verificare la corretta elaborazione dei dati\nwages_inflation.head()\n\n\n\n\n\n\n\n\nWage index\nFirst central difference\nPercent change\n\n\nYear\n\n\n\n\n\n\n\n1855\n59.0\nNaN\nNaN\n\n\n1856\n59.0\n-1.694915\n0.000000\n\n\n1857\n57.0\n-2.631579\n-3.389831\n\n\n1858\n56.0\n0.000000\n-1.754386\n\n\n1859\n57.0\n1.754386\n1.785714\n\n\n\n\n\n\n\n\n# Unisce i DataFrame 'unemployment' e 'wages_inflation' utilizzando l'indice (Year) e rimuove eventuali valori NaN\ndf = pd.merge(unemployment, wages_inflation, right_index=True, left_index=True).dropna()\n\n# Suddivide il dataset in tre periodi storici distinti per analizzare la relazione tra disoccupazione e inflazione salariale\ndfA = df[df.index &lt;= 1913].copy()       # Periodo pre-1913\ndfB = df[(df.index &gt;= 1913) & (df.index &lt;= 1948)].copy()  # 1913-1948 (tra le due guerre)\ndfC = df[(df.index &gt;= 1948) & (df.index &lt;= 1957)].copy()  # 1948-1957 (primi anni post-bellici)\n\n# Definisce le etichette temporali per i grafici\ndates = ['1861-1913', '1913-1948', '1948-1957']\n\n# Crea una figura con tre subplot affiancati per rappresentare ciascun periodo\nfig, ax = plt.subplots(1, 3, figsize=(10, 3.5))\n\n# Itera sui tre sotto-dataset per stimare e tracciare la curva di Phillips per ciascun periodo\nfor i, _ in enumerate([dfA, dfB, dfC]):\n    \n    # Calcola il termine di aggiustamento per evitare problemi con il logaritmo\n    a = abs(_[\"Percent change\"].min()) + 0.9\n    \n    # Trasforma in logaritmo il tasso di inflazione salariale e il tasso di disoccupazione\n    _[\"Log wage inflation rate\"] = np.log(_[\"Percent change\"] + a)\n    _[\"Log unemployment rate\"] = np.log(_[\"Unemployment rate\"])\n    \n    # Definisce le variabili per la regressione lineare (modello di Phillips)\n    X = _[\"Log unemployment rate\"]\n    Y = _[\"Log wage inflation rate\"]\n    X = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\n    model = sm.OLS(Y, X).fit()  # Stima il modello di regressione\n    R2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n    \n    # Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\n    _[\"Predicted log wage inflation rate\"] = model.predict(X)\n    _[\"Predicted wage inflation rate\"] = np.exp(_[\"Predicted log wage inflation rate\"]) - a\n    \n    # Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\n    sorted_data = _.sort_values(by='Unemployment rate')\n    \n    # Disegna una linea orizzontale per rappresentare l'asse y=0\n    ax[i].axhline(y=0, color='silver', linewidth=2)\n    \n    # Crea uno scatter plot con i dati osservati\n    ax[i].scatter(_['Unemployment rate'], _['Percent change'], alpha=0.7, \n                  label=\"Observations\", color=bmh_colors[0])\n    \n    # Disegna la curva di Phillips stimata dalla regressione\n    ax[i].plot(sorted_data['Unemployment rate'], sorted_data['Predicted wage inflation rate'], \n               color=bmh_colors[1], label=\"Phillips Curve\")\n    \n    # Imposta il titolo per ciascun sottografico\n    ax[i].set_title(f\"UK, {dates[i]}\", loc='left')\n    \n    # Etichette degli assi\n    ax[i].set_xlabel('Unemployment Rate')\n    ax[i].set_ylabel('Wage Inflation Rate')\n    \n    # Attiva la griglia\n    ax[i].grid(True)\n    \n    # Aggiunge la legenda\n    ax[i].legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "href": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "title": "La curva di Phillips",
    "section": "Paper di Samuelson e Solow",
    "text": "Paper di Samuelson e Solow\n\n# Carica i dati sulla disoccupazione (BLS) dal file Excel, saltando le prime 16 righe e impostando 'Year' come indice\nunemp = pd.read_excel('BLS1.xlsx', skiprows=16).set_index('Year')\n\n# Carica i dati sulle retribuzioni orarie medie da 25 industrie manifatturiere degli Stati Uniti (NBER)\nearnings = pd.read_csv('Average Hourly Earnings, Twenty-Five Manufacturing Industries for United States.csv').set_index('observation_date')\n\n# Converte l'indice di 'earnings' in formato datetime (anno-mese-giorno)\nearnings.index = pd.to_datetime(earnings.index, format='%Y-%m-%d')\n\n# Raggruppa i dati sulle retribuzioni annualmente, calcolando la media per ogni anno\nearnings = earnings.groupby(pd.Grouper(freq='Y')).mean()\n\n# Estrae solo l'anno dall'indice per facilitarne l'elaborazione\nearnings.index = earnings.index.year\n\n# Filtra i dati per gli anni dal 1929 al 1947\nearnings = earnings[(earnings.index &gt;= 1929) & (earnings.index &lt;= 1947)]\n\n# Rinomina la colonna per avere una denominazione chiara\nearnings.columns = ['HOURLY_WAGE']\n\n# Rinomina la colonna della disoccupazione per chiarezza\nunemp.columns = ['UNEMPLOYMENT']\n\n# Unisce i due DataFrame sui rispettivi indici (anno)\ndf = pd.merge(unemp, earnings, right_index=True, left_index=True)\n\n# Calcola la variazione percentuale annuale delle retribuzioni orarie\ndf['PCT_CHANGE'] = df['HOURLY_WAGE'].pct_change() * 100\n\n# Rimuove le righe con valori mancanti (NaN) creati durante il calcolo della variazione percentuale\ndf = df.dropna()\n\n# Mostra le prime righe del DataFrame per verificare i dati\ndf.head()\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n\n\n\n\n\n\n\n\n\nUNEMPLOYMENT\nHOURLY_WAGE\nPCT_CHANGE\n\n\nYear\n\n\n\n\n\n\n\n1930\n8.7\n0.589000\n-0.113058\n\n\n1931\n15.9\n0.563833\n-4.272779\n\n\n1932\n23.6\n0.497583\n-11.749926\n\n\n1933\n24.9\n0.490583\n-1.406800\n\n\n1934\n21.7\n0.579750\n18.175641\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"PCT_CHANGE\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"PCT_CHANGE\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"UNEMPLOYMENT\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='UNEMPLOYMENT')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['UNEMPLOYMENT'], df['PCT_CHANGE'], alpha=0.7, label=\"Observations\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['UNEMPLOYMENT'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Phillips Curve\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"US, 1930-1947\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Unemployment Rate')\nax.set_ylabel('Wage Inflation Rate')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#e-oggi",
    "href": "notebooks/La curva di Phillips.html#e-oggi",
    "title": "La curva di Phillips",
    "section": "E oggi?",
    "text": "E oggi?\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"Estrae i dati dal datawarehouse della BCE\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'  # URL di base per il servizio web della BCE\n    format_ = '?format=csvdata'  # Impostazione del formato dei dati in CSV\n    df = pd.read_csv(url_+key+format_)  # Legge i dati CSV dal servizio web BCE usando la chiave\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]  # Seleziona le colonne di interesse (periodo e valore osservato)\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n    df = df.set_index('TIME_PERIOD')  # Imposta il periodo come indice del dataframe\n    df.columns = [obs_name]  # Rinomina la colonna con il nome della variabile\n    return df\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Chiave per la wage inflation (inflazione salariale)\nwage_inflation_key = 'MNA/Q.Y.I9.W2.S1.S1._Z.COM_PS._Z._T._Z.IX.V.N'  # Codice identificativo del salario per addetto (indice)\nwage_inflation = make_df_ECB(wage_inflation_key, 'Compensation per employee')  # Ottieni i dati dal database ECB\n\nwage_inflation = wage_inflation[wage_inflation.index &gt;= '2000-01-01']  # Filtra i dati dal 2000 in poi\n\n# Combina i dati di disoccupazione e salario in un unico dataframe\ndf = pd.merge(unemp_rate, wage_inflation, right_index=True, left_index=True)\n\n# Raggruppa i dati per anno e calcola la media per il tasso di disoccupazione e l'ultimo valore per la compensazione per addetto\ndf = df.groupby(pd.Grouper(freq='Y'))[['Unemployment Rate', 'Compensation per employee']].agg({'Unemployment Rate':'mean',\n                                                                                             'Compensation per employee':'last'})\n\ndf.index = df.index.year  # Imposta l'indice come anno\ndf['Wage inflation rate'] = df['Compensation per employee'].pct_change()*100  # Calcola il tasso di inflazione salariale come variazione percentuale\ndf = df.dropna()  # Rimuove i valori mancanti\n\ndf.head()  # Mostra le prime righe del dataframe\n\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_21508\\1025282998.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_21508\\1025282998.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n\n\n\n\n\n\n\n\n\nUnemployment Rate\nCompensation per employee\nWage inflation rate\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n2001\n8.508061\n70.332653\n2.705440\n\n\n2002\n8.753020\n72.127430\n2.551840\n\n\n2003\n9.188271\n73.757671\n2.260224\n\n\n2004\n9.399281\n75.327356\n2.128164\n\n\n2005\n9.231180\n77.193789\n2.477762\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"Wage inflation rate\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"Wage inflation rate\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"Unemployment Rate\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='Unemployment Rate')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['Unemployment Rate'], df['Wage inflation rate'], alpha=0.7, label=\"Observations\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['Unemployment Rate'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Phillips Curve\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"EU, 2001-2024\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Unemployment Rate')\nax.set_ylabel('Wage Inflation Rate')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Marco Bernardini",
    "section": "Education",
    "text": "Education\nUniversità Bicocca, Milano (IT)\nBSc in Economics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Marco Bernardini",
    "section": "Experience",
    "text": "Experience\nSDG Group, Milano (IT)\nConsultant\nLeonardo Assicurazioni, Milano (IT)\nLead Data Analyst"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\nLeonardo Assicurazioni, Milan\nLeonardo Assicurazioni is a leading insurance distributor with a robust network of over 400 consultants based in Milan. After three years in technical roles, I founded and led the company’s data analytics office for five years, driving key insights and innovations.\n\nData Analyst\n\nLed and trained a team of three, ensuring the successful delivery of key data projects that enhanced business operations and decision-making.\nConsolidated and structured raw data from multiple sources and formats (SQL, JSON, legacy systems).\nIncreased revenue by developing unsupervised clustering models for client segmentation and supervised classification models (KNN, Decision Tree, Logistic Regression) to optimize sales campaign efficiency.\nReduced costs by building classification models to prevent churn and insolvencies and collaborating with the quality control department to develop fraud-prevention models.\nWorked with HR to design and analyze employee surveys, extracting insights and developing predictive models to reduce turnover.\nConducted in-depth sales data analysis using Python, generating actionable insights and presenting findings with Matplotlib visualizations.\nBuilt over 50 Power BI dashboards from scratch in close collaboration with top management to support decision-making.\nDeveloped interactive web-based data visualizations with D3.js, allowing stakeholders to explore complex datasets intuitively on portable devices.\nCreated structured reports and presentations in Excel and PowerPoint to effectively communicate insights to senior leadership.\nServed as a translator in key meetings with foreign investors and presented at large conferences.\nOrganized and monitored sales competitions in close collaboration with the sales director.\nAutomated key processes by working with the IT team to develop custom VBA applications, significantly reducing manual effort and improving efficiency.\n\n\n\nUnderwriter and Sales Support Specialist\n\nManaged back-office and control activities, ensuring efficient operations\nStreamlined administrative workflows, reducing processing times and improving efficiency\nGained deep knowledge of insurance products and the market through one year of experience in the underwriting office."
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "CV",
    "section": "Certificates",
    "text": "Certificates\n\nGRE\n\n\n\nOverall 336\n\n\n\n\nQuant 170\n\n\n\n\nVerbal 166\n\n\n\n\n\nIELTS\n\n\n\nOverall 8/9\n\n\n\n\nReading 9/9\n\n\n\n\nListening 9/9\n\n\n\n\nWriting 7.5/9\n\n\n\n\nSpeaking 7/9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "&lt;h1&gt;Your Name&lt;/h1&gt;\n&lt;p&gt;I'm a macroeconomics researcher with a passion for data, finance, and programming. Currently pursuing my MSc at LSE, I explore economic crises, nonlinear models, and financial stability. I also enjoy endurance sports and mountaineering.&lt;/p&gt;"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This section provides a collection of resources on data analysis, including notebooks and practical insights drawn from my professional experience."
  },
  {
    "objectID": "data-analysis.html#marketing",
    "href": "data-analysis.html#marketing",
    "title": "Data Analysis",
    "section": "Marketing",
    "text": "Marketing\n\nUsers Cohorts Analysis\nUnderstanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention engagement, and growth is through cohort analysis."
  },
  {
    "objectID": "macroeconomics.html",
    "href": "macroeconomics.html",
    "title": "Macroeconomics",
    "section": "",
    "text": "This section contains various resources related to macroeconomics and econometrics, including articles, research, interactive notebooks and other useful resources."
  },
  {
    "objectID": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "href": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "title": "Macroeconomics",
    "section": "Corso introduttivo di Macroeconomia con Python",
    "text": "Corso introduttivo di Macroeconomia con Python\n\nLe variabili macroeconomiche\nA notebook (in Italian) exploring macroeconomic variables and their applications.\nLa curva di Phillips\nA notebook (in Italian) exploring one of the most relevant relationships in macroeconomics"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html",
    "href": "notebooks/Le variabili macroeconomiche.html",
    "title": "Le variabili macroeconomiche",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport eurostat\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n#     \"axes.grid\": False,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"extracts data from the BCE datawarehouse\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'\n    format_ = '?format=csvdata'\n    df = pd.read_csv(url_+key+format_)\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n    df = df.set_index('TIME_PERIOD')\n    df.columns = [obs_name]\n    return df"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "href": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "title": "Le variabili macroeconomiche",
    "section": "Produzione Aggregata",
    "text": "Produzione Aggregata\n\nDefinizione\nLa misura della produzione aggregata nella contabilità nazionale è chiamata prodotto interno lordo, o PIL. Il PIL può essere definito in tre modi equivalenti:\n\n\nIl PIL è il valore dei beni e dei servizi finali prodotti nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma del valore aggiunto nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma dei redditi dell’economia in un dato periodo di tempo\n\n\nDalla definizione (3) deriva che, in una economia, produzione aggregata e reddito aggregato siano sempre uguali\n\n\nCome si misura il PIL?\nNormalmente, le informazioni sono raccolte dalle autorità fiscali di un paese:\n\n\nLe imprese registrano le proprie vendite\n\n\nLe imprese pagano tasse sul valore aggiunto\n\n\nGli individui dichiarano il reddito percepito\n\n\n\n\nPIL nominale e PIL reale\nIl PIL nominale è la somma del valore dei beni finali valutati al loro prezzo corrente, dunque la sua crescita è influenzata da I. l’aumento della produzione II. l’aumento dei prezzi. Il PIl reale è la somma del valore dei beni finali valutati a prezzi costanti: di solito si utilizzano i prezzi di un anno di riferimento\n\n\nPIL pro capite\nSpesso, per misurare il tenore di vita in un paese, si utilizza il PIL pro capite, ovvero il PIL reale diviso per la popolazione del paese\n\n# Creazione del DataFrame con i dati del PIL dell'Area Euro\n# La chiave specificata corrisponde al PIL reale trimestrale in euro a prezzi concatenati\nPIL_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'\npil = make_df_ECB(PIL_KEY, 'PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie utilizzata rappresenta il PIL reale dell'Area Euro con dati trimestrali (Q).\nI valori sono espressi in miliardi di euro (EUR) a prezzi concatenati (LR), il che significa\nche sono corretti per l'inflazione e consentono un confronto diretto nel tempo.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse: \n  - Crisi del debito europeo (2009) con una significativa contrazione del PIL.\n  - Pandemia di Covid-19 (2020) con una forte caduta seguita da una ripresa.\n\"\"\"\n\n# Creazione del grafico del PIL\npil.plot()\nplt.title('PIL Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('PIL (Miliardi di euro, prezzi concatenati)')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', pil.loc['2009-01-01']), \n             xytext=(-20, 60),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', pil.loc['2020-04-01']), \n             xytext=(-40, -40),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTasso di crescita del PIL\nSpesso, si utilizza il concetto di crescita del PIL per indicare espansioni (periodi di crescita positiva) e recessioni (almeno due trimestri consecutivi di crescita negativa). Il tasso di crescita del PIL è semplicemente definito come: $ Y_t - Y_{t-1} $\n\n# Creazione del DataFrame con i dati della crescita del PIL reale\n# La chiave specificata corrisponde alla crescita del PIL reale trimestrale in termini percentuali\nREAL_GDP_GROWTH_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.GY'\nreal_gdp_growth = make_df_ECB(REAL_GDP_GROWTH_KEY, 'Crescita PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie rappresenta la crescita percentuale del PIL reale dell'Area Euro con dati trimestrali (Q).\nMisura il tasso di variazione del PIL rispetto al trimestre precedente, esprimendolo in termini percentuali.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Indicatore: Crescita percentuale del PIL reale (GY)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse:\n  - Crisi del debito europeo (2009) con crescita negativa e recessione prolungata.\n  - Pandemia di Covid-19 (2020) con una caduta storica seguita da un rimbalzo.\n\"\"\"\n\n# Creazione del grafico della crescita del PIL\nreal_gdp_growth.plot()\nplt.title('Crescita del PIL nell\\'Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('Crescita %')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Aggiunta di una linea orizzontale per evidenziare il valore zero (assenza di crescita)\nplt.axhline(y=0, color='#525252', linestyle='--', linewidth=1)\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', real_gdp_growth.loc['2009-01-01']), \n             xytext=(-100, -30),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', real_gdp_growth.loc['2020-04-01']), \n             xytext=(-150, 10),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLa composizione del PIL\nPer comprendere cosa determini la domanda di beni, si può scomporre la produzione aggregata in base alla tipologia di acquirente dei beni. Si possono individuare cinque componenti:\n\n\nIl consumo (C), ovvero beni e servizi acquistati dai consumatori, ovvero le famiglie\n\n\nL’investimento (I) che include l’investimento non residenziale (impianti e macchinari) da parte delle aziende, e l’investimento residenziale da parte delle famiglie\n\n\nLa spesa pubblica in beni e servizi (G), ovvero i beni acquistati dallo stato e dagli enti pubblici. Si noti che non fanno parte di questa categoria pensioni, servizi assistenziali quali il reddito di cittadinanza, o gli interessi pagati sul debito.\n\n\nImportazioni (IM), che entrano nell’equazione con segno negativo\n\n\nEsportazioni (X), che entrano nell’equazione con segno positivo\n\n\nSi ha dunque che: \\(Y = C + I + G + (X - IM)\\) Dove spesso si usa parlare di esportazioni nette o saldo commerciale del termine \\(X - IM\\).\n\n# Definisco le chiavi per i vari indicatori economici relativi all'Italia\n# La struttura di ogni chiave segue il formato: \n# MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N\n\nIT_gdp_key = 'MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N'  # PIL (Prodotto Interno Lordo)\n\nIT_inv_key = 'MNA/Q.Y.IT.W0.S1.S1.D.P51G.N11G._T._Z.EUR.V.N'  # Investimenti\n\nIT_cons_key = 'MNA/Q.Y.IT.W0.S1M.S1.D.P31._Z._Z._T.EUR.V.N'  # Consumi\n\nIT_gov_exp_key = 'MNA/Q.Y.IT.W0.S13.S1.D.P3._Z._Z._T.EUR.V.N'  # Spesa Pubblica\n\nIT_imp_key = 'MNA/Q.Y.IT.W1.S1.S1.C.P7._Z._Z._Z.EUR.V.N'  # Importazioni\n\nIT_exp_key = 'MNA/Q.Y.IT.W1.S1.S1.D.P6._Z._Z._Z.EUR.V.N'  # Esportazioni\n\n# Creo i DataFrame per ogni indicatore economico utilizzando le chiavi\nIT_gdp = make_df_ECB(IT_gdp_key, 'GDP')  # Prodotto Interno Lordo (PIL)\nIT_inv = make_df_ECB(IT_inv_key, 'INV')  # Investimenti\nIT_cons = make_df_ECB(IT_cons_key, 'CONS')  # Consumi\nIT_gov_exp = make_df_ECB(IT_gov_exp_key, 'GOV_EXP')  # Spesa Pubblica\nIT_imp = make_df_ECB(IT_imp_key, 'IMP')  # Importazioni\nIT_exp = make_df_ECB(IT_exp_key, 'EXP')  # Esportazioni\n\n# Unisco tutti i DataFrame in uno solo per l'analisi aggregata delle spese\nimport functools as ft\ndfs = [IT_gdp, IT_inv, IT_cons, IT_gov_exp, IT_imp, IT_exp]\nIT_expenditures = ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), dfs)\n\n# Calcolo la bilancia commerciale (BOT) come la differenza tra esportazioni e importazioni\nIT_expenditures['BOT'] = IT_expenditures['EXP'] - IT_expenditures['IMP']\n\n# Creo un grafico a barre impilate per visualizzare la spesa aggregata\nax = IT_expenditures[['INV', 'CONS', 'GOV_EXP', 'BOT']][-32:].plot.bar(stacked=True, figsize=(9, 5))\n\n# Modifico le etichette dell'asse x per visualizzare l'anno e il trimestre\nax.set_xticklabels([f'{x.year} Q{x.quarter}' for x in IT_expenditures.index[-32:]])\n\n# Aggiungo il titolo e le etichette degli assi\nplt.title('Spesa Aggregata - IT')\nplt.xlabel('Trimestre')\nplt.ylabel('Spesa')\n\n# Aggiungo una legenda con le categorie di spesa\nax.legend(['I', 'C', 'G', 'X-IM', 'Y'],\n          title='Categorie',\n          loc='upper left',      \n          frameon=True,\n          fontsize='x-small',\n          ncol=4\n          )\n\n# Adatto il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPIL e PNL\nIn alcuni casi, è utile distinguere tra: PIL, che misura il valore totale dei beni e servizi prodotti all’interno di un paese, indipendentemente dalla proprietà delle risorse e PNL (Prodotto Nazionale Lordo), che misura il valore totale dei beni e servizi prodotti da cittadini di un paese, indipendentemente da dove si trovino nel mondo. Sono emblematici il caso tedesco e quello irlandese.\n\n# Definiamo le colonne di interesse per i dati\ncols = ['geo\\TIME_PERIOD', '2020', '2021', '2022', '2023']\n\n# Otteniamo i dati del Prodotto Nazionale Lordo (PNL) pro capite\ngni_pc_tuples = eurostat.get_data('nama_10_pp')  # Recupera i dati dal database Eurostat\ngni_pc = pd.DataFrame(gni_pc_tuples[1:], columns=gni_pc_tuples[0])  # Crea il DataFrame\ngni_pc = gni_pc[cols]  # Seleziona solo le colonne di interesse\ngni_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne per chiarezza\n\n# Otteniamo i dati del Prodotto Interno Lordo (PIL) pro capite\ngdp_pc_tuples = eurostat.get_data('nama_10_pc')  # Recupera i dati del PIL pro capite\ngdp_pc = pd.DataFrame(gdp_pc_tuples[1:], columns=gdp_pc_tuples[0])  # Crea il DataFrame\ngdp_pc = gdp_pc.query(\"unit=='CP_PPS_EU27_2020_HAB' & na_item == 'B1GQ'\")  # Filtro per unità e tipo di dato\ngdp_pc = gdp_pc[cols]  # Seleziona solo le colonne di interesse\ngdp_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne\n\n# Definiamo la lista dei paesi per i quali vogliamo visualizzare i dati\ncountries = ['IT', 'DE', 'IE']\n\n# Creiamo un grafico a sottotrame per visualizzare i dati\nfig, axs = plt.subplots(1, 3, sharey=True, figsize=(9, 4))  # Crea 3 sottotrame affiancate\naxs = axs.flatten()  # Appiattisce l'array di assi per facilitare l'accesso\n\n# Ciclo sui paesi per plottare i dati di ciascuno\nfor n, country in enumerate(countries):\n    # Selezioniamo i dati del paese corrente per PNL e PIL\n    gni_pc_country = gni_pc[gni_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PNL\n    gdp_pc_country = gdp_pc[gdp_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PIL\n    \n    # Plottiamo i dati sulla stessa sottotrama\n    axs[n].plot(gni_pc_country, label='PNL pro capite')  # PNL\n    axs[n].plot(gdp_pc_country, label='PIL pro capite')  # PIL\n    \n    # Aggiungiamo il titolo e le etichette\n    axs[n].set_title(country, loc='left')  # Titolo a sinistra per ogni paese\n    axs[n].set_xlabel('Anno')  # Etichetta asse X\n    axs[n].set_ylabel('Valore')  # Etichetta asse Y\n\n    # Aggiungiamo la legenda\n    axs[n].legend(\n        loc='upper left',  # Posizione della legenda\n        frameon=True,  # Abilita il frame della legenda\n        fontsize='x-small'  # Font di dimensioni piccole\n    )\n\n# Titolo principale del grafico\nfig.suptitle('PIL e PNL pro capite in Italia, Germania, Irlanda')\n\n# Miglioriamo il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Mostriamo il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di disoccupazione",
    "text": "Tasso di disoccupazione\nIl tasso di disoccupazione si calcola come rapporto tra disoccupati (ovvero coloro i quali non hanno un lavoro, ma lo stanno cercando) e la forza lavoro (data dalla somma di occupati e disoccupati). A questo proposito, è importante sottolineare come chi non ha un lavoro, ma non ne sta nemmeno cercando uno, sia considerato fuori dalla forza lavoro, e dunque non incluso nel conteggio dei disoccupati. Si può quindi calcolare il tasso di partecipazione come forza lavoro divisa per il totale della popolazione.\n\nCome si misura il tasso di disoccupazione?\nGeneralmente, si effettuano indagini a campione sulle famiglie (Labour Force Surveys)\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo un grafico con due assi\nfig, ax1 = plt.subplots()  # Crea una figura e un asse\n\n# Plot del tasso di disoccupazione\nax1.plot(unemp_rate)  # Traccia il grafico del tasso di disoccupazione\nax1.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax1.set_ylabel('Tasso di Disoccupazione', color=bmh_colors[0])  # Etichetta per l'asse Y con colore personalizzato\n\n# Creiamo un secondo asse per la crescita del PIL\nax2 = ax1.twinx()  # Crea un secondo asse Y condiviso sull'asse X\nax2.plot(real_gdp_growth[real_gdp_growth.index &gt;= unemp_rate.index.min()], color='#525252', linestyle=':')  # Traccia il grafico della crescita del PIL con linea tratteggiata\nax2.set_ylabel('Crescita del PIL')  # Etichetta per l'asse Y della crescita del PIL\nax2.grid(False)  # Disabilita la griglia per il secondo asse\n\n# Titolo del grafico\nplt.title('Disoccupazione e crescita del PIL nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di Inflazione",
    "text": "Tasso di Inflazione\nL’inflazione rappresenta un aumento sostenuto del livello generale dei prezzi. Il tasso di inflazione è il tasso a cui il livello dei prezzi aumenta nel tempo.\n\nCome si misura l’inflazione?\nIl livello dei prezzi si può calcolare in due modi:\n\n\nUtilizzando il deflatore del PIL, ovvero il rapporto tra il PIL nominale e il PIL reale nell’anno di interesse\n\n\nLindice armonizzato dei prezzi al consumo, ovvero un numero indice che fissa un prezzo pari a 100 nell’anno base\n\n\nChiamando \\(P_t\\) il livello generale dei prezzi nell’anno \\(t\\), segue che il tasso di inflazione \\(\\pi\\) viene definito come \\(\\pi =\n\\frac{P_t - P_{t-1}}{P_{t-1}} \\approx \\ln P_t - \\ln P_{t-1}\\)\n\n# Chiave per l'Inflation Rate\nHICP_key = 'ICP/M.U2.N.000000.4.ANR'  # Codice identificativo dell'Inflation Rate\nHICP = make_df_ECB(HICP_key, 'Inflation Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo il grafico\nfig, ax = plt.subplots()  # Crea una figura e un asse\n\n# Traccia l'inflazione\nax.plot(HICP)  # Traccia il grafico dell'inflazione\n\n# Imposta etichetta per l'asse X e Y\nax.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax.set_ylabel('Tasso di Inflazione')  # Etichetta per l'asse Y\n\n# Titolo del grafico\nplt.title('Tasso di Inflazione nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  }
]