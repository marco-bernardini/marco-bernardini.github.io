[
  {
    "objectID": "notebooks/User Cohorts.html",
    "href": "notebooks/User Cohorts.html",
    "title": "User Cohorts Analysis",
    "section": "",
    "text": "Understanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention, engagement, and growth is through cohort analysis.\nA user cohort table segments users based on shared characteristics—such as sign-up month or first purchase—and tracks their activity across time. This approach helps businesses measure customer retention, assess the impact of product changes, and optimize marketing strategies.\nIn this notebook, we’ll walk through the process of analyzing a cohort table step by step. Using Python, we’ll transform raw data into actionable insights, visualizing retention curves and uncovering trends that can inform strategic decisions. Whether you’re working in SaaS, e-commerce, or mobile apps, this guide will equip you with the tools to extract meaningful patterns from user data.\nLet’s dive in.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\ncmap = plt.get_cmap('viridis')\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/User Cohorts.html#right-aligned-table",
    "href": "notebooks/User Cohorts.html#right-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Right-aligned table",
    "text": "Right-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active in the given month\nFor example, the number of users who made their first purchase in March and are still engaging with the platform in June are 9,468\n\nnc_cohorts_right = pd.read_csv('user_cohorts.csv')\nnc_cohorts_right.rename(columns={'Num Users': 'month'}, inplace=True)\nind = nc_cohorts_right['month']\nnc_cohorts_right.set_index('month', inplace=True)\n\nnc_cohorts_right_filled = nc_cohorts_right.fillna(0)\n\nstyled_df = nc_cohorts_right.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\nnan\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\n\n\nMar\nnan\nnan\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\n\n\nApr\nnan\nnan\nnan\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\n\n\nMay\nnan\nnan\nnan\nnan\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\n\n\nJun\nnan\nnan\nnan\nnan\nnan\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\n\n\nJul\nnan\nnan\nnan\nnan\nnan\nnan\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\n\n\nAug\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n109,117\n43,172\n35,336\n33,469\n30,535\n\n\nSep\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n116,187\n40,402\n35,154\n31,603\n\n\nOct\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n117,185\n34,462\n29,055\n\n\nNov\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n107,597\n32,920\n\n\nDec\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n97,164\n\n\n\n\n\n\nretained_clients = nc_cohorts_right.Dec.sum()\nprint(f\"Total reatined clients: {int(retained_clients)}\")\n\nTotal reatined clients: 288895\n\n\nWe can more intuitively visualize the table with a simple chart.\n\ncolors = cmap(np.linspace(0, 1, nc_cohorts_right.shape[1]))\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.stackplot(nc_cohorts_right_filled.index, nc_cohorts_right_filled, labels=nc_cohorts_right_filled.columns, colors=colors, alpha=0.8)\n\nax.legend(loc='upper left', title='Cohort')\nax.set_title('Users Per Month', loc='left')\nax.set_xlabel('Month')\nax.set_ylabel('Number of Users')\nax.grid(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, I noticed that the growth is slowing down. Remember, you always want to confirm your visual intuition with data: in this case, i approximated the growth curve with a known (continuous) function. As the function approximates well the curve, I can use its first derivative to draw conclusions about the instantaneous growth.\n\ntrend = nc_cohorts_right.sum()\nt = np.arange(1, len(trend) + 1)\n\ndef logistic_function(t, L, k, t0):\n    return L / (1 + np.exp(-k * (t - t0)))\n\n[L, k, t0], _ = curve_fit(logistic_function, t, trend, p0=[trend.max(), 1, 0])\n\napprox = logistic_function(t, L, k, t0)\nintensity = k * L * np.exp(-k * (t - t0)) / (1 + np.exp(-k * (t - t0)))**2\n\nR2 = 1 - np.sum((trend - approx) ** 2) / np.sum((trend - np.mean(trend)) ** 2)\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 6))\nax[0].plot(trend, label=\"Trend\")\nax[0].plot(approx, label=\"Logistic Fit\")\nax[0].set_title(f'Approximation of Growth with Logistic Function: R² = {round(R2, 3)}')\nax[0].legend()\n\nax[1].plot(intensity, color=\"red\", label=\"Derivative\")\nax[1].set_title(\"Derivative of Logistic Function (Growth Intensity)\")\nax[1].legend()\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/User Cohorts.html#left-aligned-table",
    "href": "notebooks/User Cohorts.html#left-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Left-aligned table",
    "text": "Left-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active after n months, where n is the column header. n=0 corresponds to the month in which the users made their first purchase\nThis table is very useful, as it allows to analyze the behaviour of different cohorts during their lifetime\n\nnc = nc_cohorts_right.copy().to_numpy()\nfor i, row in enumerate(nc):\n    a = np.argwhere(np.isnan(row))\n    if a.size != 0:\n        start = a.max() + 1\n        temp = row[start:]\n        if temp.size &lt; 12:\n            temp = np.append(temp, np.full(shape=(12 - temp.size), fill_value=np.nan))\n            nc[i] = temp\n    else:\n        pass\n    \nnc_cohorts_left = pd.DataFrame(nc, index=ind)\n\nstyled_df = nc_cohorts_left.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\nnan\n\n\nMar\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\nnan\nnan\n\n\nApr\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\nnan\nnan\nnan\n\n\nMay\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\nnan\nnan\nnan\nnan\n\n\nJun\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\nnan\nnan\nnan\nnan\nnan\n\n\nJul\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nAug\n109,117\n43,172\n35,336\n33,469\n30,535\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nSep\n116,187\n40,402\n35,154\n31,603\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nOct\n117,185\n34,462\n29,055\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nNov\n107,597\n32,920\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nDec\n97,164\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\nprint(f\"Total initial users: {int(nc_cohorts_left[0].sum())}\")\n\nTotal initial users: 765395\n\n\n\nAssessing retention rates\nWith a left-aligned table, we can analyse retention rates for every cohort. In this case, retention rates are calculated as: \\[\n    \\frac{Active\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nretention = nc_cohorts_left.copy()\n\nfor col in retention.columns:\n    if col == 0:\n        retention[col] = 1\n    else:\n        retention[col] = nc_cohorts_left[col]/nc_cohorts_left[0]\n\n\nstyled_df = retention.style.background_gradient(cmap=cmap, axis=1, vmax=.60, vmin=.30).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n100.00%\n60.14%\n55.07%\n49.64%\n50.36%\n52.54%\n50.00%\n50.72%\n50.36%\n49.28%\n49.64%\n48.91%\n\n\nFeb\n100.00%\n54.33%\n47.06%\n45.85%\n43.04%\n42.28%\n41.44%\n37.69%\n38.72%\n36.66%\n36.22%\nnan%\n\n\nMar\n100.00%\n55.07%\n49.26%\n47.24%\n43.94%\n43.32%\n40.04%\n38.86%\n37.44%\n35.21%\nnan%\nnan%\n\n\nApr\n100.00%\n52.29%\n47.72%\n44.09%\n41.57%\n39.16%\n36.92%\n36.29%\n33.03%\nnan%\nnan%\nnan%\n\n\nMay\n100.00%\n47.50%\n41.45%\n38.84%\n36.39%\n34.41%\n34.01%\n31.10%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n100.00%\n45.60%\n39.84%\n36.10%\n33.28%\n32.96%\n29.70%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n100.00%\n44.34%\n37.49%\n33.25%\n32.17%\n29.35%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n100.00%\n39.56%\n32.38%\n30.67%\n27.98%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n100.00%\n34.77%\n30.26%\n27.20%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n100.00%\n29.41%\n24.79%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n100.00%\n30.60%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n100.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nWe can then easily visualize the retention trend across different cohorts with a line chart. Ideally, retention rates should improve with time, although this is not the case.\n\nfig, ax = plt.subplots(1, 1)\nretention.T.plot(marker='s', ax=ax, color=colors, alpha=0.67)\n\nax.set_title('Hanging Ribbons', loc='left')\nax.set_ylabel('% Of Cohort Retained')\nax.set_xlabel('Time')\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\nax.legend(\n    title='Cohort',\n    loc='upper center',         \n    ncol=5,\n    frameon=True\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can easily check that retention is negatively correlated with time (and size of the cohorts)\n\nfrom sklearn.preprocessing import minmax_scale\nscaled_nc_cohorts_left = minmax_scale(range(1, len(nc_cohorts_left.index)+1), axis=0)\nscaled_retention = minmax_scale(retention.iloc[:, 1:4], axis=0)\n\ndf_scaled = pd.DataFrame(\n    scaled_nc_cohorts_left, columns=[\"nc_cohorts_left\"]\n).join(pd.DataFrame(scaled_retention, columns=[\"retention_1\", \"retention_2\", \"retention_3\"]))\n\ncorr_matrix = df_scaled.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=False)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nAssessing churn rates\nWith a left-aligned table, we can also analyse churn rates for every cohort. Churn rates in the following table are calculated as: \\[\n    \\frac{Churned\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nchurn = nc_cohorts_left.copy()\n\nfor col in churn.columns:\n    if col == 0:\n        churn[col] = 0\n    else:\n        churn[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])/nc_cohorts_left[col-1]\n\nstyled_df = churn.style.background_gradient(cmap=cmap, axis=1, vmax=.2, vmin=0).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n0.00%\n39.86%\n8.43%\n9.87%\n-1.46%\n-4.32%\n4.83%\n-1.45%\n0.71%\n2.16%\n-0.74%\n1.46%\n\n\nFeb\n0.00%\n45.67%\n13.38%\n2.56%\n6.13%\n1.76%\n2.00%\n9.04%\n-2.72%\n5.30%\n1.22%\nnan%\n\n\nMar\n0.00%\n44.93%\n10.56%\n4.10%\n6.98%\n1.42%\n7.56%\n2.95%\n3.65%\n5.95%\nnan%\nnan%\n\n\nApr\n0.00%\n47.71%\n8.73%\n7.60%\n5.72%\n5.81%\n5.70%\n1.72%\n8.99%\nnan%\nnan%\nnan%\n\n\nMay\n0.00%\n52.50%\n12.74%\n6.29%\n6.31%\n5.45%\n1.16%\n8.55%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n0.00%\n54.40%\n12.62%\n9.40%\n7.79%\n0.96%\n9.92%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n0.00%\n55.66%\n15.46%\n11.31%\n3.24%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n0.00%\n60.44%\n18.15%\n5.28%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n0.00%\n65.23%\n12.99%\n10.10%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n0.00%\n70.59%\n15.69%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n0.00%\n69.40%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n0.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nIt’s interesting to calculate and plot the weighted average churn rate for every month. In this case, most of the users disengage after the first month: this is because they were attracted with offers and promotions, and left the platforms as soon as they used those incentives.\n\nchurned = nc_cohorts_left.copy()\nretained = nc_cohorts_left.copy()\n\nfor col in churned.columns:\n    if col == 0:\n        churned[col] = 0\n    else:\n        churned[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])\n        \navg_churn = np.zeros(11)\n\nretained_sum = retained.iloc[:-1,:].sum()\nchurned_sum = churned.iloc[:-1,1:].sum()\n\nfor i in range(len(avg_churn)):\n    avg_churn[i] = churned_sum[i+1] / retained_sum[(i)]\n    \navg_churn\n\narray([0.61894928, 0.12467046, 0.0687431 , 0.05115784, 0.03639244,\n       0.04167227, 0.03471718, 0.0376834 , 0.02842538, 0.00237718,\n       0.00210748])\n\n\n\nstd = churn.iloc[:-1,1:].std()\n\nfig, ax = plt.subplots(1, 1,figsize=(8,5))\nax.plot(range(1, 12, 1), avg_churn, label='Mean', color='#00A082')\n\nplt.fill_between(range(1, 12, 1), avg_churn - std, avg_churn + std, alpha=0.2, label='±1 Std Dev', color='#00A082')\n\nplt.xlabel('Month')\nplt.ylabel('% of churned customers (relative to prev. month)')\nplt.title('Churned customers in lifetime month', loc='left')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\nI then wanted to see how many additional clients could’ve been retained at the end of the year by reducing the churn rate by 5 percentage points in the first month\n\navg_churn_proj = avg_churn.copy()\navg_churn_proj[0] = avg_churn_proj[0]-.05\nstarting_users = nc_cohorts_left[0].values\nprojection = np.zeros([12,12])\nprojection[:,0] = starting_users\nfor i in range(11):\n    projection[:(11-i),i+1] = projection[:(11-i),i] * (1-avg_churn_proj[i])\n    \ntotal_clients = 0\nfor i in range(len(projection)):\n    total_clients += projection[i, 11-i]\n    \nprint(f\"Additional clients: {int(total_clients - retained_clients)}\")\n\nAdditional clients: 42441.379632713506\n\n\nTo gauge the average time clients who get past the first month get engaged with the service, it is userful to calculate the (truncated) lifespan of the users. This is simply calculated as: \\[\n\\frac{1}{avg\\, churn\\, rate}\n\\]\n\n1/((nc_cohorts_left.sum()[1:-1]*avg_churn[1:]).sum()/nc_cohorts_left.sum()[1:-1].sum())\n\n14.111089070031147"
  },
  {
    "objectID": "macroeconomics.html",
    "href": "macroeconomics.html",
    "title": "Macroeconomics",
    "section": "",
    "text": "This section contains various resources related to macroeconomics and econometrics, including articles, research, interactive notebooks and other useful resources."
  },
  {
    "objectID": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "href": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "title": "Macroeconomics",
    "section": "Corso introduttivo di Macroeconomia con Python",
    "text": "Corso introduttivo di Macroeconomia con Python\n\nLe variabili macroeconomiche\nAn interactive notebook (in Italian) exploring macroeconomic variables and their applications."
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This section provides a collection of resources on data analysis, including notebooks and practical insights drawn from my professional experience."
  },
  {
    "objectID": "data-analysis.html#marketing",
    "href": "data-analysis.html#marketing",
    "title": "Data Analysis",
    "section": "Marketing",
    "text": "Marketing\n\nUsers Cohorts Analysis\nUnderstanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention engagement, and growth is through cohort analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "&lt;h1&gt;Your Name&lt;/h1&gt;\n&lt;p&gt;I'm a macroeconomics researcher with a passion for data, finance, and programming. Currently pursuing my MSc at LSE, I explore economic crises, nonlinear models, and financial stability. I also enjoy endurance sports and mountaineering.&lt;/p&gt;"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\nLeonardo Assicurazioni, Milan\nLeonardo Assicurazioni is a leading insurance distributor with a robust network of over 400 consultants based in Milan. After three years in technical roles, I founded and led the company’s data analytics office for five years, driving key insights and innovations.\n\nData Analyst\n\nLed and trained a team of three, ensuring the successful delivery of key data projects that enhanced business operations and decision-making.\nConsolidated and structured raw data from multiple sources and formats (SQL, JSON, legacy systems).\nIncreased revenue by developing unsupervised clustering models for client segmentation and supervised classification models (KNN, Decision Tree, Logistic Regression) to optimize sales campaign efficiency.\nReduced costs by building classification models to prevent churn and insolvencies and collaborating with the quality control department to develop fraud-prevention models.\nWorked with HR to design and analyze employee surveys, extracting insights and developing predictive models to reduce turnover.\nConducted in-depth sales data analysis using Python, generating actionable insights and presenting findings with Matplotlib visualizations.\nBuilt over 50 Power BI dashboards from scratch in close collaboration with top management to support decision-making.\nDeveloped interactive web-based data visualizations with D3.js, allowing stakeholders to explore complex datasets intuitively on portable devices.\nCreated structured reports and presentations in Excel and PowerPoint to effectively communicate insights to senior leadership.\nServed as a translator in key meetings with foreign investors and presented at large conferences.\nOrganized and monitored sales competitions in close collaboration with the sales director.\nAutomated key processes by working with the IT team to develop custom VBA applications, significantly reducing manual effort and improving efficiency.\n\n\n\nUnderwriter and Sales Support Specialist\n\nManaged back-office and control activities, ensuring efficient operations\nStreamlined administrative workflows, reducing processing times and improving efficiency\nGained deep knowledge of insurance products and the market through one year of experience in the underwriting office."
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "CV",
    "section": "Certificates",
    "text": "Certificates\n\nGRE\n\n\n\nOverall 336\n\n\n\n\nQuant 170\n\n\n\n\nVerbal 166\n\n\n\n\n\nIELTS\n\n\n\nOverall 8/9\n\n\n\n\nReading 9/9\n\n\n\n\nListening 9/9\n\n\n\n\nWriting 7.5/9\n\n\n\n\nSpeaking 7/9"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Marco Bernardini",
    "section": "Education",
    "text": "Education\nUniversità Bicocca, Milano (IT)\nBSc in Economics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Marco Bernardini",
    "section": "Experience",
    "text": "Experience\nLeonardo Assicurazioni, Milano (IT)\nLead Data Analyst"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html",
    "href": "notebooks/Le variabili macroeconomiche.html",
    "title": "Le variabili macroeconomiche",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport eurostat\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n#     \"axes.grid\": False,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"extracts data from the BCE datawarehouse\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'\n    format_ = '?format=csvdata'\n    df = pd.read_csv(url_+key+format_)\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n    df = df.set_index('TIME_PERIOD')\n    df.columns = [obs_name]\n    return df"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "href": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "title": "Le variabili macroeconomiche",
    "section": "Produzione Aggregata",
    "text": "Produzione Aggregata\n\nDefinizione\nLa misura della produzione aggregata nella contabilità nazionale è chiamata prodotto interno lordo, o PIL. Il PIL può essere definito in tre modi equivalenti:\n\n\nIl PIL è il valore dei beni e dei servizi finali prodotti nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma del valore aggiunto nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma dei redditi dell’economia in un dato periodo di tempo\n\n\nDalla definizione (3) deriva che, in una economia, produzione aggregata e reddito aggregato siano sempre uguali\n\n\nCome si misura il PIL?\nNormalmente, le informazioni sono raccolte dalle autorità fiscali di un paese:\n\n\nLe imprese registrano le proprie vendite\n\n\nLe imprese pagano tasse sul valore aggiunto\n\n\nGli individui dichiarano il reddito percepito\n\n\n\n\nPIL nominale e PIL reale\nIl PIL nominale è la somma del valore dei beni finali valutati al loro prezzo corrente, dunque la sua crescita è influenzata da I. l’aumento della produzione II. l’aumento dei prezzi. Il PIl reale è la somma del valore dei beni finali valutati a prezzi costanti: di solito si utilizzano i prezzi di un anno di riferimento\n\n\nPIL pro capite\nSpesso, per misurare il tenore di vita in un paese, si utilizza il PIL pro capite, ovvero il PIL reale diviso per la popolazione del paese\n\n# Creazione del DataFrame con i dati del PIL dell'Area Euro\n# La chiave specificata corrisponde al PIL reale trimestrale in euro a prezzi concatenati\nPIL_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'\npil = make_df_ECB(PIL_KEY, 'PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie utilizzata rappresenta il PIL reale dell'Area Euro con dati trimestrali (Q).\nI valori sono espressi in miliardi di euro (EUR) a prezzi concatenati (LR), il che significa\nche sono corretti per l'inflazione e consentono un confronto diretto nel tempo.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse: \n  - Crisi del debito europeo (2009) con una significativa contrazione del PIL.\n  - Pandemia di Covid-19 (2020) con una forte caduta seguita da una ripresa.\n\"\"\"\n\n# Creazione del grafico del PIL\npil.plot()\nplt.title('PIL Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('PIL (Miliardi di euro, prezzi concatenati)')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', pil.loc['2009-01-01']), \n             xytext=(-20, 60),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', pil.loc['2020-04-01']), \n             xytext=(-40, -40),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTasso di crescita del PIL\nSpesso, si utilizza il concetto di crescita del PIL per indicare espansioni (periodi di crescita positiva) e recessioni (almeno due trimestri consecutivi di crescita negativa). Il tasso di crescita del PIL è semplicemente definito come: $ Y_t - Y_{t-1} $\n\n# Creazione del DataFrame con i dati della crescita del PIL reale\n# La chiave specificata corrisponde alla crescita del PIL reale trimestrale in termini percentuali\nREAL_GDP_GROWTH_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.GY'\nreal_gdp_growth = make_df_ECB(REAL_GDP_GROWTH_KEY, 'Crescita PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie rappresenta la crescita percentuale del PIL reale dell'Area Euro con dati trimestrali (Q).\nMisura il tasso di variazione del PIL rispetto al trimestre precedente, esprimendolo in termini percentuali.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Indicatore: Crescita percentuale del PIL reale (GY)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse:\n  - Crisi del debito europeo (2009) con crescita negativa e recessione prolungata.\n  - Pandemia di Covid-19 (2020) con una caduta storica seguita da un rimbalzo.\n\"\"\"\n\n# Creazione del grafico della crescita del PIL\nreal_gdp_growth.plot()\nplt.title('Crescita del PIL nell\\'Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('Crescita %')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Aggiunta di una linea orizzontale per evidenziare il valore zero (assenza di crescita)\nplt.axhline(y=0, color='#525252', linestyle='--', linewidth=1)\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', real_gdp_growth.loc['2009-01-01']), \n             xytext=(-100, -30),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', real_gdp_growth.loc['2020-04-01']), \n             xytext=(-150, 10),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLa composizione del PIL\nPer comprendere cosa determini la domanda di beni, si può scomporre la produzione aggregata in base alla tipologia di acquirente dei beni. Si possono individuare cinque componenti:\n\n\nIl consumo (C), ovvero beni e servizi acquistati dai consumatori, ovvero le famiglie\n\n\nL’investimento (I) che include l’investimento non residenziale (impianti e macchinari) da parte delle aziende, e l’investimento residenziale da parte delle famiglie\n\n\nLa spesa pubblica in beni e servizi (G), ovvero i beni acquistati dallo stato e dagli enti pubblici. Si noti che non fanno parte di questa categoria pensioni, servizi assistenziali quali il reddito di cittadinanza, o gli interessi pagati sul debito.\n\n\nImportazioni (IM), che entrano nell’equazione con segno negativo\n\n\nEsportazioni (X), che entrano nell’equazione con segno positivo\n\n\nSi ha dunque che: \\(Y = C + I + G + (X - IM)\\) Dove spesso si usa parlare di esportazioni nette o saldo commerciale del termine \\(X - IM\\).\n\n# Definisco le chiavi per i vari indicatori economici relativi all'Italia\n# La struttura di ogni chiave segue il formato: \n# MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N\n\nIT_gdp_key = 'MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N'  # PIL (Prodotto Interno Lordo)\n\nIT_inv_key = 'MNA/Q.Y.IT.W0.S1.S1.D.P51G.N11G._T._Z.EUR.V.N'  # Investimenti\n\nIT_cons_key = 'MNA/Q.Y.IT.W0.S1M.S1.D.P31._Z._Z._T.EUR.V.N'  # Consumi\n\nIT_gov_exp_key = 'MNA/Q.Y.IT.W0.S13.S1.D.P3._Z._Z._T.EUR.V.N'  # Spesa Pubblica\n\nIT_imp_key = 'MNA/Q.Y.IT.W1.S1.S1.C.P7._Z._Z._Z.EUR.V.N'  # Importazioni\n\nIT_exp_key = 'MNA/Q.Y.IT.W1.S1.S1.D.P6._Z._Z._Z.EUR.V.N'  # Esportazioni\n\n# Creo i DataFrame per ogni indicatore economico utilizzando le chiavi\nIT_gdp = make_df_ECB(IT_gdp_key, 'GDP')  # Prodotto Interno Lordo (PIL)\nIT_inv = make_df_ECB(IT_inv_key, 'INV')  # Investimenti\nIT_cons = make_df_ECB(IT_cons_key, 'CONS')  # Consumi\nIT_gov_exp = make_df_ECB(IT_gov_exp_key, 'GOV_EXP')  # Spesa Pubblica\nIT_imp = make_df_ECB(IT_imp_key, 'IMP')  # Importazioni\nIT_exp = make_df_ECB(IT_exp_key, 'EXP')  # Esportazioni\n\n# Unisco tutti i DataFrame in uno solo per l'analisi aggregata delle spese\nimport functools as ft\ndfs = [IT_gdp, IT_inv, IT_cons, IT_gov_exp, IT_imp, IT_exp]\nIT_expenditures = ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), dfs)\n\n# Calcolo la bilancia commerciale (BOT) come la differenza tra esportazioni e importazioni\nIT_expenditures['BOT'] = IT_expenditures['EXP'] - IT_expenditures['IMP']\n\n# Creo un grafico a barre impilate per visualizzare la spesa aggregata\nax = IT_expenditures[['INV', 'CONS', 'GOV_EXP', 'BOT']][-32:].plot.bar(stacked=True, figsize=(9, 5))\n\n# Modifico le etichette dell'asse x per visualizzare l'anno e il trimestre\nax.set_xticklabels([f'{x.year} Q{x.quarter}' for x in IT_expenditures.index[-32:]])\n\n# Aggiungo il titolo e le etichette degli assi\nplt.title('Spesa Aggregata - IT')\nplt.xlabel('Trimestre')\nplt.ylabel('Spesa')\n\n# Aggiungo una legenda con le categorie di spesa\nax.legend(['I', 'C', 'G', 'X-IM', 'Y'],\n          title='Categorie',\n          loc='upper left',      \n          frameon=True,\n          fontsize='x-small',\n          ncol=4\n          )\n\n# Adatto il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPIL e PNL\nIn alcuni casi, è utile distinguere tra: PIL, che misura il valore totale dei beni e servizi prodotti all’interno di un paese, indipendentemente dalla proprietà delle risorse e PNL (Prodotto Nazionale Lordo), che misura il valore totale dei beni e servizi prodotti da cittadini di un paese, indipendentemente da dove si trovino nel mondo. Sono emblematici il caso tedesco e quello irlandese.\n\n# Definiamo le colonne di interesse per i dati\ncols = ['geo\\TIME_PERIOD', '2020', '2021', '2022', '2023']\n\n# Otteniamo i dati del Prodotto Nazionale Lordo (PNL) pro capite\ngni_pc_tuples = eurostat.get_data('nama_10_pp')  # Recupera i dati dal database Eurostat\ngni_pc = pd.DataFrame(gni_pc_tuples[1:], columns=gni_pc_tuples[0])  # Crea il DataFrame\ngni_pc = gni_pc[cols]  # Seleziona solo le colonne di interesse\ngni_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne per chiarezza\n\n# Otteniamo i dati del Prodotto Interno Lordo (PIL) pro capite\ngdp_pc_tuples = eurostat.get_data('nama_10_pc')  # Recupera i dati del PIL pro capite\ngdp_pc = pd.DataFrame(gdp_pc_tuples[1:], columns=gdp_pc_tuples[0])  # Crea il DataFrame\ngdp_pc = gdp_pc.query(\"unit=='CP_PPS_EU27_2020_HAB' & na_item == 'B1GQ'\")  # Filtro per unità e tipo di dato\ngdp_pc = gdp_pc[cols]  # Seleziona solo le colonne di interesse\ngdp_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne\n\n# Definiamo la lista dei paesi per i quali vogliamo visualizzare i dati\ncountries = ['IT', 'DE', 'IE']\n\n# Creiamo un grafico a sottotrame per visualizzare i dati\nfig, axs = plt.subplots(1, 3, sharey=True, figsize=(9, 4))  # Crea 3 sottotrame affiancate\naxs = axs.flatten()  # Appiattisce l'array di assi per facilitare l'accesso\n\n# Ciclo sui paesi per plottare i dati di ciascuno\nfor n, country in enumerate(countries):\n    # Selezioniamo i dati del paese corrente per PNL e PIL\n    gni_pc_country = gni_pc[gni_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PNL\n    gdp_pc_country = gdp_pc[gdp_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PIL\n    \n    # Plottiamo i dati sulla stessa sottotrama\n    axs[n].plot(gni_pc_country, label='PNL pro capite')  # PNL\n    axs[n].plot(gdp_pc_country, label='PIL pro capite')  # PIL\n    \n    # Aggiungiamo il titolo e le etichette\n    axs[n].set_title(country, loc='left')  # Titolo a sinistra per ogni paese\n    axs[n].set_xlabel('Anno')  # Etichetta asse X\n    axs[n].set_ylabel('Valore')  # Etichetta asse Y\n\n    # Aggiungiamo la legenda\n    axs[n].legend(\n        loc='upper left',  # Posizione della legenda\n        frameon=True,  # Abilita il frame della legenda\n        fontsize='x-small'  # Font di dimensioni piccole\n    )\n\n# Titolo principale del grafico\nfig.suptitle('PIL e PNL pro capite in Italia, Germania, Irlanda')\n\n# Miglioriamo il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Mostriamo il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di disoccupazione",
    "text": "Tasso di disoccupazione\nIl tasso di disoccupazione si calcola come rapporto tra disoccupati (ovvero coloro i quali non hanno un lavoro, ma lo stanno cercando) e la forza lavoro (data dalla somma di occupati e disoccupati). A questo proposito, è importante sottolineare come chi non ha un lavoro, ma non ne sta nemmeno cercando uno, sia considerato fuori dalla forza lavoro, e dunque non incluso nel conteggio dei disoccupati. Si può quindi calcolare il tasso di partecipazione come forza lavoro divisa per il totale della popolazione.\n\nCome si misura il tasso di disoccupazione?\nGeneralmente, si effettuano indagini a campione sulle famiglie (Labour Force Surveys)\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database Eurostat\n\n# Creiamo un grafico con due assi\nfig, ax1 = plt.subplots()  # Crea una figura e un asse\n\n# Plot del tasso di disoccupazione\nax1.plot(unemp_rate)  # Traccia il grafico del tasso di disoccupazione\nax1.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax1.set_ylabel('Tasso di Disoccupazione', color=bmh_colors[0])  # Etichetta per l'asse Y con colore personalizzato\n\n# Creiamo un secondo asse per la crescita del PIL\nax2 = ax1.twinx()  # Crea un secondo asse Y condiviso sull'asse X\nax2.plot(real_gdp_growth[real_gdp_growth.index &gt;= unemp_rate.index.min()], color='#525252', linestyle=':')  # Traccia il grafico della crescita del PIL con linea tratteggiata\nax2.set_ylabel('Crescita del PIL')  # Etichetta per l'asse Y della crescita del PIL\nax2.grid(False)  # Disabilita la griglia per il secondo asse\n\n# Titolo del grafico\nplt.title('Disoccupazione e crescita del PIL nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di Inflazione",
    "text": "Tasso di Inflazione\nL’inflazione rappresenta un aumento sostenuto del livello generale dei prezzi. Il tasso di inflazione è il tasso a cui il livello dei prezzi aumenta nel tempo.\n\nCome si misura l’inflazione?\nIl livello dei prezzi si può calcolare in due modi:\n\n\nUtilizzando il deflatore del PIL, ovvero il rapporto tra il PIL nominale e il PIL reale nell’anno di interesse\n\n\nLindice armonizzato dei prezzi al consumo, ovvero un numero indice che fissa un prezzo pari a 100 nell’anno base\n\n\nChiamando \\(P_t\\) il livello generale dei prezzi nell’anno \\(t\\), segue che il tasso di inflazione \\(\\pi\\) viene definito come \\(\\pi =\n\\frac{P_t - P_{t-1}}{P_{t-1}} \\approx \\ln P_t - \\ln P_{t-1}\\)\n\n# Chiave per l'Inflation Rate\nHICP_key = 'ICP/M.U2.N.000000.4.ANR'  # Codice identificativo dell'Inflation Rate\nHICP = make_df_ECB(HICP_key, 'Inflation Rate')  # Ottieni i dati dal database Eurostat\n\n# Creiamo il grafico\nfig, ax = plt.subplots()  # Crea una figura e un asse\n\n# Traccia l'inflazione\nax.plot(HICP)  # Traccia il grafico dell'inflazione\n\n# Imposta etichetta per l'asse X e Y\nax.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax.set_ylabel('Tasso di Inflazione')  # Etichetta per l'asse Y\n\n# Titolo del grafico\nplt.title('Tasso di Inflazione nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  }
]