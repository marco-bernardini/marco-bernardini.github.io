[
  {
    "objectID": "notebooks/User Cohorts.html",
    "href": "notebooks/User Cohorts.html",
    "title": "User Cohorts Analysis",
    "section": "",
    "text": "Understanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention, engagement, and growth is through cohort analysis.\nA user cohort table segments users based on shared characteristics—such as sign-up month or first purchase—and tracks their activity across time. This approach helps businesses measure customer retention, assess the impact of product changes, and optimize marketing strategies.\nIn this notebook, we’ll walk through the process of analyzing a cohort table step by step. Using Python, we’ll transform raw data into actionable insights, visualizing retention curves and uncovering trends that can inform strategic decisions. Whether you’re working in SaaS, e-commerce, or mobile apps, this guide will equip you with the tools to extract meaningful patterns from user data.\nLet’s dive in.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\ncmap = plt.get_cmap('viridis')\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/User Cohorts.html#right-aligned-table",
    "href": "notebooks/User Cohorts.html#right-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Right-aligned table",
    "text": "Right-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active in the given month\nFor example, the number of users who made their first purchase in March and are still engaging with the platform in June are 9,468\n\nnc_cohorts_right = pd.read_csv('user_cohorts.csv')\nnc_cohorts_right.rename(columns={'Num Users': 'month'}, inplace=True)\nind = nc_cohorts_right['month']\nnc_cohorts_right.set_index('month', inplace=True)\n\nnc_cohorts_right_filled = nc_cohorts_right.fillna(0)\n\nstyled_df = nc_cohorts_right.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\nnan\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\n\n\nMar\nnan\nnan\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\n\n\nApr\nnan\nnan\nnan\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\n\n\nMay\nnan\nnan\nnan\nnan\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\n\n\nJun\nnan\nnan\nnan\nnan\nnan\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\n\n\nJul\nnan\nnan\nnan\nnan\nnan\nnan\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\n\n\nAug\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n109,117\n43,172\n35,336\n33,469\n30,535\n\n\nSep\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n116,187\n40,402\n35,154\n31,603\n\n\nOct\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n117,185\n34,462\n29,055\n\n\nNov\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n107,597\n32,920\n\n\nDec\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n97,164\n\n\n\n\n\n\nretained_clients = nc_cohorts_right.Dec.sum()\nprint(f\"Total reatined clients: {int(retained_clients)}\")\n\nTotal reatined clients: 288895\n\n\nWe can more intuitively visualize the table with a simple chart.\n\ncolors = cmap(np.linspace(0, 1, nc_cohorts_right.shape[1]))\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.stackplot(nc_cohorts_right_filled.index, nc_cohorts_right_filled, labels=nc_cohorts_right_filled.columns, colors=colors, alpha=0.8)\n\nax.legend(loc='upper left', title='Cohort')\nax.set_title('Users Per Month', loc='left')\nax.set_xlabel('Month')\nax.set_ylabel('Number of Users')\nax.grid(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, I noticed that the growth is slowing down. Remember, you always want to confirm your visual intuition with data: in this case, i approximated the growth curve with a known (continuous) function. As the function approximates well the curve, I can use its first derivative to draw conclusions about the instantaneous growth.\n\ntrend = nc_cohorts_right.sum()\nt = np.arange(1, len(trend) + 1)\n\ndef logistic_function(t, L, k, t0):\n    return L / (1 + np.exp(-k * (t - t0)))\n\n[L, k, t0], _ = curve_fit(logistic_function, t, trend, p0=[trend.max(), 1, 0])\n\napprox = logistic_function(t, L, k, t0)\nintensity = k * L * np.exp(-k * (t - t0)) / (1 + np.exp(-k * (t - t0)))**2\n\nR2 = 1 - np.sum((trend - approx) ** 2) / np.sum((trend - np.mean(trend)) ** 2)\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 6))\nax[0].plot(trend, label=\"Trend\")\nax[0].plot(approx, label=\"Logistic Fit\")\nax[0].set_title(f'Approximation of Growth with Logistic Function: R² = {round(R2, 3)}')\nax[0].legend()\n\nax[1].plot(intensity, color=\"red\", label=\"Derivative\")\nax[1].set_title(\"Derivative of Logistic Function (Growth Intensity)\")\nax[1].legend()\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/User Cohorts.html#left-aligned-table",
    "href": "notebooks/User Cohorts.html#left-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Left-aligned table",
    "text": "Left-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active after n months, where n is the column header. n=0 corresponds to the month in which the users made their first purchase\nThis table is very useful, as it allows to analyze the behaviour of different cohorts during their lifetime\n\nnc = nc_cohorts_right.copy().to_numpy()\nfor i, row in enumerate(nc):\n    a = np.argwhere(np.isnan(row))\n    if a.size != 0:\n        start = a.max() + 1\n        temp = row[start:]\n        if temp.size &lt; 12:\n            temp = np.append(temp, np.full(shape=(12 - temp.size), fill_value=np.nan))\n            nc[i] = temp\n    else:\n        pass\n    \nnc_cohorts_left = pd.DataFrame(nc, index=ind)\n\nstyled_df = nc_cohorts_left.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\nnan\n\n\nMar\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\nnan\nnan\n\n\nApr\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\nnan\nnan\nnan\n\n\nMay\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\nnan\nnan\nnan\nnan\n\n\nJun\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\nnan\nnan\nnan\nnan\nnan\n\n\nJul\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nAug\n109,117\n43,172\n35,336\n33,469\n30,535\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nSep\n116,187\n40,402\n35,154\n31,603\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nOct\n117,185\n34,462\n29,055\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nNov\n107,597\n32,920\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nDec\n97,164\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\nprint(f\"Total initial users: {int(nc_cohorts_left[0].sum())}\")\n\nTotal initial users: 765395\n\n\n\nAssessing retention rates\nWith a left-aligned table, we can analyse retention rates for every cohort. In this case, retention rates are calculated as: \\[\n    \\frac{Active\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nretention = nc_cohorts_left.copy()\n\nfor col in retention.columns:\n    if col == 0:\n        retention[col] = 1\n    else:\n        retention[col] = nc_cohorts_left[col]/nc_cohorts_left[0]\n\n\nstyled_df = retention.style.background_gradient(cmap=cmap, axis=1, vmax=.60, vmin=.30).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n100.00%\n60.14%\n55.07%\n49.64%\n50.36%\n52.54%\n50.00%\n50.72%\n50.36%\n49.28%\n49.64%\n48.91%\n\n\nFeb\n100.00%\n54.33%\n47.06%\n45.85%\n43.04%\n42.28%\n41.44%\n37.69%\n38.72%\n36.66%\n36.22%\nnan%\n\n\nMar\n100.00%\n55.07%\n49.26%\n47.24%\n43.94%\n43.32%\n40.04%\n38.86%\n37.44%\n35.21%\nnan%\nnan%\n\n\nApr\n100.00%\n52.29%\n47.72%\n44.09%\n41.57%\n39.16%\n36.92%\n36.29%\n33.03%\nnan%\nnan%\nnan%\n\n\nMay\n100.00%\n47.50%\n41.45%\n38.84%\n36.39%\n34.41%\n34.01%\n31.10%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n100.00%\n45.60%\n39.84%\n36.10%\n33.28%\n32.96%\n29.70%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n100.00%\n44.34%\n37.49%\n33.25%\n32.17%\n29.35%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n100.00%\n39.56%\n32.38%\n30.67%\n27.98%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n100.00%\n34.77%\n30.26%\n27.20%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n100.00%\n29.41%\n24.79%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n100.00%\n30.60%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n100.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nWe can then easily visualize the retention trend across different cohorts with a line chart. Ideally, retention rates should improve with time, although this is not the case.\n\nfig, ax = plt.subplots(1, 1)\nretention.T.plot(marker='s', ax=ax, color=colors, alpha=0.67)\n\nax.set_title('Hanging Ribbons', loc='left')\nax.set_ylabel('% Of Cohort Retained')\nax.set_xlabel('Time')\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\nax.legend(\n    title='Cohort',\n    loc='upper center',         \n    ncol=5,\n    frameon=True\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can easily check that retention is negatively correlated with time (and size of the cohorts)\n\nfrom sklearn.preprocessing import minmax_scale\nscaled_nc_cohorts_left = minmax_scale(range(1, len(nc_cohorts_left.index)+1), axis=0)\nscaled_retention = minmax_scale(retention.iloc[:, 1:4], axis=0)\n\ndf_scaled = pd.DataFrame(\n    scaled_nc_cohorts_left, columns=[\"nc_cohorts_left\"]\n).join(pd.DataFrame(scaled_retention, columns=[\"retention_1\", \"retention_2\", \"retention_3\"]))\n\ncorr_matrix = df_scaled.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=False)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nAssessing churn rates\nWith a left-aligned table, we can also analyse churn rates for every cohort. Churn rates in the following table are calculated as: \\[\n    \\frac{Churned\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nchurn = nc_cohorts_left.copy()\n\nfor col in churn.columns:\n    if col == 0:\n        churn[col] = 0\n    else:\n        churn[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])/nc_cohorts_left[col-1]\n\nstyled_df = churn.style.background_gradient(cmap=cmap, axis=1, vmax=.2, vmin=0).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n0.00%\n39.86%\n8.43%\n9.87%\n-1.46%\n-4.32%\n4.83%\n-1.45%\n0.71%\n2.16%\n-0.74%\n1.46%\n\n\nFeb\n0.00%\n45.67%\n13.38%\n2.56%\n6.13%\n1.76%\n2.00%\n9.04%\n-2.72%\n5.30%\n1.22%\nnan%\n\n\nMar\n0.00%\n44.93%\n10.56%\n4.10%\n6.98%\n1.42%\n7.56%\n2.95%\n3.65%\n5.95%\nnan%\nnan%\n\n\nApr\n0.00%\n47.71%\n8.73%\n7.60%\n5.72%\n5.81%\n5.70%\n1.72%\n8.99%\nnan%\nnan%\nnan%\n\n\nMay\n0.00%\n52.50%\n12.74%\n6.29%\n6.31%\n5.45%\n1.16%\n8.55%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n0.00%\n54.40%\n12.62%\n9.40%\n7.79%\n0.96%\n9.92%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n0.00%\n55.66%\n15.46%\n11.31%\n3.24%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n0.00%\n60.44%\n18.15%\n5.28%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n0.00%\n65.23%\n12.99%\n10.10%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n0.00%\n70.59%\n15.69%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n0.00%\n69.40%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n0.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nIt’s interesting to calculate and plot the weighted average churn rate for every month. In this case, most of the users disengage after the first month: this is because they were attracted with offers and promotions, and left the platforms as soon as they used those incentives.\n\nchurned = nc_cohorts_left.copy()\nretained = nc_cohorts_left.copy()\n\nfor col in churned.columns:\n    if col == 0:\n        churned[col] = 0\n    else:\n        churned[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])\n        \navg_churn = np.zeros(11)\n\nretained_sum = retained.iloc[:-1,:].sum()\nchurned_sum = churned.iloc[:-1,1:].sum()\n\nfor i in range(len(avg_churn)):\n    avg_churn[i] = churned_sum[i+1] / retained_sum[(i)]\n    \navg_churn\n\narray([0.61894928, 0.12467046, 0.0687431 , 0.05115784, 0.03639244,\n       0.04167227, 0.03471718, 0.0376834 , 0.02842538, 0.00237718,\n       0.00210748])\n\n\n\nstd = churn.iloc[:-1,1:].std()\n\nfig, ax = plt.subplots(1, 1,figsize=(8,5))\nax.plot(range(1, 12, 1), avg_churn, label='Mean', color='#00A082')\n\nplt.fill_between(range(1, 12, 1), avg_churn - std, avg_churn + std, alpha=0.2, label='±1 Std Dev', color='#00A082')\n\nplt.xlabel('Month')\nplt.ylabel('% of churned customers (relative to prev. month)')\nplt.title('Churned customers in lifetime month', loc='left')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\nI then wanted to see how many additional clients could’ve been retained at the end of the year by reducing the churn rate by 5 percentage points in the first month\n\navg_churn_proj = avg_churn.copy()\navg_churn_proj[0] = avg_churn_proj[0]-.05\nstarting_users = nc_cohorts_left[0].values\nprojection = np.zeros([12,12])\nprojection[:,0] = starting_users\nfor i in range(11):\n    projection[:(11-i),i+1] = projection[:(11-i),i] * (1-avg_churn_proj[i])\n    \ntotal_clients = 0\nfor i in range(len(projection)):\n    total_clients += projection[i, 11-i]\n    \nprint(f\"Additional clients: {int(total_clients - retained_clients)}\")\n\nAdditional clients: 42441.379632713506\n\n\nTo gauge the average time clients who get past the first month get engaged with the service, it is userful to calculate the (truncated) lifespan of the users. This is simply calculated as: \\[\n\\frac{1}{avg\\, churn\\, rate}\n\\]\n\n1/((nc_cohorts_left.sum()[1:-1]*avg_churn[1:]).sum()/nc_cohorts_left.sum()[1:-1].sum())\n\n14.111089070031147"
  },
  {
    "objectID": "notebooks/La legge di Okun.html",
    "href": "notebooks/La legge di Okun.html",
    "title": "La legge di Okun",
    "section": "",
    "text": "All’inizio degli anni ’60, la crescita economica e la riduzione della disoccupazione erano temi centrali per gli economisti americani. Arthur M. Okun si specializzò nello studio del prodotto potenziale, ovvero il livello di output raggiungibile in condizioni di disoccupazione ottimale (all’epoca stimata intorno al 4%) e di una domanda sufficiente ad assorbire l’offerta disponibile.\nDal punto di vista pratico, era fondamentale quantificare l’impatto di una variazione della disoccupazione sulla produzione. Ad esempio, una riduzione del tasso di disoccupazione dal 6% al 4,5% avrebbe avvicinato il prodotto al suo livello potenziale, riducendo il “gap” tra output effettivo e potenziale. Al contrario, un aumento della disoccupazione avrebbe avuto l’effetto opposto.\nL’obiettivo di Okun era individuare una relazione quantitativa tra il prodotto nazionale lordo (PNL) e il tasso di disoccupazione. In questa sezione analizzeremo le due metodologie con cui Okun ha stimato tale relazione.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport warnings\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n\n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n\n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n\n    \"axes.linewidth\": 1.5,\n\n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": False,  # Disable the left spine (y-axis)\n    \"axes.spines.bottom\": True,\n\n    # Grid settings\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.2,\n    \"grid.linestyle\": '--',\n    \"grid.linewidth\": 0.7,\n    \"xtick.direction\": \"out\",\n    \"ytick.direction\": \"out\",\n    \"xtick.bottom\": True,  # Keep x-ticks at the bottom\n    \"ytick.left\": False,   # Remove y-ticks\n})\n\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#i.-differenze-percentuali",
    "href": "notebooks/La legge di Okun.html#i.-differenze-percentuali",
    "title": "La legge di Okun",
    "section": "I. Differenze percentuali",
    "text": "I. Differenze percentuali\nIn questo primo esercizio, viene utilizzato un modello di regressione lineare con intercetta, dove le variazioni nel tasso di disoccupazione Y vengono regredite sulle variazioni percentuali di PNL X. Tutti i dati sono rilevati trimestralmente. Le serie utilizzate sono: - https://fred.stlouisfed.org/series/UNRATE per il tasso di disoccupazione - https://www.nber.org/research/data/american-business-cycle-continuity-and-change-historic-data-tables per il PNL\n\n# Carica il dataset contenente il tasso di disoccupazione da un file CSV\nunemp = pd.read_csv('UNRATE_OKUN.csv')\n\n# Converte la colonna delle date in formato datetime per una gestione più efficace delle serie temporali\nunemp['observation_date'] = pd.to_datetime(unemp['observation_date'], format='%Y-%m-%d')\n\n# Imposta la colonna delle date come indice del DataFrame per lavorare con serie temporali\nunemp = unemp.set_index('observation_date')\n\n# Raggruppa i dati per trimestre calcolando la media del tasso di disoccupazione per ogni periodo\nunemp = unemp.groupby(pd.Grouper(freq='Q'))['UNRATE'].mean().to_frame()\n\n# Converte l'indice in un PeriodIndex con frequenza trimestrale per una gestione più accurata delle serie storiche\nunemp.index = pd.PeriodIndex(unemp.index, freq='Q')\n\n# Calcola la variazione trimestrale del tasso di disoccupazione\nunemp['change_unemp'] = unemp['UNRATE'].diff()\n\n# Rimuove eventuali valori NaN generati dalla differenziazione\nunemp = unemp.dropna()\n\n# Filtra i dati fino al quarto trimestre del 1960 per allinearsi al periodo di interesse\nunemp = unemp[unemp.index &lt;= '1960Q4']\n\n# Visualizza le prime righe del DataFrame risultante\nunemp.head()\n\n\n\n\n\n\n\n\nUNRATE\nchange_unemp\n\n\nobservation_date\n\n\n\n\n\n\n1948Q2\n3.666667\n-0.066667\n\n\n1948Q3\n3.766667\n0.100000\n\n\n1948Q4\n3.833333\n0.066667\n\n\n1949Q1\n4.666667\n0.833333\n\n\n1949Q2\n5.866667\n1.200000\n\n\n\n\n\n\n\n\n# Carica il dataset contenente il prodotto nazionale lordo (PNL) da un file CSV\ngnp = pd.read_csv('abcq.csv')\n\n# Crea una nuova colonna per la data, combinando l'anno e il trimestre\ngnp['observation_date'] = gnp['year'].astype(str) + 'Q' + gnp['quarter'].astype(str)\n\n# Imposta la colonna 'observation_date' come indice del DataFrame\ngnp = gnp.set_index('observation_date')\n\n# Converte l'indice in un PeriodIndex con frequenza trimestrale per una gestione più accurata delle serie storiche\ngnp.index = pd.PeriodIndex(gnp.index, freq='Q')\n\n# Seleziona solo la colonna 'GNP' e la converte in un DataFrame\ngnp = gnp['GNP'].to_frame()\n\n# Calcola la variazione percentuale trimestrale del prodotto nazionale lordo\ngnp['pct_change_gnp'] = gnp['GNP'].pct_change() * 100\n\n# Rimuove eventuali valori NaN generati dalla differenziazione\ngnp = gnp.dropna()\n\n# Filtra i dati per mantenere solo il periodo tra il secondo trimestre del 1948 e il quarto trimestre del 1960\ngnp = gnp[(gnp.index &lt;= '1960Q4') & (gnp.index &gt;= '1948Q2')]\n\n# Visualizza le prime righe del DataFrame risultante\ngnp.head()\n\n\n\n\n\n\n\n\nGNP\npct_change_gnp\n\n\nobservation_date\n\n\n\n\n\n\n1948Q2\n257.5\n3.000000\n\n\n1948Q3\n264.5\n2.718447\n\n\n1948Q4\n265.9\n0.529301\n\n\n1949Q1\n260.5\n-2.030839\n\n\n1949Q2\n257.0\n-1.343570\n\n\n\n\n\n\n\n\n# Unisce i DataFrame 'unemp' e 'gnp' utilizzando l'indice come chiave di unione\ndf = pd.merge(unemp, gnp, right_index=True, left_index=True)\n\n# Definisce la variabile indipendente X (le variazioni percentuali del PNL)\nX = df['pct_change_gnp']\n\n# Definisce la variabile dipendente Y (le variazioni del tasso di disoccupazione)\nY = df['change_unemp']\n\n# Aggiunge un termine costante al modello di regressione per tener conto dell'intercetta\nX = sm.add_constant(X)\n\n# Crea il modello di regressione lineare (OLS) con Y come dipendente e X come indipendente\nmodel = sm.OLS(Y, X)\n\n# Adatta il modello ai dati\nresults = model.fit()\n\n# Stampa i risultati della regressione\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.631\nModel:                            OLS   Adj. R-squared:                  0.623\nMethod:                 Least Squares   F-statistic:                     83.74\nDate:                Wed, 19 Mar 2025   Prob (F-statistic):           3.51e-12\nTime:                        10:30:06   Log-Likelihood:                -19.032\nNo. Observations:                  51   AIC:                             42.06\nDf Residuals:                      49   BIC:                             45.93\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              0.4238      0.065      6.546      0.000       0.294       0.554\npct_change_gnp    -0.2669      0.029     -9.151      0.000      -0.325      -0.208\n==============================================================================\nOmnibus:                        6.492   Durbin-Watson:                   1.585\nProb(Omnibus):                  0.039   Jarque-Bera (JB):                5.400\nSkew:                           0.713   Prob(JB):                       0.0672\nKurtosis:                       3.714   Cond. No.                         3.12\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nL’interpretazione dei coefficienti è immediata: tenendo fisso il PNL il tasso di disoccupazione aumenterà di .42 punti da un trimestre all’altro, in quanto la crescita della forza lavoro e i processi tecnologici spingeranno in basso il rapporto occupati/forza lavoro attiva. Per ogni aumento di 1 punto percentuale del PNL, il tasso di disoccupazione diminuirà di .27 punti. Di converso, un aumento del tasso di disoccupazione di 1 punto percentuale comporterà una riduzione del PNL del 3.7%.\nSi può notare che i valori ottenuti non sono dissimili da quelli dell’articolo originale, che riporta una approssimazione: \\[\n    Y = .3 - .3X\n\\] In questi casi, il metodo di rilevazione dei dati ed eventuali manipolazioni successive possono essere responsabili di leggere variazioni nei coefficienti.\n\n# Calcola le previsioni per la variazione del tasso di disoccupazione usando il modello di regressione\ndf['pred_unemp_change'] = results.predict(X)\n\n# Ordina i dati in base alla variazione percentuale del PNL (X)\nsorted_data = df.sort_values(by='pct_change_gnp')\n\n# Crea una figura con un solo subplot\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunge una linea orizzontale a y=0 per facilitare la lettura del grafico\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea un grafico a dispersione per visualizzare le osservazioni reali (variabile dipendente vs variabile indipendente)\nax.scatter(df['pct_change_gnp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Aggiunge la linea di regressione (legge di Okun) sul grafico\nax.plot(sorted_data['pct_change_gnp'], sorted_data['pred_unemp_change'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Imposta il titolo del grafico e lo allinea a sinistra\nax.set_title(f\"US, 1948-1960\", loc='left')\n\n# Imposta le etichette degli assi\nax.set_xlabel('Variazione Del PNL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunge la griglia al grafico per facilitare la lettura\nax.grid(True)\n\n# Aggiunge la legenda al grafico\nax.legend()\n\n# Ottimizza il layout del grafico per una visualizzazione migliore\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#ii.-elasticità",
    "href": "notebooks/La legge di Okun.html#ii.-elasticità",
    "title": "La legge di Okun",
    "section": "II. Elasticità",
    "text": "II. Elasticità\nQuesto metodo è molto importante dal punto di vista pratico, in quanto consente di associare dei coefficienti ai livelli di PNL e tasso di occupazione, piuttosto che alle loro variazioni percentuali. Questo servirà in seguito per stimare il PNL potenziale. In particolare, si ipotizza che:\n\nci sia elesticità costante (\\(a\\)) fra il rapporto tra prodotto osservato (\\(A\\)-actual) e potenziale (\\(P\\)) ed il rapporto fra tasso di occupazione (\\(N\\)) e tasso di occupazione potenziale (\\(N_F\\)). In simboli: \\(\\frac{N}{N_F} = \\left( \\frac{A}{P} \\right)^a\\)\nil prodotto potenziale cresca ad un tasso costante \\(r\\) in funzione del tempo: \\(P-t = P_0 e^{rt}\\)\n\nIl risultato restituisce l’equazione del modello, i cui parametri verranno stimati tramite OLS: \\[\\ln N_t = \\ln \\frac{N_F}{P_0^a} + a \\ln A_t - (ar)t\\]\n\n# Calcola il tasso di occupazione sottraendo il tasso di disoccupazione da 100\ndf['emprate'] = 100 - df['UNRATE']\n\n# Calcola il logaritmo naturale del tasso di occupazione\ndf['logN'] = np.log(df['emprate'])\n\n# Calcola il logaritmo naturale del Prodotto Nazionale Lordo (PNL)\ndf['logA'] = np.log(df['GNP'])\n\n# Crea una variabile trend che va da 1 fino al numero di osservazioni (usata per catturare l'effetto di trend nel modello)\ndf['trend'] = range(1, 1+len(df))\n\n# Mostra le prime righe del DataFrame risultante per una rapida verifica\ndf.head()\n\n\n\n\n\n\n\n\nUNRATE\nchange_unemp\nGNP\npct_change_gnp\npred_unemp_change\nemprate\nlogN\nlogA\ntrend\n\n\nobservation_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n1948Q2\n3.666667\n-0.066667\n257.5\n3.000000\n-0.376806\n96.333333\n4.567814\n5.551020\n1\n\n\n1948Q3\n3.766667\n0.100000\n264.5\n2.718447\n-0.301666\n96.233333\n4.566776\n5.577841\n2\n\n\n1948Q4\n3.833333\n0.066667\n265.9\n0.529301\n0.282564\n96.166667\n4.566083\n5.583120\n3\n\n\n1949Q1\n4.666667\n0.833333\n260.5\n-2.030839\n0.965804\n95.333333\n4.557380\n5.562603\n4\n\n\n1949Q2\n5.866667\n1.200000\n257.0\n-1.343570\n0.782388\n94.133333\n4.544712\n5.549076\n5\n\n\n\n\n\n\n\n\n# Definisce la variabile dipendente Y (logaritmo naturale del tasso di occupazione)\nY = df['logN']\n\n# Definisce la matrice delle variabili indipendenti X (logaritmo del PNL e trend)\nX = df[['logA', 'trend']]\n\n# Aggiunge una costante a X per includere l'intercetta nel modello\nX = sm.add_constant(X)\n\n# Crea un modello di regressione lineare ordinario (OLS) con Y come variabile dipendente e X come variabili indipendenti\nmodel = sm.OLS(Y,X)\n\n# Calcola i risultati del modello\nresults = model.fit()\n\n# Stampa il sommario dei risultati della regressione\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   logN   R-squared:                       0.854\nModel:                            OLS   Adj. R-squared:                  0.848\nMethod:                 Least Squares   F-statistic:                     140.6\nDate:                Wed, 19 Mar 2025   Prob (F-statistic):           8.53e-21\nTime:                        10:30:06   Log-Likelihood:                 196.88\nNo. Observations:                  51   AIC:                            -387.8\nDf Residuals:                      48   BIC:                            -382.0\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.9807      0.101     29.463      0.000       2.777       3.184\nlogA           0.2853      0.018     15.662      0.000       0.249       0.322\ntrend         -0.0043      0.000    -16.516      0.000      -0.005      -0.004\n==============================================================================\nOmnibus:                        3.874   Durbin-Watson:                   0.521\nProb(Omnibus):                  0.144   Jarque-Bera (JB):                3.453\nSkew:                          -0.636   Prob(JB):                        0.178\nKurtosis:                       2.930   Cond. No.                     4.24e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.24e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nDove il coefficiente di logA è l’elasticità dell’output al tasso di occupazione; il coefficiente del tempo è, per definizione, il prodotto tra il coefficiente di logA e il tasso di crescita costante del prodotto potenziale; l’intercetta restituisce il benchmark \\(P_0\\) per ogni livello di \\(N_F\\), che verrà preso pari a 96.\nIterate regressioni hanno portato Okun al seguente risultato: \\[\n    P = A \\left( 1+ .032 \\left( U-4 \\right) \\right)\n\\]\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Crea una nuova figura per il grafico 3D con dimensioni personalizzate\nfig = plt.figure(figsize=(10, 7))\n\n# Aggiunge un subplot 3D alla figura\nax = fig.add_subplot(111, projection='3d')\n\n# Crea un grafico scatter 3D con logA, trend e logN, etichettato come 'Osservazioni'\nax.scatter(df['logA'], df['trend'], df['logN'], label='Osservazioni', alpha=0.7)\n\n# Definisce i valori di intervallo per logA e trend (in modo da poter creare una superficie)\nlogA_range = np.linspace(df['logA'].min(), df['logA'].max(), 20)\ntrend_range = np.linspace(df['trend'].min(), df['trend'].max(), 20)\n\n# Crea una griglia di valori per logA e trend\nlogA_grid, trend_grid = np.meshgrid(logA_range, trend_range)\n\n# Calcola i valori adattati della superficie utilizzando i parametri stimati dal modello di regressione\nfitted_values = results.params['const'] + results.params['logA'] * logA_grid + results.params['trend'] * trend_grid\n\n# Aggiunge la superficie al grafico 3D, colorandola con un colore specifico e un'alpha di trasparenza\nax.plot_surface(logA_grid, trend_grid, fitted_values, color=bmh_colors[1], alpha=0.5)\n\n# Imposta le etichette degli assi\nax.set_xlabel('logA')\nax.set_ylabel('Trend')\nax.set_zlabel('logN')\n\n# Imposta il titolo del grafico\nax.set_title('Legge Di Okun, livelli')\n\n# Modifica l'angolazione della visualizzazione per rendere più chiara la superficie\nax.view_init(elev=20, azim=45)\n\n# Aggiunge la legenda al grafico\nplt.legend()\n\n# Mostra il grafico\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcola il PNL potenziale utilizzando la formula di Okun (assumendo un tasso di disoccupazione ottimale del 4%)\ndf['potential_gnp'] = df['GNP']*(1+.032*(df['UNRATE']-4))\n\n# Crea un grafico con un subplot\nfig, ax = plt.subplots(1,1)\n\n# Filtra i dati dal primo trimestre del 1954 e traccia il PNL osservato\ndf[df.index&gt;='1954Q1']['GNP'].plot(label='PNL', ax=ax)\n\n# Filtra i dati dal primo trimestre del 1954 e traccia il PNL potenziale calcolato\ndf[df.index&gt;='1954Q1']['potential_gnp'].plot(label='PNL potenziale', ax=ax)\n\n# Aggiunge etichette agli assi\nplt.ylabel('PNL (in miliardi di dollari)')\nplt.xlabel('Trimestre')\n\n# Aggiunge il titolo al grafico, posizionandolo a sinistra\nplt.title('Calcolo Del PNL Potenziale Secondo Okun', loc='left')\n\n# Mostra la legenda per distinguere le due linee\nplt.legend()\n\n# Regola la disposizione del layout per ottimizzare lo spazio\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#e-oggi",
    "href": "notebooks/La legge di Okun.html#e-oggi",
    "title": "La legge di Okun",
    "section": "E oggi?",
    "text": "E oggi?\n\n# Funzione per estrarre i dati dal datawarehouse della BCE\ndef make_df_ECB(key, obs_name):\n    \"\"\"Estrae i dati dal datawarehouse della BCE\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'  # URL di base per il servizio web della BCE\n    format_ = '?format=csvdata'  # Impostazione del formato dei dati in CSV\n    df = pd.read_csv(url_+key+format_)  # Legge i dati CSV dal servizio web BCE usando la chiave\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]  # Seleziona le colonne di interesse (periodo e valore osservato)\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n    df = df.set_index('TIME_PERIOD')  # Imposta il periodo come indice del dataframe\n    df.columns = [obs_name]  # Rinomina la colonna con il nome della variabile\n    return df\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp = make_df_ECB(unemp_rate_key, 'unemp')  # Ottieni i dati dal database ECB\nunemp['change_unemp'] = unemp['unemp'].diff()  # Calcola la variazione del tasso di disoccupazione\n\n# Chiave per il PIL (GDP)\ngdp_key = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'  # Codice identificativo del PIL\ngdp = make_df_ECB(gdp_key, 'gdp')  # Ottieni i dati dal database ECB\ngdp['pct_change_gdp'] = gdp['gdp'].pct_change()*100  # Calcola la variazione percentuale del PIL\n\n# Unisci i dati del tasso di disoccupazione e del PIL\ndf = pd.merge(unemp, gdp, right_index=True, left_index=True)\ndf = df.dropna()  # Rimuove le righe con valori NaN\n\n# Mostra le prime righe del dataframe risultante\ndf.head()\n\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_8352\\1158864950.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_8352\\1158864950.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n\n\n\n\n\n\n\n\n\nunemp\nchange_unemp\ngdp\npct_change_gdp\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n2000-04-01\n9.128157\n-0.223128\n2.433026e+06\n0.896372\n\n\n2000-07-01\n8.982247\n-0.145910\n2.449339e+06\n0.670484\n\n\n2000-10-01\n8.751499\n-0.230748\n2.460466e+06\n0.454298\n\n\n2001-01-01\n8.532471\n-0.219028\n2.487753e+06\n1.109010\n\n\n2001-04-01\n8.472304\n-0.060167\n2.489155e+06\n0.056341\n\n\n\n\n\n\n\n\n# Definizione delle variabili indipendente (X) e dipendente (Y)\nX = df['pct_change_gdp']  # Variabile indipendente: variazione percentuale del PIL\nY = df['change_unemp']  # Variabile dipendente: variazione del tasso di disoccupazione\n\n# Aggiunta dell'intercetta (costante) al modello\nX = sm.add_constant(X)  # Aggiunge una colonna di 1 alla variabile indipendente per includere l'intercetta\n\n# Creazione del modello di regressione lineare (OLS)\nmodel = sm.OLS(Y, X)  # OLS: Ordinary Least Squares (minimi quadrati ordinari)\n\n# Fitting del modello (stima dei parametri)\nresults = model.fit()  # Calcola i risultati del modello di regressione\n\n# Stampa del sommario dei risultati\nprint(results.summary())  # Visualizza un riepilogo completo dei risultati della regressione\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                 -0.005\nMethod:                 Least Squares   F-statistic:                    0.5490\nDate:                Wed, 19 Mar 2025   Prob (F-statistic):              0.460\nTime:                        10:30:07   Log-Likelihood:                 2.0828\nNo. Observations:                  99   AIC:                           -0.1656\nDf Residuals:                      97   BIC:                             5.025\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst             -0.0286      0.024     -1.170      0.245      -0.077       0.020\npct_change_gdp    -0.0101      0.014     -0.741      0.460      -0.037       0.017\n==============================================================================\nOmnibus:                       42.392   Durbin-Watson:                   0.743\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              109.819\nSkew:                           1.560   Prob(JB):                     1.42e-24\nKurtosis:                       7.110   Cond. No.                         1.84\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIl modello non passa il test di zero-slopes, e nessun coefficiente è significativo. Il modello mostra autocorrelazione positiva (il limite inferiore di DW è intorno a 1.5). come si può constatare dal seguente grafico, la relazione tra variazione nel tasso di disoccupazione e variazione percentuale del PIL è molto debole in Europa, nel periodo considerato.\n\n# Calcolo delle previsioni della variazione del tasso di disoccupazione\ndf['pred_unemp_change'] = results.predict(X)\n\n# Ordinamento dei dati per la variabile indipendente (PIL)\nsorted_data = df.sort_values(by='pct_change_gdp')\n\n# Creazione della figura e dell'asse per il grafico\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunta di una linea orizzontale a y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Creazione del grafico a dispersione (scatter plot) per le osservazioni effettive\nax.scatter(df['pct_change_gdp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Aggiunta della retta di regressione (Legge di Okun)\nax.plot(sorted_data['pct_change_gdp'], sorted_data['pred_unemp_change'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Titolo e etichette degli assi\nax.set_title(f\"EU, 2000-2024\", loc='left')\nax.set_xlabel('Variazione Del PIL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunta di una griglia al grafico\nax.grid(True)\n\n# Aggiunta della legenda\nax.legend()\n\n# Ottimizzazione della disposizione del grafico\nplt.tight_layout()\n\n\n\n\n\n\n\n\nCome esercizio, ho replicato il modello aggiungendo i lag di GDP e disoccupazione (come suggerito dal test di DW) - il modello ora riporta un elevato \\(R^2\\) e coefficienti significativi, tuttavia ha perso il sapore caratteristico dell’originale curva di Okun.\n\n# Creazione delle variabili laggate per la variazione del PIL e per la disoccupazione\ndf['pct_change_gdp_lag1'] = df['pct_change_gdp'].shift(1)\ndf['unemp_lag1'] = df['change_unemp'].shift(1)\n\n# Rimozione dei valori nulli generati dalla creazione dei lag\ndf = df.dropna()\n\n# Creazione delle variabili indipendenti (incluso il termine costante)\nX = df[['pct_change_gdp', 'pct_change_gdp_lag1', 'unemp_lag1']]\nX = sm.add_constant(X)\n\n# Variabile dipendente (cambio del tasso di disoccupazione)\nY = df['change_unemp']\n\n# Creazione del modello di regressione dinamica con stima robusta degli errori (HAC)\nmodel_dyn = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 4})\n\n# Visualizzazione dei risultati del modello\nprint(model_dyn.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.760\nModel:                            OLS   Adj. R-squared:                  0.752\nMethod:                 Least Squares   F-statistic:                     272.3\nDate:                Wed, 19 Mar 2025   Prob (F-statistic):           3.23e-46\nTime:                        10:30:08   Log-Likelihood:                 71.457\nNo. Observations:                  98   AIC:                            -134.9\nDf Residuals:                      94   BIC:                            -124.6\nDf Model:                           3                                         \nCovariance Type:                  HAC                                         \n=======================================================================================\n                          coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.0219      0.015      1.476      0.140      -0.007       0.051\npct_change_gdp         -0.0260      0.012     -2.166      0.030      -0.050      -0.002\npct_change_gdp_lag1    -0.0766      0.005    -16.759      0.000      -0.086      -0.068\nunemp_lag1              0.6233      0.042     14.905      0.000       0.541       0.705\n==============================================================================\nOmnibus:                        5.478   Durbin-Watson:                   2.021\nProb(Omnibus):                  0.065   Jarque-Bera (JB):                6.170\nSkew:                           0.289   Prob(JB):                       0.0457\nKurtosis:                       4.085   Cond. No.                         8.21\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 4 lags and without small sample correction\n\n\n\n# Previsione dei cambiamenti nel tasso di disoccupazione usando il modello dinamico\ndf['pred_unemp_change_2'] = model_dyn.predict(X)\n\n# Ordinamento dei dati in base alla variazione del PIL\nsorted_data = df.sort_values(by='pct_change_gdp')\n\n# Creazione del grafico\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunta della linea orizzontale per il valore zero\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Creazione dello scatter plot per le osservazioni\nax.scatter(df['pct_change_gdp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Creazione della linea della Legge di Okun basata sulle previsioni\nax.scatter(sorted_data['pct_change_gdp'], sorted_data['pred_unemp_change_2'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Titolo del grafico\nax.set_title(f\"EU, 2000-2024\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Variazione Del PIL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunta della griglia\nax.grid(True)\n\n# Aggiunta della legenda\nax.legend()\n\n# Ottimizzazione del layout\nplt.tight_layout()"
  },
  {
    "objectID": "macroeconomics.html",
    "href": "macroeconomics.html",
    "title": "Macroeconomics",
    "section": "",
    "text": "This section contains various resources related to macroeconomics and econometrics, including articles, research, interactive notebooks and other useful resources."
  },
  {
    "objectID": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "href": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "title": "Macroeconomics",
    "section": "Corso introduttivo di Macroeconomia con Python",
    "text": "Corso introduttivo di Macroeconomia con Python\n\nLe variabili macroeconomiche\nA notebook (in Italian) exploring macroeconomic variables and their applications.\nLa curva di Phillips\nA notebook (in Italian) exploring the relationship between inflation and unemployment\nLa legge di Okun\nA notebook (in Italian) exploring the relationship between unemployment and output"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This section provides a collection of resources on data analysis, including notebooks and practical insights drawn from my professional experience."
  },
  {
    "objectID": "data-analysis.html#marketing",
    "href": "data-analysis.html#marketing",
    "title": "Data Analysis",
    "section": "Marketing",
    "text": "Marketing\n\nUsers Cohorts Analysis\nUnderstanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention engagement, and growth is through cohort analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "&lt;h1&gt;Your Name&lt;/h1&gt;\n&lt;p&gt;I'm a macroeconomics researcher with a passion for data, finance, and programming. Currently pursuing my MSc at LSE, I explore economic crises, nonlinear models, and financial stability. I also enjoy endurance sports and mountaineering.&lt;/p&gt;"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "BSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\nLeonardo Assicurazioni, Milan\nLeonardo Assicurazioni is a leading insurance distributor with a robust network of over 400 consultants based in Milan. After three years in technical roles, I founded and led the company’s data analytics office for five years, driving key insights and innovations.\n\nData Analyst\n\nLed and trained a team of three, ensuring the successful delivery of key data projects that enhanced business operations and decision-making.\nConsolidated and structured raw data from multiple sources and formats (SQL, JSON, legacy systems).\nIncreased revenue by developing unsupervised clustering models for client segmentation and supervised classification models (KNN, Decision Tree, Logistic Regression) to optimize sales campaign efficiency.\nReduced costs by building classification models to prevent churn and insolvencies and collaborating with the quality control department to develop fraud-prevention models.\nWorked with HR to design and analyze employee surveys, extracting insights and developing predictive models to reduce turnover.\nConducted in-depth sales data analysis using Python, generating actionable insights and presenting findings with Matplotlib visualizations.\nBuilt over 50 Power BI dashboards from scratch in close collaboration with top management to support decision-making.\nDeveloped interactive web-based data visualizations with D3.js, allowing stakeholders to explore complex datasets intuitively on portable devices.\nCreated structured reports and presentations in Excel and PowerPoint to effectively communicate insights to senior leadership.\nServed as a translator in key meetings with foreign investors and presented at large conferences.\nOrganized and monitored sales competitions in close collaboration with the sales director.\nAutomated key processes by working with the IT team to develop custom VBA applications, significantly reducing manual effort and improving efficiency.\n\n\n\nUnderwriter and Sales Support Specialist\n\nManaged back-office and control activities, ensuring efficient operations\nStreamlined administrative workflows, reducing processing times and improving efficiency\nGained deep knowledge of insurance products and the market through one year of experience in the underwriting office."
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "CV",
    "section": "Certificates",
    "text": "Certificates\n\nGRE\n\n\n\nOverall 336\n\n\n\n\nQuant 170\n\n\n\n\nVerbal 166\n\n\n\n\n\nIELTS\n\n\n\nOverall 8/9\n\n\n\n\nReading 9/9\n\n\n\n\nListening 9/9\n\n\n\n\nWriting 7.5/9\n\n\n\n\nSpeaking 7/9"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Marco Bernardini",
    "section": "Education",
    "text": "Education\nUniversità Bicocca, Milano (IT)\nBSc in Economics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Marco Bernardini",
    "section": "Experience",
    "text": "Experience\nSDG Group, Milano (IT)\nConsultant\nLeonardo Assicurazioni, Milano (IT)\nLead Data Analyst"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html",
    "href": "notebooks/La curva di Phillips.html",
    "title": "La curva di Phillips",
    "section": "",
    "text": "Nel finire degli anni 50 l’economista della LSE A.W.Phillips osservò una relazione inversa fra disoccupazione e inflazione salariale, ovvero il tasso di crescita dei salari nominali, nell’economia britannica. Pochi anni dopo, Samuelson e Solow mostrarono che una simile relazione esisteva anche nell’economia americana, legando non più la disoccupazione solamente all’inflazione salariale, ma al tasso di crescita generale dei prezzi. La curva divenne così uno strumento utilizzato dai policy makers che, dopo aver valutato il trade-off tra inflazione e disoccupazione, mettevano in atto strategie atte a raggiungere un punto specifico della curva.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport warnings\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n#     \"axes.grid\": False,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "href": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "title": "La curva di Phillips",
    "section": "Paper di Phillips",
    "text": "Paper di Phillips\nIn questa sezione, replicheremo l’articolo originale di Phillips, utilizzando i dati forniti dalla Bank Of England (https://www.bankofengland.co.uk/statistics/research-datasets)\n\n# Importa i dati sulla disoccupazione dal file Excel contenente dati macroeconomici del Regno Unito\nunemployment = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                             sheet_name='A50. Employment & unemployment', \n                             skiprows=2, usecols='A,J')\n\n# Rimuove le prime due righe che contengono informazioni non necessarie\nunemployment = unemployment[2:]\n\n# Rinomina la colonna non etichettata in 'Year' per maggiore chiarezza\nunemployment.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n\n# Imposta l'anno come indice del DataFrame\nunemployment.set_index('Year', inplace=True)\n\n# Converte la colonna del tasso di disoccupazione in formato numerico (float)\nunemployment['Unemployment rate'] = unemployment['Unemployment rate'].astype(float)\n\n# Mostra le prime righe del DataFrame per verificare la corretta importazione dei dati\nunemployment.head()\n\n\n\n\n\n\n\n\nUnemployment rate\n\n\nYear\n\n\n\n\n\n1855\n3.731877\n\n\n1856\n3.518180\n\n\n1857\n3.945176\n\n\n1858\n5.215345\n\n\n1859\n3.276380\n\n\n\n\n\n\n\n\n# Importa i dati sui salari e l'inflazione dal file Excel contenente dati macroeconomici del Regno Unito\nwages_inflation = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                                sheet_name='A47. Wages and prices', \n                                usecols='A,BD')\n\n# Rinomina le colonne per una maggiore chiarezza\nwages_inflation.columns = ['Year', 'Wage index']\n\n# Rimuove le prime sei righe non necessarie\nwages_inflation = wages_inflation[6:]\n\n# Converte la colonna 'Year' in formato intero\nwages_inflation['Year'] = wages_inflation['Year'].astype(int)\n\n# Converte la colonna 'Wage index' in formato numerico (float)\nwages_inflation['Wage index'] = wages_inflation['Wage index'].astype(float)\n\n# Filtra i dati per includere solo gli anni a partire dal 1855\nwages_inflation = wages_inflation[wages_inflation['Year'] &gt;= 1855]\n\n# Imposta l'anno come indice del DataFrame\nwages_inflation.set_index('Year', inplace=True)\n\n# Calcola la differenza centrale prima (approssimazione numerica della derivata prima) e la esprime in percentuale.\n# Questa è la misura utilizzata nell'articolo originale, qui utilizzeremo invece la variazione percentuale\nwages_inflation['First central difference'] = (\n    (wages_inflation['Wage index'].shift(-1) - wages_inflation['Wage index'].shift(1)) / \n    (2 * wages_inflation['Wage index'])\n) * 100\n\n# Calcola la variazione percentuale anno su anno del Wage index\nwages_inflation['Percent change'] = wages_inflation['Wage index'].pct_change() * 100\n\n# Mostra le prime righe del DataFrame per verificare la corretta elaborazione dei dati\nwages_inflation.head()\n\n\n\n\n\n\n\n\nWage index\nFirst central difference\nPercent change\n\n\nYear\n\n\n\n\n\n\n\n1855\n59.0\nNaN\nNaN\n\n\n1856\n59.0\n-1.694915\n0.000000\n\n\n1857\n57.0\n-2.631579\n-3.389831\n\n\n1858\n56.0\n0.000000\n-1.754386\n\n\n1859\n57.0\n1.754386\n1.785714\n\n\n\n\n\n\n\n\n# Unisce i DataFrame 'unemployment' e 'wages_inflation' utilizzando l'indice (Year) e rimuove eventuali valori NaN\ndf = pd.merge(unemployment, wages_inflation, right_index=True, left_index=True).dropna()\n\n# Suddivide il dataset in tre periodi storici distinti per analizzare la relazione tra disoccupazione e inflazione salariale\ndfA = df[df.index &lt;= 1913].copy()       # Periodo pre-1913\ndfB = df[(df.index &gt;= 1913) & (df.index &lt;= 1948)].copy()  # 1913-1948 (tra le due guerre)\ndfC = df[(df.index &gt;= 1948) & (df.index &lt;= 1957)].copy()  # 1948-1957 (primi anni post-bellici)\n\n# Definisce le etichette temporali per i grafici\ndates = ['1861-1913', '1913-1948', '1948-1957']\n\n# Crea una figura con tre subplot affiancati per rappresentare ciascun periodo\nfig, ax = plt.subplots(1, 3, figsize=(10, 3.5))\n\n# Itera sui tre sotto-dataset per stimare e tracciare la curva di Phillips per ciascun periodo\nfor i, _ in enumerate([dfA, dfB, dfC]):\n    \n    # Calcola il termine di aggiustamento per evitare problemi con il logaritmo\n    a = abs(_[\"Percent change\"].min()) + 0.9\n    \n    # Trasforma in logaritmo il tasso di inflazione salariale e il tasso di disoccupazione\n    _[\"Log wage inflation rate\"] = np.log(_[\"Percent change\"] + a)\n    _[\"Log unemployment rate\"] = np.log(_[\"Unemployment rate\"])\n    \n    # Definisce le variabili per la regressione lineare (modello di Phillips)\n    X = _[\"Log unemployment rate\"]\n    Y = _[\"Log wage inflation rate\"]\n    X = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\n    model = sm.OLS(Y, X).fit()  # Stima il modello di regressione\n    R2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n    \n    # Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\n    _[\"Predicted log wage inflation rate\"] = model.predict(X)\n    _[\"Predicted wage inflation rate\"] = np.exp(_[\"Predicted log wage inflation rate\"]) - a\n    \n    # Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\n    sorted_data = _.sort_values(by='Unemployment rate')\n    \n    # Disegna una linea orizzontale per rappresentare l'asse y=0\n    ax[i].axhline(y=0, color='silver', linewidth=2)\n    \n    # Crea uno scatter plot con i dati osservati\n    ax[i].scatter(_['Unemployment rate'], _['Percent change'], alpha=0.7, \n                  label=\"Osservazioni\", color=bmh_colors[0])\n    \n    # Disegna la curva di Phillips stimata dalla regressione\n    ax[i].plot(sorted_data['Unemployment rate'], sorted_data['Predicted wage inflation rate'], \n               color=bmh_colors[1], label=\"Curva Di Phillips\")\n    \n    # Imposta il titolo per ciascun sottografico\n    ax[i].set_title(f\"UK, {dates[i]}\", loc='left')\n    \n    # Etichette degli assi\n    ax[i].set_xlabel('Tasso Di Disoccupazione')\n    ax[i].set_ylabel('Tasso Di Inflazione Salariale')\n    \n    # Attiva la griglia\n    ax[i].grid(True)\n    \n    # Aggiunge la legenda\n    ax[i].legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "href": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "title": "La curva di Phillips",
    "section": "Paper di Samuelson e Solow",
    "text": "Paper di Samuelson e Solow\nIn questa sezione, cercheremo di analizzare il fenomeno all’interno dell’economia Americana, per la quale i dati sono purtroppo frammentari (si vedano https://data.bls.gov/pdq/SurveyOutputServlet e https://fred.stlouisfed.org/series/M08142USM055NNBR)\n\n# Carica i dati sulla disoccupazione (BLS) dal file Excel, saltando le prime 16 righe e impostando 'Year' come indice\nunemp = pd.read_excel('BLS1.xlsx', skiprows=16).set_index('Year')\n\n# Carica i dati sulle retribuzioni orarie medie da 25 industrie manifatturiere degli Stati Uniti (NBER)\nearnings = pd.read_csv('Average Hourly Earnings, Twenty-Five Manufacturing Industries for United States.csv').set_index('observation_date')\n\n# Converte l'indice di 'earnings' in formato datetime (anno-mese-giorno)\nearnings.index = pd.to_datetime(earnings.index, format='%Y-%m-%d')\n\n# Raggruppa i dati sulle retribuzioni annualmente, calcolando la media per ogni anno\nearnings = earnings.groupby(pd.Grouper(freq='Y')).mean()\n\n# Estrae solo l'anno dall'indice per facilitarne l'elaborazione\nearnings.index = earnings.index.year\n\n# Filtra i dati per gli anni dal 1929 al 1947\nearnings = earnings[(earnings.index &gt;= 1929) & (earnings.index &lt;= 1947)]\n\n# Rinomina la colonna per avere una denominazione chiara\nearnings.columns = ['HOURLY_WAGE']\n\n# Rinomina la colonna della disoccupazione per chiarezza\nunemp.columns = ['UNEMPLOYMENT']\n\n# Unisce i due DataFrame sui rispettivi indici (anno)\ndf = pd.merge(unemp, earnings, right_index=True, left_index=True)\n\n# Calcola la variazione percentuale annuale delle retribuzioni orarie\ndf['PCT_CHANGE'] = df['HOURLY_WAGE'].pct_change() * 100\n\n# Rimuove le righe con valori mancanti (NaN) creati durante il calcolo della variazione percentuale\ndf = df.dropna()\n\n# Mostra le prime righe del DataFrame per verificare i dati\ndf.head()\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n\n\n\n\n\n\n\n\n\nUNEMPLOYMENT\nHOURLY_WAGE\nPCT_CHANGE\n\n\nYear\n\n\n\n\n\n\n\n1930\n8.7\n0.589000\n-0.113058\n\n\n1931\n15.9\n0.563833\n-4.272779\n\n\n1932\n23.6\n0.497583\n-11.749926\n\n\n1933\n24.9\n0.490583\n-1.406800\n\n\n1934\n21.7\n0.579750\n18.175641\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"PCT_CHANGE\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"PCT_CHANGE\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"UNEMPLOYMENT\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='UNEMPLOYMENT')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['UNEMPLOYMENT'], df['PCT_CHANGE'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['UNEMPLOYMENT'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Curva Di Phillips\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"US, 1930-1947\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Tasso Di Disoccupazione')\nax.set_ylabel('Tasso Di Inflazione Salariale')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#e-oggi",
    "href": "notebooks/La curva di Phillips.html#e-oggi",
    "title": "La curva di Phillips",
    "section": "E oggi?",
    "text": "E oggi?\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"Estrae i dati dal datawarehouse della BCE\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'  # URL di base per il servizio web della BCE\n    format_ = '?format=csvdata'  # Impostazione del formato dei dati in CSV\n    df = pd.read_csv(url_+key+format_)  # Legge i dati CSV dal servizio web BCE usando la chiave\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]  # Seleziona le colonne di interesse (periodo e valore osservato)\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n    df = df.set_index('TIME_PERIOD')  # Imposta il periodo come indice del dataframe\n    df.columns = [obs_name]  # Rinomina la colonna con il nome della variabile\n    return df\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Chiave per la wage inflation (inflazione salariale)\nwage_inflation_key = 'MNA/Q.Y.I9.W2.S1.S1._Z.COM_PS._Z._T._Z.IX.V.N'  # Codice identificativo del salario per addetto (indice)\nwage_inflation = make_df_ECB(wage_inflation_key, 'Compensation per employee')  # Ottieni i dati dal database ECB\n\nwage_inflation = wage_inflation[wage_inflation.index &gt;= '2000-01-01']  # Filtra i dati dal 2000 in poi\n\n# Combina i dati di disoccupazione e salario in un unico dataframe\ndf = pd.merge(unemp_rate, wage_inflation, right_index=True, left_index=True)\n\n# Raggruppa i dati per anno e calcola la media per il tasso di disoccupazione e l'ultimo valore per la compensazione per addetto\ndf = df.groupby(pd.Grouper(freq='Y'))[['Unemployment Rate', 'Compensation per employee']].agg({'Unemployment Rate':'mean',\n                                                                                             'Compensation per employee':'last'})\n\ndf.index = df.index.year  # Imposta l'indice come anno\ndf['Wage inflation rate'] = df['Compensation per employee'].pct_change()*100  # Calcola il tasso di inflazione salariale come variazione percentuale\ndf = df.dropna()  # Rimuove i valori mancanti\n\ndf.head()  # Mostra le prime righe del dataframe\n\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_12692\\1025282998.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\nC:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_12692\\1025282998.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n\n\n\n\n\n\n\n\n\nUnemployment Rate\nCompensation per employee\nWage inflation rate\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n2001\n8.508061\n70.332653\n2.705440\n\n\n2002\n8.753020\n72.127430\n2.551840\n\n\n2003\n9.188271\n73.757671\n2.260224\n\n\n2004\n9.399281\n75.327356\n2.128164\n\n\n2005\n9.231180\n77.193789\n2.477762\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"Wage inflation rate\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"Wage inflation rate\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"Unemployment Rate\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='Unemployment Rate')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['Unemployment Rate'], df['Wage inflation rate'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['Unemployment Rate'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Curva Di Phillips\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"EU, 2001-2024\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Tasso Di Disoccupazione')\nax.set_ylabel('Tasso Di Inflazione Salariale')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html",
    "href": "notebooks/Le variabili macroeconomiche.html",
    "title": "Le variabili macroeconomiche",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport math\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.colors as mcolors\nfrom matplotlib import font_manager\nimport datetime\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nfrom scipy.optimize import curve_fit\nimport eurostat\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfont_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\Inter-VariableFont_opsz,wght.ttf\"\nfont_manager.fontManager.addfont(font_path)\nbold_font_path = \"C:\\\\Users\\\\Marco\\\\Downloads\\\\Inter\\\\static\\\\Inter_18pt-Bold.ttf\"\nfont_manager.fontManager.addfont(bold_font_path)\n\nplt.style.use('bmh')\nbmh_colors = ['#348ABD', '#a60628']\n\n\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    \n    'font.family': 'Inter',\n    'axes.titleweight': 'bold',\n    'axes.titlepad': 10,\n    'axes.labelsize': 10,\n    'axes.titlesize': 14,\n    'axes.titlecolor': '#525252',\n    'axes.labelcolor': '#525252',\n    'figure.titlesize': 18,\n    'figure.titleweight': 'bold',\n    \"lines.linewidth\": 2,\n    \n    'legend.fontsize': 10,\n    'legend.facecolor': 'white',\n    \"legend.frameon\": False,\n    \n    \"axes.linewidth\": 1.5,\n    \n    \"xtick.color\": \"silver\",\n    \"ytick.color\": \"silver\",\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    \"xtick.major.width\": 1.5,\n    \"ytick.major.width\": 1.5,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": True,\n    \"axes.spines.bottom\": True,\n#     \"axes.grid\": False,\n    \"grid.alpha\": 0.2,\n    \"xtick.direction\":\"out\",\n    \"ytick.direction\":\"out\",\n    \"xtick.bottom\":False,\n    \"ytick.left\":False,\n})\n\n%matplotlib inline\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"extracts data from the BCE datawarehouse\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'\n    format_ = '?format=csvdata'\n    df = pd.read_csv(url_+key+format_)\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n    df = df.set_index('TIME_PERIOD')\n    df.columns = [obs_name]\n    return df"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "href": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "title": "Le variabili macroeconomiche",
    "section": "Produzione Aggregata",
    "text": "Produzione Aggregata\n\nDefinizione\nLa misura della produzione aggregata nella contabilità nazionale è chiamata prodotto interno lordo, o PIL. Il PIL può essere definito in tre modi equivalenti:\n\n\nIl PIL è il valore dei beni e dei servizi finali prodotti nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma del valore aggiunto nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma dei redditi dell’economia in un dato periodo di tempo\n\n\nDalla definizione (3) deriva che, in una economia, produzione aggregata e reddito aggregato siano sempre uguali\n\n\nCome si misura il PIL?\nNormalmente, le informazioni sono raccolte dalle autorità fiscali di un paese:\n\n\nLe imprese registrano le proprie vendite\n\n\nLe imprese pagano tasse sul valore aggiunto\n\n\nGli individui dichiarano il reddito percepito\n\n\n\n\nPIL nominale e PIL reale\nIl PIL nominale è la somma del valore dei beni finali valutati al loro prezzo corrente, dunque la sua crescita è influenzata da I. l’aumento della produzione II. l’aumento dei prezzi. Il PIl reale è la somma del valore dei beni finali valutati a prezzi costanti: di solito si utilizzano i prezzi di un anno di riferimento\n\n\nPIL pro capite\nSpesso, per misurare il tenore di vita in un paese, si utilizza il PIL pro capite, ovvero il PIL reale diviso per la popolazione del paese\n\n# Creazione del DataFrame con i dati del PIL dell'Area Euro\n# La chiave specificata corrisponde al PIL reale trimestrale in euro a prezzi concatenati\nPIL_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'\npil = make_df_ECB(PIL_KEY, 'PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie utilizzata rappresenta il PIL reale dell'Area Euro con dati trimestrali (Q).\nI valori sono espressi in miliardi di euro (EUR) a prezzi concatenati (LR), il che significa\nche sono corretti per l'inflazione e consentono un confronto diretto nel tempo.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse: \n  - Crisi del debito europeo (2009) con una significativa contrazione del PIL.\n  - Pandemia di Covid-19 (2020) con una forte caduta seguita da una ripresa.\n\"\"\"\n\n# Creazione del grafico del PIL\npil.plot()\nplt.title('PIL Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('PIL (Miliardi di euro, prezzi concatenati)')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', pil.loc['2009-01-01']), \n             xytext=(-20, 60),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', pil.loc['2020-04-01']), \n             xytext=(-40, -40),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTasso di crescita del PIL\nSpesso, si utilizza il concetto di crescita del PIL per indicare espansioni (periodi di crescita positiva) e recessioni (almeno due trimestri consecutivi di crescita negativa). Il tasso di crescita del PIL è semplicemente definito come: $ Y_t - Y_{t-1} $\n\n# Creazione del DataFrame con i dati della crescita del PIL reale\n# La chiave specificata corrisponde alla crescita del PIL reale trimestrale in termini percentuali\nREAL_GDP_GROWTH_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.GY'\nreal_gdp_growth = make_df_ECB(REAL_GDP_GROWTH_KEY, 'Crescita PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie rappresenta la crescita percentuale del PIL reale dell'Area Euro con dati trimestrali (Q).\nMisura il tasso di variazione del PIL rispetto al trimestre precedente, esprimendolo in termini percentuali.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Indicatore: Crescita percentuale del PIL reale (GY)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse:\n  - Crisi del debito europeo (2009) con crescita negativa e recessione prolungata.\n  - Pandemia di Covid-19 (2020) con una caduta storica seguita da un rimbalzo.\n\"\"\"\n\n# Creazione del grafico della crescita del PIL\nreal_gdp_growth.plot()\nplt.title('Crescita del PIL nell\\'Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('Crescita %')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Aggiunta di una linea orizzontale per evidenziare il valore zero (assenza di crescita)\nplt.axhline(y=0, color='#525252', linestyle='--', linewidth=1)\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', real_gdp_growth.loc['2009-01-01']), \n             xytext=(-100, -30),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', real_gdp_growth.loc['2020-04-01']), \n             xytext=(-150, 10),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLa composizione del PIL\nPer comprendere cosa determini la domanda di beni, si può scomporre la produzione aggregata in base alla tipologia di acquirente dei beni. Si possono individuare cinque componenti:\n\n\nIl consumo (C), ovvero beni e servizi acquistati dai consumatori, ovvero le famiglie\n\n\nL’investimento (I) che include l’investimento non residenziale (impianti e macchinari) da parte delle aziende, e l’investimento residenziale da parte delle famiglie\n\n\nLa spesa pubblica in beni e servizi (G), ovvero i beni acquistati dallo stato e dagli enti pubblici. Si noti che non fanno parte di questa categoria pensioni, servizi assistenziali quali il reddito di cittadinanza, o gli interessi pagati sul debito.\n\n\nImportazioni (IM), che entrano nell’equazione con segno negativo\n\n\nEsportazioni (X), che entrano nell’equazione con segno positivo\n\n\nSi ha dunque che: \\(Y = C + I + G + (X - IM)\\) Dove spesso si usa parlare di esportazioni nette o saldo commerciale del termine \\(X - IM\\).\n\n# Definisco le chiavi per i vari indicatori economici relativi all'Italia\n# La struttura di ogni chiave segue il formato: \n# MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N\n\nIT_gdp_key = 'MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N'  # PIL (Prodotto Interno Lordo)\n\nIT_inv_key = 'MNA/Q.Y.IT.W0.S1.S1.D.P51G.N11G._T._Z.EUR.V.N'  # Investimenti\n\nIT_cons_key = 'MNA/Q.Y.IT.W0.S1M.S1.D.P31._Z._Z._T.EUR.V.N'  # Consumi\n\nIT_gov_exp_key = 'MNA/Q.Y.IT.W0.S13.S1.D.P3._Z._Z._T.EUR.V.N'  # Spesa Pubblica\n\nIT_imp_key = 'MNA/Q.Y.IT.W1.S1.S1.C.P7._Z._Z._Z.EUR.V.N'  # Importazioni\n\nIT_exp_key = 'MNA/Q.Y.IT.W1.S1.S1.D.P6._Z._Z._Z.EUR.V.N'  # Esportazioni\n\n# Creo i DataFrame per ogni indicatore economico utilizzando le chiavi\nIT_gdp = make_df_ECB(IT_gdp_key, 'GDP')  # Prodotto Interno Lordo (PIL)\nIT_inv = make_df_ECB(IT_inv_key, 'INV')  # Investimenti\nIT_cons = make_df_ECB(IT_cons_key, 'CONS')  # Consumi\nIT_gov_exp = make_df_ECB(IT_gov_exp_key, 'GOV_EXP')  # Spesa Pubblica\nIT_imp = make_df_ECB(IT_imp_key, 'IMP')  # Importazioni\nIT_exp = make_df_ECB(IT_exp_key, 'EXP')  # Esportazioni\n\n# Unisco tutti i DataFrame in uno solo per l'analisi aggregata delle spese\nimport functools as ft\ndfs = [IT_gdp, IT_inv, IT_cons, IT_gov_exp, IT_imp, IT_exp]\nIT_expenditures = ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), dfs)\n\n# Calcolo la bilancia commerciale (BOT) come la differenza tra esportazioni e importazioni\nIT_expenditures['BOT'] = IT_expenditures['EXP'] - IT_expenditures['IMP']\n\n# Creo un grafico a barre impilate per visualizzare la spesa aggregata\nax = IT_expenditures[['INV', 'CONS', 'GOV_EXP', 'BOT']][-32:].plot.bar(stacked=True, figsize=(9, 5))\n\n# Modifico le etichette dell'asse x per visualizzare l'anno e il trimestre\nax.set_xticklabels([f'{x.year} Q{x.quarter}' for x in IT_expenditures.index[-32:]])\n\n# Aggiungo il titolo e le etichette degli assi\nplt.title('Spesa Aggregata - IT')\nplt.xlabel('Trimestre')\nplt.ylabel('Spesa')\n\n# Aggiungo una legenda con le categorie di spesa\nax.legend(['I', 'C', 'G', 'X-IM', 'Y'],\n          title='Categorie',\n          loc='upper left',      \n          frameon=True,\n          fontsize='x-small',\n          ncol=4\n          )\n\n# Adatto il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPIL e PNL\nIn alcuni casi, è utile distinguere tra: PIL, che misura il valore totale dei beni e servizi prodotti all’interno di un paese, indipendentemente dalla proprietà delle risorse e PNL (Prodotto Nazionale Lordo), che misura il valore totale dei beni e servizi prodotti da cittadini di un paese, indipendentemente da dove si trovino nel mondo. Sono emblematici il caso tedesco e quello irlandese.\n\n# Definiamo le colonne di interesse per i dati\ncols = ['geo\\TIME_PERIOD', '2020', '2021', '2022', '2023']\n\n# Otteniamo i dati del Prodotto Nazionale Lordo (PNL) pro capite\ngni_pc_tuples = eurostat.get_data('nama_10_pp')  # Recupera i dati dal database Eurostat\ngni_pc = pd.DataFrame(gni_pc_tuples[1:], columns=gni_pc_tuples[0])  # Crea il DataFrame\ngni_pc = gni_pc[cols]  # Seleziona solo le colonne di interesse\ngni_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne per chiarezza\n\n# Otteniamo i dati del Prodotto Interno Lordo (PIL) pro capite\ngdp_pc_tuples = eurostat.get_data('nama_10_pc')  # Recupera i dati del PIL pro capite\ngdp_pc = pd.DataFrame(gdp_pc_tuples[1:], columns=gdp_pc_tuples[0])  # Crea il DataFrame\ngdp_pc = gdp_pc.query(\"unit=='CP_PPS_EU27_2020_HAB' & na_item == 'B1GQ'\")  # Filtro per unità e tipo di dato\ngdp_pc = gdp_pc[cols]  # Seleziona solo le colonne di interesse\ngdp_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne\n\n# Definiamo la lista dei paesi per i quali vogliamo visualizzare i dati\ncountries = ['IT', 'DE', 'IE']\n\n# Creiamo un grafico a sottotrame per visualizzare i dati\nfig, axs = plt.subplots(1, 3, sharey=True, figsize=(9, 4))  # Crea 3 sottotrame affiancate\naxs = axs.flatten()  # Appiattisce l'array di assi per facilitare l'accesso\n\n# Ciclo sui paesi per plottare i dati di ciascuno\nfor n, country in enumerate(countries):\n    # Selezioniamo i dati del paese corrente per PNL e PIL\n    gni_pc_country = gni_pc[gni_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PNL\n    gdp_pc_country = gdp_pc[gdp_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PIL\n    \n    # Plottiamo i dati sulla stessa sottotrama\n    axs[n].plot(gni_pc_country, label='PNL pro capite')  # PNL\n    axs[n].plot(gdp_pc_country, label='PIL pro capite')  # PIL\n    \n    # Aggiungiamo il titolo e le etichette\n    axs[n].set_title(country, loc='left')  # Titolo a sinistra per ogni paese\n    axs[n].set_xlabel('Anno')  # Etichetta asse X\n    axs[n].set_ylabel('Valore')  # Etichetta asse Y\n\n    # Aggiungiamo la legenda\n    axs[n].legend(\n        loc='upper left',  # Posizione della legenda\n        frameon=True,  # Abilita il frame della legenda\n        fontsize='x-small'  # Font di dimensioni piccole\n    )\n\n# Titolo principale del grafico\nfig.suptitle('PIL e PNL pro capite in Italia, Germania, Irlanda')\n\n# Miglioriamo il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Mostriamo il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di disoccupazione",
    "text": "Tasso di disoccupazione\nIl tasso di disoccupazione si calcola come rapporto tra disoccupati (ovvero coloro i quali non hanno un lavoro, ma lo stanno cercando) e la forza lavoro (data dalla somma di occupati e disoccupati). A questo proposito, è importante sottolineare come chi non ha un lavoro, ma non ne sta nemmeno cercando uno, sia considerato fuori dalla forza lavoro, e dunque non incluso nel conteggio dei disoccupati. Si può quindi calcolare il tasso di partecipazione come forza lavoro divisa per il totale della popolazione.\n\nCome si misura il tasso di disoccupazione?\nGeneralmente, si effettuano indagini a campione sulle famiglie (Labour Force Surveys)\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo un grafico con due assi\nfig, ax1 = plt.subplots()  # Crea una figura e un asse\n\n# Plot del tasso di disoccupazione\nax1.plot(unemp_rate)  # Traccia il grafico del tasso di disoccupazione\nax1.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax1.set_ylabel('Tasso di Disoccupazione', color=bmh_colors[0])  # Etichetta per l'asse Y con colore personalizzato\n\n# Creiamo un secondo asse per la crescita del PIL\nax2 = ax1.twinx()  # Crea un secondo asse Y condiviso sull'asse X\nax2.plot(real_gdp_growth[real_gdp_growth.index &gt;= unemp_rate.index.min()], color='#525252', linestyle=':')  # Traccia il grafico della crescita del PIL con linea tratteggiata\nax2.set_ylabel('Crescita del PIL')  # Etichetta per l'asse Y della crescita del PIL\nax2.grid(False)  # Disabilita la griglia per il secondo asse\n\n# Titolo del grafico\nplt.title('Disoccupazione e crescita del PIL nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di Inflazione",
    "text": "Tasso di Inflazione\nL’inflazione rappresenta un aumento sostenuto del livello generale dei prezzi. Il tasso di inflazione è il tasso a cui il livello dei prezzi aumenta nel tempo.\n\nCome si misura l’inflazione?\nIl livello dei prezzi si può calcolare in due modi:\n\n\nUtilizzando il deflatore del PIL, ovvero il rapporto tra il PIL nominale e il PIL reale nell’anno di interesse\n\n\nLindice armonizzato dei prezzi al consumo, ovvero un numero indice che fissa un prezzo pari a 100 nell’anno base\n\n\nChiamando \\(P_t\\) il livello generale dei prezzi nell’anno \\(t\\), segue che il tasso di inflazione \\(\\pi\\) viene definito come \\(\\pi =\n\\frac{P_t - P_{t-1}}{P_{t-1}} \\approx \\ln P_t - \\ln P_{t-1}\\)\n\n# Chiave per l'Inflation Rate\nHICP_key = 'ICP/M.U2.N.000000.4.ANR'  # Codice identificativo dell'Inflation Rate\nHICP = make_df_ECB(HICP_key, 'Inflation Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo il grafico\nfig, ax = plt.subplots()  # Crea una figura e un asse\n\n# Traccia l'inflazione\nax.plot(HICP)  # Traccia il grafico dell'inflazione\n\n# Imposta etichetta per l'asse X e Y\nax.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax.set_ylabel('Tasso di Inflazione')  # Etichetta per l'asse Y\n\n# Titolo del grafico\nplt.title('Tasso di Inflazione nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  }
]