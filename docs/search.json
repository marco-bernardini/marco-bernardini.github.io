[
  {
    "objectID": "notebooks/User Cohorts.html",
    "href": "notebooks/User Cohorts.html",
    "title": "User Cohorts Analysis",
    "section": "",
    "text": "Understanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention, engagement, and growth is through cohort analysis.\nA user cohort table segments users based on shared characteristics—such as sign-up month or first purchase—and tracks their activity across time. This approach helps businesses measure customer retention, assess the impact of product changes, and optimize marketing strategies.\nIn this notebook, we’ll walk through the process of analyzing a cohort table step by step. Using Python, we’ll transform raw data into actionable insights, visualizing retention curves and uncovering trends that can inform strategic decisions. Whether you’re working in SaaS, e-commerce, or mobile apps, this guide will equip you with the tools to extract meaningful patterns from user data.\nLet’s dive in.\nfrom config import *\n%matplotlib inline\ncmap = plt.get_cmap('viridis')"
  },
  {
    "objectID": "notebooks/User Cohorts.html#right-aligned-table",
    "href": "notebooks/User Cohorts.html#right-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Right-aligned table",
    "text": "Right-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active in the given month\nFor example, the number of users who made their first purchase in March and are still engaging with the platform in June are 9,468\n\nnc_cohorts_right = pd.read_csv('user_cohorts.csv')\nnc_cohorts_right.rename(columns={'Num Users': 'month'}, inplace=True)\nind = nc_cohorts_right['month']\nnc_cohorts_right.set_index('month', inplace=True)\n\nnc_cohorts_right_filled = nc_cohorts_right.fillna(0)\n\nstyled_df = nc_cohorts_right.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\nnan\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\n\n\nMar\nnan\nnan\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\n\n\nApr\nnan\nnan\nnan\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\n\n\nMay\nnan\nnan\nnan\nnan\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\n\n\nJun\nnan\nnan\nnan\nnan\nnan\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\n\n\nJul\nnan\nnan\nnan\nnan\nnan\nnan\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\n\n\nAug\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n109,117\n43,172\n35,336\n33,469\n30,535\n\n\nSep\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n116,187\n40,402\n35,154\n31,603\n\n\nOct\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n117,185\n34,462\n29,055\n\n\nNov\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n107,597\n32,920\n\n\nDec\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n97,164\n\n\n\n\n\n\nretained_clients = nc_cohorts_right.Dec.sum()\nprint(f\"Total reatined clients: {int(retained_clients)}\")\n\nTotal reatined clients: 288895\n\n\nWe can more intuitively visualize the table with a simple chart.\n\ncolors = cmap(np.linspace(0, 1, nc_cohorts_right.shape[1]))\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.stackplot(nc_cohorts_right_filled.index, nc_cohorts_right_filled, labels=nc_cohorts_right_filled.columns, colors=colors, alpha=0.8)\n\nax.legend(loc='upper left', title='Cohort')\nax.set_title('Users Per Month', loc='left')\nax.set_xlabel('Month')\nax.set_ylabel('Number of Users')\nax.grid(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, I noticed that the growth is slowing down. Remember, you always want to confirm your visual intuition with data: in this case, i approximated the growth curve with a known (continuous) function. As the function approximates well the curve, I can use its first derivative to draw conclusions about the instantaneous growth.\n\ntrend = nc_cohorts_right.sum()\nt = np.arange(1, len(trend) + 1)\n\ndef logistic_function(t, L, k, t0):\n    return L / (1 + np.exp(-k * (t - t0)))\n\n[L, k, t0], _ = curve_fit(logistic_function, t, trend, p0=[trend.max(), 1, 0])\n\napprox = logistic_function(t, L, k, t0)\nintensity = k * L * np.exp(-k * (t - t0)) / (1 + np.exp(-k * (t - t0)))**2\n\nR2 = 1 - np.sum((trend - approx) ** 2) / np.sum((trend - np.mean(trend)) ** 2)\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 6))\nax[0].plot(trend, label=\"Trend\")\nax[0].plot(approx, label=\"Logistic Fit\")\nax[0].set_title(f'Approximation of Growth with Logistic Function: R² = {round(R2, 3)}')\nax[0].legend()\n\nax[1].plot(intensity, color=\"red\", label=\"Derivative\")\nax[1].set_title(\"Derivative of Logistic Function (Growth Intensity)\")\nax[1].legend()\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/User Cohorts.html#left-aligned-table",
    "href": "notebooks/User Cohorts.html#left-aligned-table",
    "title": "User Cohorts Analysis",
    "section": "Left-aligned table",
    "text": "Left-aligned table\nIn the following table: * every row corresponds to a single cohort of users, in this case those who made their first purchase in the given month * every column indicates the number of users still active after n months, where n is the column header. n=0 corresponds to the month in which the users made their first purchase\nThis table is very useful, as it allows to analyze the behaviour of different cohorts during their lifetime\n\nnc = nc_cohorts_right.copy().to_numpy()\nfor i, row in enumerate(nc):\n    a = np.argwhere(np.isnan(row))\n    if a.size != 0:\n        start = a.max() + 1\n        temp = row[start:]\n        if temp.size &lt; 12:\n            temp = np.append(temp, np.full(shape=(12 - temp.size), fill_value=np.nan))\n            nc[i] = temp\n    else:\n        pass\n    \nnc_cohorts_left = pd.DataFrame(nc, index=ind)\n\nstyled_df = nc_cohorts_left.style.background_gradient(cmap=cmap, axis=1, vmax=80000, vmin=1000).format('{:,.0f}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n689\n414\n379\n342\n347\n362\n344\n349\n347\n339\n342\n337\n\n\nFeb\n5,595\n3,039\n2,633\n2,565\n2,408\n2,366\n2,318\n2,109\n2,166\n2,051\n2,026\nnan\n\n\nMar\n20,041\n11,037\n9,872\n9,468\n8,807\n8,682\n8,025\n7,788\n7,504\n7,057\nnan\nnan\n\n\nApr\n30,278\n15,831\n14,449\n13,351\n12,587\n11,856\n11,180\n10,988\n9,999\nnan\nnan\nnan\n\n\nMay\n35,630\n16,924\n14,768\n13,840\n12,966\n12,260\n12,118\n11,082\nnan\nnan\nnan\nnan\n\n\nJun\n47,489\n21,653\n18,921\n17,141\n15,806\n15,654\n14,102\nnan\nnan\nnan\nnan\nnan\n\n\nJul\n78,425\n34,774\n29,399\n26,073\n25,229\n23,016\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nAug\n109,117\n43,172\n35,336\n33,469\n30,535\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nSep\n116,187\n40,402\n35,154\n31,603\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nOct\n117,185\n34,462\n29,055\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nNov\n107,597\n32,920\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nDec\n97,164\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\nprint(f\"Total initial users: {int(nc_cohorts_left[0].sum())}\")\n\nTotal initial users: 765395\n\n\n\nAssessing retention rates\nWith a left-aligned table, we can analyse retention rates for every cohort. In this case, retention rates are calculated as: \\[\n    \\frac{Active\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nretention = nc_cohorts_left.copy()\n\nfor col in retention.columns:\n    if col == 0:\n        retention[col] = 1\n    else:\n        retention[col] = nc_cohorts_left[col]/nc_cohorts_left[0]\n\n\nstyled_df = retention.style.background_gradient(cmap=cmap, axis=1, vmax=.60, vmin=.30).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n100.00%\n60.14%\n55.07%\n49.64%\n50.36%\n52.54%\n50.00%\n50.72%\n50.36%\n49.28%\n49.64%\n48.91%\n\n\nFeb\n100.00%\n54.33%\n47.06%\n45.85%\n43.04%\n42.28%\n41.44%\n37.69%\n38.72%\n36.66%\n36.22%\nnan%\n\n\nMar\n100.00%\n55.07%\n49.26%\n47.24%\n43.94%\n43.32%\n40.04%\n38.86%\n37.44%\n35.21%\nnan%\nnan%\n\n\nApr\n100.00%\n52.29%\n47.72%\n44.09%\n41.57%\n39.16%\n36.92%\n36.29%\n33.03%\nnan%\nnan%\nnan%\n\n\nMay\n100.00%\n47.50%\n41.45%\n38.84%\n36.39%\n34.41%\n34.01%\n31.10%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n100.00%\n45.60%\n39.84%\n36.10%\n33.28%\n32.96%\n29.70%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n100.00%\n44.34%\n37.49%\n33.25%\n32.17%\n29.35%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n100.00%\n39.56%\n32.38%\n30.67%\n27.98%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n100.00%\n34.77%\n30.26%\n27.20%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n100.00%\n29.41%\n24.79%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n100.00%\n30.60%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n100.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nWe can then easily visualize the retention trend across different cohorts with a line chart. Ideally, retention rates should improve with time, although this is not the case.\n\nfig, ax = plt.subplots(1, 1)\nretention.T.plot(marker='s', ax=ax, color=colors, alpha=0.67)\n\nax.set_title('Hanging Ribbons', loc='left')\nax.set_ylabel('% Of Cohort Retained')\nax.set_xlabel('Time')\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\nax.legend(\n    title='Cohort',\n    loc='upper center',         \n    ncol=5,\n    frameon=True\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can easily check that retention is negatively correlated with time (and size of the cohorts)\n\nfrom sklearn.preprocessing import minmax_scale\nscaled_nc_cohorts_left = minmax_scale(range(1, len(nc_cohorts_left.index)+1), axis=0)\nscaled_retention = minmax_scale(retention.iloc[:, 1:4], axis=0)\n\ndf_scaled = pd.DataFrame(\n    scaled_nc_cohorts_left, columns=[\"nc_cohorts_left\"]\n).join(pd.DataFrame(scaled_retention, columns=[\"retention_1\", \"retention_2\", \"retention_3\"]))\n\ncorr_matrix = df_scaled.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=False)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nAssessing churn rates\nWith a left-aligned table, we can also analyse churn rates for every cohort. Churn rates in the following table are calculated as: \\[\n    \\frac{Churned\\, users\\, at\\, month\\, n}{Active\\, users\\, at\\, month\\, n-1}\n\\]\n\nchurn = nc_cohorts_left.copy()\n\nfor col in churn.columns:\n    if col == 0:\n        churn[col] = 0\n    else:\n        churn[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])/nc_cohorts_left[col-1]\n\nstyled_df = churn.style.background_gradient(cmap=cmap, axis=1, vmax=.2, vmin=0).format('{:.2%}')\n\nstyled_df = styled_df.apply(\n    lambda row: ['background-color: white; color: white' if pd.isna(v) else '' for v in row]\n)\n\nstyled_df\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nmonth\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nJan\n0.00%\n39.86%\n8.43%\n9.87%\n-1.46%\n-4.32%\n4.83%\n-1.45%\n0.71%\n2.16%\n-0.74%\n1.46%\n\n\nFeb\n0.00%\n45.67%\n13.38%\n2.56%\n6.13%\n1.76%\n2.00%\n9.04%\n-2.72%\n5.30%\n1.22%\nnan%\n\n\nMar\n0.00%\n44.93%\n10.56%\n4.10%\n6.98%\n1.42%\n7.56%\n2.95%\n3.65%\n5.95%\nnan%\nnan%\n\n\nApr\n0.00%\n47.71%\n8.73%\n7.60%\n5.72%\n5.81%\n5.70%\n1.72%\n8.99%\nnan%\nnan%\nnan%\n\n\nMay\n0.00%\n52.50%\n12.74%\n6.29%\n6.31%\n5.45%\n1.16%\n8.55%\nnan%\nnan%\nnan%\nnan%\n\n\nJun\n0.00%\n54.40%\n12.62%\n9.40%\n7.79%\n0.96%\n9.92%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nJul\n0.00%\n55.66%\n15.46%\n11.31%\n3.24%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nAug\n0.00%\n60.44%\n18.15%\n5.28%\n8.77%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nSep\n0.00%\n65.23%\n12.99%\n10.10%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nOct\n0.00%\n70.59%\n15.69%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nNov\n0.00%\n69.40%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\nDec\n0.00%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\nnan%\n\n\n\n\n\nIt’s interesting to calculate and plot the weighted average churn rate for every month. In this case, most of the users disengage after the first month: this is because they were attracted with offers and promotions, and left the platforms as soon as they used those incentives.\n\nchurned = nc_cohorts_left.copy()\nretained = nc_cohorts_left.copy()\n\nfor col in churned.columns:\n    if col == 0:\n        churned[col] = 0\n    else:\n        churned[col] = (nc_cohorts_left[col-1]-nc_cohorts_left[col])\n        \navg_churn = np.zeros(11)\n\nretained_sum = retained.iloc[:-1,:].sum()\nchurned_sum = churned.iloc[:-1,1:].sum()\n\nfor i in range(len(avg_churn)):\n    avg_churn[i] = churned_sum[i+1] / retained_sum[(i)]\n    \navg_churn\n\narray([0.61894928, 0.12467046, 0.0687431 , 0.05115784, 0.03639244,\n       0.04167227, 0.03471718, 0.0376834 , 0.02842538, 0.00237718,\n       0.00210748])\n\n\n\nstd = churn.iloc[:-1,1:].std()\n\nfig, ax = plt.subplots(1, 1,figsize=(8,5))\nax.plot(range(1, 12, 1), avg_churn, label='Mean', color='#00A082')\n\nplt.fill_between(range(1, 12, 1), avg_churn - std, avg_churn + std, alpha=0.2, label='±1 Std Dev', color='#00A082')\n\nplt.xlabel('Month')\nplt.ylabel('% of churned customers (relative to prev. month)')\nplt.title('Churned customers in lifetime month', loc='left')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\nI then wanted to see how many additional clients could’ve been retained at the end of the year by reducing the churn rate by 5 percentage points in the first month\n\navg_churn_proj = avg_churn.copy()\navg_churn_proj[0] = avg_churn_proj[0]-.05\nstarting_users = nc_cohorts_left[0].values\nprojection = np.zeros([12,12])\nprojection[:,0] = starting_users\nfor i in range(11):\n    projection[:(11-i),i+1] = projection[:(11-i),i] * (1-avg_churn_proj[i])\n    \ntotal_clients = 0\nfor i in range(len(projection)):\n    total_clients += projection[i, 11-i]\n    \nprint(f\"Additional clients: {int(total_clients - retained_clients)}\")\n\nAdditional clients: 42441.379632713506\n\n\nTo gauge the average time clients who get past the first month get engaged with the service, it is userful to calculate the (truncated) lifespan of the users. This is simply calculated as: \\[\n\\frac{1}{avg\\, churn\\, rate}\n\\]\n\n1/((nc_cohorts_left.sum()[1:-1]*avg_churn[1:]).sum()/nc_cohorts_left.sum()[1:-1].sum())\n\n14.111089070031147"
  },
  {
    "objectID": "notebooks/La regola di Taylor.html",
    "href": "notebooks/La regola di Taylor.html",
    "title": "La regola di Taylor",
    "section": "",
    "text": "La regola di Taylor (John B. Taylor, 1993) viene frequentemente utilizzata nei modelli macroeconomici, in quanto è in grado di rappresentare in maniera fedele e immediata i comportamenti della banca centrale.\nIn ogni periodo la banca centrale sceglie un target per il tasso di interesse a breve in base alle condizioni dell’economia. Per raggiungere questo tasso, la banca centrale varia l’offerta di moneta per soddisfare la quantità domandata al tasso di interesse target. La prima semplice regola che fu proposta e che gode di importanti proprietà di stabilizzazione, oltre che fondamenta empiriche nel descrivere l’effettiva attività della banca centrale è la seguente:\n\\[\n    i_{t}=\\bar{i}+\\pi_{t}+.5y_{t}+.5(\\pi_{t}-\\bar{pi})\n\\]\ndove \\(\\bar{pi}=2\\) rappresenta il livello di inflazione target, \\(\\bar{i}=2\\) il tasso di interesse naturale e \\(y\\) l’output gap.\nUna formulazione più completa, che tiene conto della tendenza delle banche centrali a modificare i tassi in maniera graduale è più comunemente utilizzata:\n\\[\n    i_{t}=\\rho i_{t-1} + \\left(1-\\rho\\right)\\left(\\bar{i}+\\varphi_{y}y_{t}+\\varphi_{\\pi}\\pi_{t}\\right)+\\varepsilon_{t}\n\\]\ndove si è aggiunto un termine stocastico, \\(\\varepsilon_{t}\\) che rappresenta shock della politica monetaria. Ovviamente \\(0\\leq \\rho \\leq 1\\). In assenza di shock, inflazione ed eccesso di domanda, la banca centrale aggiusta il tasso di interesse per seguire il livello del tasso naturale. Dall’altro lato, se l’economia si sta “scaldando”, con prezzi in aumento o un output gap positivo, la banca aumenta i tassi a breve. Generalmente, \\(\\varphi_{\\pi}\\) supera l’unità, in modo che i tassi vengano alzati in un rapporto superiore a uno quando aumenta l’inflazione. Questo fa sì che la banca possa muovere i tassi a sufficienza da stimolare o ridurre la domanda, a seconda della necessità.\nQuesta regola è stata sintetizzata seguendo il consenso in letteratura, nonchè effettuando simulazioni con modelli complessi, che vanno oltre lo scopo di questa sezione. Può comunque essere interessante vedere all’opera la regola.\n\nfrom config import *\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La regola di Taylor.html#introduzione",
    "href": "notebooks/La regola di Taylor.html#introduzione",
    "title": "La regola di Taylor",
    "section": "",
    "text": "La regola di Taylor (John B. Taylor, 1993) viene frequentemente utilizzata nei modelli macroeconomici, in quanto è in grado di rappresentare in maniera fedele e immediata i comportamenti della banca centrale.\nIn ogni periodo la banca centrale sceglie un target per il tasso di interesse a breve in base alle condizioni dell’economia. Per raggiungere questo tasso, la banca centrale varia l’offerta di moneta per soddisfare la quantità domandata al tasso di interesse target. La prima semplice regola che fu proposta e che gode di importanti proprietà di stabilizzazione, oltre che fondamenta empiriche nel descrivere l’effettiva attività della banca centrale è la seguente:\n\\[\n    i_{t}=\\bar{i}+\\pi_{t}+.5y_{t}+.5(\\pi_{t}-\\bar{pi})\n\\]\ndove \\(\\bar{pi}=2\\) rappresenta il livello di inflazione target, \\(\\bar{i}=2\\) il tasso di interesse naturale e \\(y\\) l’output gap.\nUna formulazione più completa, che tiene conto della tendenza delle banche centrali a modificare i tassi in maniera graduale è più comunemente utilizzata:\n\\[\n    i_{t}=\\rho i_{t-1} + \\left(1-\\rho\\right)\\left(\\bar{i}+\\varphi_{y}y_{t}+\\varphi_{\\pi}\\pi_{t}\\right)+\\varepsilon_{t}\n\\]\ndove si è aggiunto un termine stocastico, \\(\\varepsilon_{t}\\) che rappresenta shock della politica monetaria. Ovviamente \\(0\\leq \\rho \\leq 1\\). In assenza di shock, inflazione ed eccesso di domanda, la banca centrale aggiusta il tasso di interesse per seguire il livello del tasso naturale. Dall’altro lato, se l’economia si sta “scaldando”, con prezzi in aumento o un output gap positivo, la banca aumenta i tassi a breve. Generalmente, \\(\\varphi_{\\pi}\\) supera l’unità, in modo che i tassi vengano alzati in un rapporto superiore a uno quando aumenta l’inflazione. Questo fa sì che la banca possa muovere i tassi a sufficienza da stimolare o ridurre la domanda, a seconda della necessità.\nQuesta regola è stata sintetizzata seguendo il consenso in letteratura, nonchè effettuando simulazioni con modelli complessi, che vanno oltre lo scopo di questa sezione. Può comunque essere interessante vedere all’opera la regola.\n\nfrom config import *\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La regola di Taylor.html#loutput-gap",
    "href": "notebooks/La regola di Taylor.html#loutput-gap",
    "title": "La regola di Taylor",
    "section": "L’output Gap",
    "text": "L’output Gap\nNella sezione dedicata alla legge di Okun si è vista la definizione di output gap:\n\\[\n    y=\\frac{Y_{t}-Y^*}{Y^*}\\times100\n\\]\ndove \\(Y^*\\) rappresenta il prodotto potenziale, ottenuto in condizioni “normali” di occupazione. Nella pratica, l’ouput gap viene stimato utilizzando tecniche econometriche, scomponendo le serie storiche del PIL in 1.trend e 2.deviazioni dal trend, ovvero l’output gap. Una tecnica comunemente utilizzata è il filtro di Hodrick-Prescott, di cui viene fornita l’intuizione. La serie storica viene scomposta in trend \\(\\tau_t\\) e componente ciclica \\(c_t\\) dove \\(y_t = \\tau_t + c_t\\). In base ad un valore costante di \\(\\lambda\\) si vuole quindi trovare \\(\\tau\\) tale da minimizzare la:\n\\[\n    \\min_{\\{\\tau_t\\}} \\left(\\sum_{t=1}^{T} (y_t - \\tau_t)^2 + \\lambda \\sum_{t=2}^{T-1} \\left( (\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1}) \\right)^2\\right)\n\\]\nSi può scomporre la formula in due termini:\n- \\((y_t - \\tau_t)^2\\) penalizza scostamenti del trend dalla serie storica\n- \\(\\left( (\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1}) \\right)^2\\) penalizza variazioni repentine nel trend\nSi nota come sia essenziale la scelta del parametro \\(\\lambda\\): quando \\(\\lambda = 0\\), \\(\\tau_t = y_t\\); per \\(\\lambda \\to \\infty\\) il problema di minimizzazione restuitirà una linea retta.\nSi può vedere all’opera il metodo sotto:\n\n# Imposta la chiave per il recupero dei dati del PIL\nPIL_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'\n\n# Recupera i dati del PIL utilizzando la funzione make_df_ECB\npil = make_df_ECB(PIL_KEY, 'PIL')\n\n# Imposta il parametro lambda per il filtro di Hodrick-Prescott (HP)\nlambda_hp = 1600\n\n# Applica il filtro HP al PIL per ottenere il ciclo e la tendenza\ncycle, trend = sm.tsa.filters.hpfilter(pil.PIL, lamb=lambda_hp)\n\n# Crea un DataFrame con i dati del PIL e aggiunge le colonne per ciclo e tendenza\ngdp_decomp = pil[['PIL']]\ngdp_decomp[\"cycle\"] = cycle\ngdp_decomp[\"trend\"] = trend\n\n# Crea una figura e un oggetto assi per il grafico\nfig, ax = plt.subplots()\n\n# Traccia il PIL e la tendenza nel grafico\ngdp_decomp[['PIL','trend']].plot(ax=ax)\n\n# Imposta il titolo del grafico e la sua posizione\nax.set_title('Scomposizione del PIL', loc='left')\n\n# Imposta le etichette degli assi\nax.set_xlabel('Trimestre')\nax.set_ylabel('PIL')\n\n# Aggiunge la legenda al grafico\nplt.legend()\n\n# Ottimizza il layout del grafico per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Calcola il logaritmo naturale del PIL e lo aggiunge come nuova colonna 'log_PIL'\npil['log_PIL'] = np.log(pil['PIL'])\n\n# Applica il filtro HP al logaritmo del PIL per ottenere il ciclo e la tendenza\ncycle, trend = sm.tsa.filters.hpfilter(pil.log_PIL, lamb=lambda_hp)\n\n# Moltiplica il ciclo per 100 per esprimere l'output gap in termini percentuali\ncycle = cycle * 100\n\n# Crea una figura e un oggetto assi per il grafico\nfig, ax = plt.subplots()\n\n# Traccia il ciclo (output gap) nel grafico\ncycle.plot(ax=ax)\n\n# Imposta il titolo del grafico e la sua posizione\nax.set_title('Output Gap', loc='left')\n\n# Imposta le etichette degli assi\nax.set_xlabel('Trimestre')\nax.set_ylabel('Output Gap (%)')\n\n# Ottimizza il layout del grafico per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\nVale la pena analizzare le principali proprietà dell’output gap.\nVediamo che questo è correttamente centrato, relativamente persistente e stazionario. In questo caso, includere gli anni della pandemia fa sì che la serie sia decisamente asimmetrica e con code pesanti, il che fa sì che la serie non sia normalmente distribuita.\n\n# Importa il modulo stats da scipy per le statistiche\nimport scipy.stats as stats\n\n# Calcola la media del ciclo\nmean_cycle = np.mean(cycle)\n\n# Calcola la deviazione standard del ciclo (con correzione per il campione)\nstd_cycle = np.std(cycle, ddof=1)\n\n# Calcola l'asimmetria del ciclo (skewness)\nskew_cycle = stats.skew(cycle)\n\n# Calcola la curtosi del ciclo (kurtosis) utilizzando la definizione di Fisher\nkurt_cycle = stats.kurtosis(cycle, fisher=True)\n\n# Calcola l'autocorrelazione di ordine 1 tra ciclo e ciclo spostato di 1 periodo\nautocorr_cycle = np.corrcoef(cycle[:-1], cycle[1:])[0, 1]\n\n# Calcola la statistica di Jarque-Bera e il p-value per testare la normalità\njb_stat, jb_p_value = stats.jarque_bera(cycle)\n\n# Calcola la statistica di Dickey-Fuller per verificare la stazionarietà\nadf_stat, adf_p_value, _, _, _, _ = sm.tsa.stattools.adfuller(cycle)\n\n# Stampa i risultati delle statistiche calcolate con 4 decimali\nprint(f\"Media: {mean_cycle:.4f}\")\nprint(f\"Deviazione Standard: {std_cycle:.4f}\")\nprint(f\"Asimmetria: {skew_cycle:.4f}\")\nprint(f\"Kurtosi: {kurt_cycle:.4f}\")\nprint(f\"Autocorrelazione del primo ordine: {autocorr_cycle:.4f}\")\nprint(f\"Jarque-Bera: {jb_stat:.4f}, p-value: {jb_p_value:.4f}\")\nprint(f\"Dickey-Fuller: {adf_stat:.4f}, p-value: {adf_p_value:.4f}\")\n\nMedia: -0.0000\nDeviazione Standard: 1.8016\nAsimmetria: -3.4961\nKurtosi: 25.1821\nAutocorrelazione del primo ordine: 0.6026\nJarque-Bera: 3415.1426, p-value: 0.0000\nDickey-Fuller: -5.3898, p-value: 0.0000"
  },
  {
    "objectID": "notebooks/La regola di Taylor.html#la-regola-in-pratica",
    "href": "notebooks/La regola di Taylor.html#la-regola-in-pratica",
    "title": "La regola di Taylor",
    "section": "La regola in pratica",
    "text": "La regola in pratica\nDi seguito verrà mostrato come venga applicata la regola in pratica. Verranno estratte dal database della BCE le serie storiche di inflazione e il tasso sulle operazioni di rifinanziamento principali, utilizzato come benchmark per la regola. Per l’output gap, verrà utilizzata la serie appena ricavata.\nLa regola di Taylor utilizzata è la\n\\[\n    i_{t}=\\rho i_{t-1} + \\left(1-\\rho\\right)\\left(\\bar{i}+\\varphi_{y}y_{t}+\\varphi_{\\pi}\\pi_{t}\\right)+\\varepsilon_{t}\n\\]\nche è stata calibrata utilizzando parametri comuni in letteratura: \\(\\bar{i}=1, \\varphi_{y}=.5, \\varphi_{\\pi}=1.5\\) ed utilizzando un coefficiente di smussamento abbastanza elevato, ad indicare la preferenza della BCE di evitare repentine variazioni nei tassi.\nInteressante è notare come la regola segua da vicino l’andamento dell’inflazione ma che in pratica, dal 2008 la BCE abbia per un lungo periodo cercato di mantenere i tassi prossimi allo zero, per favorire la ripresa dell’economia, accettando livelli di inflazione sopra al target. Solamente quando questa è esplosa a seguito delle conseguenza della guerra in Ucraina, la BCE è stata costretta ad aumentare i tassi per cercare di controllarne gli effetti negativi.\n\n# Rinomina la serie del ciclo come 'y', seleziona i dati dal 1° gennaio 1999 in poi e li sposta di un periodo\ny = cycle.rename('y')[\"1999-01-01\":].shift(1)\n\n# Definisce il codice identificativo per l'Inflation Rate (HICP)\nHICP_key = 'ICP/M.U2.N.000000.4.ANR'\n\n# Recupera i dati dell'Inflation Rate dal database ECB\nHICP = make_df_ECB(HICP_key, 'pi')\n\n# Raggruppa i dati per trimestre, prendendo l'ultima osservazione di ogni trimestre\nHICP = HICP.groupby(pd.Grouper(freq='Q')).last()\n\n# Imposta l'indice come il primo giorno di ogni trimestre\nHICP.index = HICP.index.to_period('Q').to_timestamp(how='start')\n\n# Seleziona i dati dell'inflazione dal 1° gennaio 1999 in poi e li sposta di un periodo\npi = HICP['pi'][\"1999-01-01\":].shift(1)\n\n# Definisce il codice identificativo per il tasso di interesse MRO\nMRO_key = 'FM/B.U2.EUR.4F.KR.MRR_FR.LEV'\n\n# Recupera i dati del tasso di interesse MRO dal database ECB\nMRO = make_df_ECB(MRO_key, 'MRO')\n\n# Reindicizza i dati MRO con frequenza trimestrale e li riempie con il metodo forward fill\ni = MRO['MRO'].reindex(pd.date_range(start=\"1999-01-01\", end=\"2025-01-01\", freq=\"Q\").to_period('Q').to_timestamp(how='start'), method='ffill')\n\n# Crea una nuova serie spostata di un periodo e la rinomina come 'ilag1'\nilag1 = i.shift(1).rename('ilag1')\n\n# Combina le serie 'ilag1', 'pi', 'y' e 'i' in un unico DataFrame, rimuovendo i valori mancanti\ntaylor = pd.concat([ilag1, pi, y, i], axis=1).dropna()\n\n\n# Calcola la Regola di Taylor (TR) come una combinazione lineare di tassi di interesse, output gap e inflazione\ntaylor['TR'] = .7*taylor['ilag1'] + .3* (1 + .5*taylor['y'] + 1.5*(taylor['pi']))\n\n# Crea una figura e un oggetto assi per il grafico con dimensioni specifiche\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# Traccia il grafico dell'Output Gap (y) con linea tratteggiata\ny.plot(ax=ax, label='Output Gap (y)', color='#525252', linestyle=':', linewidth=1)\n\n# Traccia il grafico dell'Inflazione (π) con linea tratteggiata\npi.plot(ax=ax, label='Tasso Di Inflazione (π)', color='#525252', linestyle='--', linewidth=1)\n\n# Traccia il grafico del tasso di interesse MRO (i) con una linea solida\ni.plot(ax=ax, label='MRO (i)', color=bmh_colors[0], linestyle='-', linewidth=2)\n\n# Traccia il grafico della Regola di Taylor (TR) con una linea solida\ntaylor.TR.plot(ax=ax, label='Regola Di Taylor (TR)', color=bmh_colors[1], linestyle='-', linewidth=2)\n\n# Imposta il titolo del grafico e la sua posizione\nax.set_title('Regola Di Taylor: Teoria E Pratica', loc='left')\n\n# Imposta le etichette degli assi\nax.set_xlabel('Trimestre')\nax.set_ylabel('%')\n\n# Aggiunge la legenda al grafico\nax.legend(loc='best')\n\n# Ottimizza il layout del grafico per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html",
    "href": "notebooks/La curva di Phillips.html",
    "title": "La curva di Phillips",
    "section": "",
    "text": "Nel finire degli anni 50 l’economista della LSE A.W.Phillips osservò una relazione inversa fra disoccupazione e inflazione salariale, ovvero il tasso di crescita dei salari nominali, nell’economia britannica. Pochi anni dopo, Samuelson e Solow mostrarono che una simile relazione esisteva anche nell’economia americana, legando non più la disoccupazione solamente all’inflazione salariale, ma al tasso di crescita generale dei prezzi. La curva divenne così uno strumento utilizzato dai policy makers che, dopo aver valutato il trade-off tra inflazione e disoccupazione, mettevano in atto strategie atte a raggiungere un punto specifico della curva.\n\nfrom config import *\nfrom config import make_df_ECB\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#introduzione",
    "href": "notebooks/La curva di Phillips.html#introduzione",
    "title": "La curva di Phillips",
    "section": "",
    "text": "Nel finire degli anni 50 l’economista della LSE A.W.Phillips osservò una relazione inversa fra disoccupazione e inflazione salariale, ovvero il tasso di crescita dei salari nominali, nell’economia britannica. Pochi anni dopo, Samuelson e Solow mostrarono che una simile relazione esisteva anche nell’economia americana, legando non più la disoccupazione solamente all’inflazione salariale, ma al tasso di crescita generale dei prezzi. La curva divenne così uno strumento utilizzato dai policy makers che, dopo aver valutato il trade-off tra inflazione e disoccupazione, mettevano in atto strategie atte a raggiungere un punto specifico della curva.\n\nfrom config import *\nfrom config import make_df_ECB\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "href": "notebooks/La curva di Phillips.html#paper-di-phillips",
    "title": "La curva di Phillips",
    "section": "Paper di Phillips",
    "text": "Paper di Phillips\nIn questa sezione, replicheremo l’articolo originale di Phillips, utilizzando i dati forniti dalla Bank Of England (https://www.bankofengland.co.uk/statistics/research-datasets)\n\n# Importa i dati sulla disoccupazione dal file Excel contenente dati macroeconomici del Regno Unito\nunemployment = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                             sheet_name='A50. Employment & unemployment', \n                             skiprows=2, usecols='A,J')\n\n# Rimuove le prime due righe che contengono informazioni non necessarie\nunemployment = unemployment[2:]\n\n# Rinomina la colonna non etichettata in 'Year' per maggiore chiarezza\nunemployment.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n\n# Imposta l'anno come indice del DataFrame\nunemployment.set_index('Year', inplace=True)\n\n# Converte la colonna del tasso di disoccupazione in formato numerico (float)\nunemployment['Unemployment rate'] = unemployment['Unemployment rate'].astype(float)\n\n# Mostra le prime righe del DataFrame per verificare la corretta importazione dei dati\nunemployment.head()\n\n\n\n\n\n\n\n\nUnemployment rate\n\n\nYear\n\n\n\n\n\n1855\n3.731877\n\n\n1856\n3.518180\n\n\n1857\n3.945176\n\n\n1858\n5.215345\n\n\n1859\n3.276380\n\n\n\n\n\n\n\n\n# Importa i dati sui salari e l'inflazione dal file Excel contenente dati macroeconomici del Regno Unito\nwages_inflation = pd.read_excel('a-millennium-of-macroeconomic-data-for-the-uk.xlsx', \n                                sheet_name='A47. Wages and prices', \n                                usecols='A,BD')\n\n# Rinomina le colonne per una maggiore chiarezza\nwages_inflation.columns = ['Year', 'Wage index']\n\n# Rimuove le prime sei righe non necessarie\nwages_inflation = wages_inflation[6:]\n\n# Converte la colonna 'Year' in formato intero\nwages_inflation['Year'] = wages_inflation['Year'].astype(int)\n\n# Converte la colonna 'Wage index' in formato numerico (float)\nwages_inflation['Wage index'] = wages_inflation['Wage index'].astype(float)\n\n# Filtra i dati per includere solo gli anni a partire dal 1855\nwages_inflation = wages_inflation[wages_inflation['Year'] &gt;= 1855]\n\n# Imposta l'anno come indice del DataFrame\nwages_inflation.set_index('Year', inplace=True)\n\n# Calcola la differenza centrale prima (approssimazione numerica della derivata prima) e la esprime in percentuale.\n# Questa è la misura utilizzata nell'articolo originale, qui utilizzeremo invece la variazione percentuale\nwages_inflation['First central difference'] = (\n    (wages_inflation['Wage index'].shift(-1) - wages_inflation['Wage index'].shift(1)) / \n    (2 * wages_inflation['Wage index'])\n) * 100\n\n# Calcola la variazione percentuale anno su anno del Wage index\nwages_inflation['Percent change'] = wages_inflation['Wage index'].pct_change() * 100\n\n# Mostra le prime righe del DataFrame per verificare la corretta elaborazione dei dati\nwages_inflation.head()\n\n\n\n\n\n\n\n\nWage index\nFirst central difference\nPercent change\n\n\nYear\n\n\n\n\n\n\n\n1855\n59.0\nNaN\nNaN\n\n\n1856\n59.0\n-1.694915\n0.000000\n\n\n1857\n57.0\n-2.631579\n-3.389831\n\n\n1858\n56.0\n0.000000\n-1.754386\n\n\n1859\n57.0\n1.754386\n1.785714\n\n\n\n\n\n\n\n\n# Unisce i DataFrame 'unemployment' e 'wages_inflation' utilizzando l'indice (Year) e rimuove eventuali valori NaN\ndf = pd.merge(unemployment, wages_inflation, right_index=True, left_index=True).dropna()\n\n# Suddivide il dataset in tre periodi storici distinti per analizzare la relazione tra disoccupazione e inflazione salariale\ndfA = df[df.index &lt;= 1913].copy()       # Periodo pre-1913\ndfB = df[(df.index &gt;= 1913) & (df.index &lt;= 1948)].copy()  # 1913-1948 (tra le due guerre)\ndfC = df[(df.index &gt;= 1948) & (df.index &lt;= 1957)].copy()  # 1948-1957 (primi anni post-bellici)\n\n# Definisce le etichette temporali per i grafici\ndates = ['1861-1913', '1913-1948', '1948-1957']\n\n# Crea una figura con tre subplot affiancati per rappresentare ciascun periodo\nfig, ax = plt.subplots(1, 3, figsize=(10, 3.5))\n\n# Itera sui tre sotto-dataset per stimare e tracciare la curva di Phillips per ciascun periodo\nfor i, _ in enumerate([dfA, dfB, dfC]):\n    \n    # Calcola il termine di aggiustamento per evitare problemi con il logaritmo\n    a = abs(_[\"Percent change\"].min()) + 0.9\n    \n    # Trasforma in logaritmo il tasso di inflazione salariale e il tasso di disoccupazione\n    _[\"Log wage inflation rate\"] = np.log(_[\"Percent change\"] + a)\n    _[\"Log unemployment rate\"] = np.log(_[\"Unemployment rate\"])\n    \n    # Definisce le variabili per la regressione lineare (modello di Phillips)\n    X = _[\"Log unemployment rate\"]\n    Y = _[\"Log wage inflation rate\"]\n    X = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\n    model = sm.OLS(Y, X).fit()  # Stima il modello di regressione\n    R2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n    \n    # Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\n    _[\"Predicted log wage inflation rate\"] = model.predict(X)\n    _[\"Predicted wage inflation rate\"] = np.exp(_[\"Predicted log wage inflation rate\"]) - a\n    \n    # Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\n    sorted_data = _.sort_values(by='Unemployment rate')\n    \n    # Disegna una linea orizzontale per rappresentare l'asse y=0\n    ax[i].axhline(y=0, color='silver', linewidth=2)\n    \n    # Crea uno scatter plot con i dati osservati\n    ax[i].scatter(_['Unemployment rate'], _['Percent change'], alpha=0.7, \n                  label=\"Osservazioni\", color=bmh_colors[0])\n    \n    # Disegna la curva di Phillips stimata dalla regressione\n    ax[i].plot(sorted_data['Unemployment rate'], sorted_data['Predicted wage inflation rate'], \n               color=bmh_colors[1], label=\"Curva Di Phillips\")\n    \n    # Imposta il titolo per ciascun sottografico\n    ax[i].set_title(f\"UK, {dates[i]}\", loc='left')\n    \n    # Etichette degli assi\n    ax[i].set_xlabel('Tasso Di Disoccupazione')\n    ax[i].set_ylabel('Tasso Di Inflazione Salariale')\n    \n    # Attiva la griglia\n    ax[i].grid(True)\n    \n    # Aggiunge la legenda\n    ax[i].legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "href": "notebooks/La curva di Phillips.html#paper-di-samuelson-e-solow",
    "title": "La curva di Phillips",
    "section": "Paper di Samuelson e Solow",
    "text": "Paper di Samuelson e Solow\nIn questa sezione, cercheremo di analizzare il fenomeno all’interno dell’economia Americana, per la quale i dati sono purtroppo frammentari (si vedano https://data.bls.gov/pdq/SurveyOutputServlet e https://fred.stlouisfed.org/series/M08142USM055NNBR)\n\n# Carica i dati sulla disoccupazione (BLS) dal file Excel, saltando le prime 16 righe e impostando 'Year' come indice\nunemp = pd.read_excel('BLS1.xlsx', skiprows=16).set_index('Year')\n\n# Carica i dati sulle retribuzioni orarie medie da 25 industrie manifatturiere degli Stati Uniti (NBER)\nearnings = pd.read_csv('Average Hourly Earnings, Twenty-Five Manufacturing Industries for United States.csv').set_index('observation_date')\n\n# Converte l'indice di 'earnings' in formato datetime (anno-mese-giorno)\nearnings.index = pd.to_datetime(earnings.index, format='%Y-%m-%d')\n\n# Raggruppa i dati sulle retribuzioni annualmente, calcolando la media per ogni anno\nearnings = earnings.groupby(pd.Grouper(freq='Y')).mean()\n\n# Estrae solo l'anno dall'indice per facilitarne l'elaborazione\nearnings.index = earnings.index.year\n\n# Filtra i dati per gli anni dal 1929 al 1947\nearnings = earnings[(earnings.index &gt;= 1929) & (earnings.index &lt;= 1947)]\n\n# Rinomina la colonna per avere una denominazione chiara\nearnings.columns = ['HOURLY_WAGE']\n\n# Rinomina la colonna della disoccupazione per chiarezza\nunemp.columns = ['UNEMPLOYMENT']\n\n# Unisce i due DataFrame sui rispettivi indici (anno)\ndf = pd.merge(unemp, earnings, right_index=True, left_index=True)\n\n# Calcola la variazione percentuale annuale delle retribuzioni orarie\ndf['PCT_CHANGE'] = df['HOURLY_WAGE'].pct_change() * 100\n\n# Rimuove le righe con valori mancanti (NaN) creati durante il calcolo della variazione percentuale\ndf = df.dropna()\n\n# Mostra le prime righe del DataFrame per verificare i dati\ndf.head()\n\n\n\n\n\n\n\n\nUNEMPLOYMENT\nHOURLY_WAGE\nPCT_CHANGE\n\n\nYear\n\n\n\n\n\n\n\n1930\n8.7\n0.589000\n-0.113058\n\n\n1931\n15.9\n0.563833\n-4.272779\n\n\n1932\n23.6\n0.497583\n-11.749926\n\n\n1933\n24.9\n0.490583\n-1.406800\n\n\n1934\n21.7\n0.579750\n18.175641\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"PCT_CHANGE\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"PCT_CHANGE\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"UNEMPLOYMENT\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='UNEMPLOYMENT')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['UNEMPLOYMENT'], df['PCT_CHANGE'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['UNEMPLOYMENT'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Curva Di Phillips\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"US, 1930-1947\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Tasso Di Disoccupazione')\nax.set_ylabel('Tasso Di Inflazione Salariale')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La curva di Phillips.html#e-oggi",
    "href": "notebooks/La curva di Phillips.html#e-oggi",
    "title": "La curva di Phillips",
    "section": "E oggi?",
    "text": "E oggi?\n\ndef make_df_ECB(key, obs_name):\n    \"\"\"Estrae i dati dal datawarehouse della BCE\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'  # URL di base per il servizio web della BCE\n    format_ = '?format=csvdata'  # Impostazione del formato dei dati in CSV\n    df = pd.read_csv(url_+key+format_)  # Legge i dati CSV dal servizio web BCE usando la chiave\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]  # Seleziona le colonne di interesse (periodo e valore osservato)\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n    df = df.set_index('TIME_PERIOD')  # Imposta il periodo come indice del dataframe\n    df.columns = [obs_name]  # Rinomina la colonna con il nome della variabile\n    return df\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Chiave per la wage inflation (inflazione salariale)\nwage_inflation_key = 'MNA/Q.Y.I9.W2.S1.S1._Z.COM_PS._Z._T._Z.IX.V.N'  # Codice identificativo del salario per addetto (indice)\nwage_inflation = make_df_ECB(wage_inflation_key, 'Compensation per employee')  # Ottieni i dati dal database ECB\n\nwage_inflation = wage_inflation[wage_inflation.index &gt;= '2000-01-01']  # Filtra i dati dal 2000 in poi\n\n# Combina i dati di disoccupazione e salario in un unico dataframe\ndf = pd.merge(unemp_rate, wage_inflation, right_index=True, left_index=True)\n\n# Raggruppa i dati per anno e calcola la media per il tasso di disoccupazione e l'ultimo valore per la compensazione per addetto\ndf = df.groupby(pd.Grouper(freq='Y'))[['Unemployment Rate', 'Compensation per employee']].agg({'Unemployment Rate':'mean',\n                                                                                             'Compensation per employee':'last'})\n\ndf.index = df.index.year  # Imposta l'indice come anno\ndf['Wage inflation rate'] = df['Compensation per employee'].pct_change()*100  # Calcola il tasso di inflazione salariale come variazione percentuale\ndf = df.dropna()  # Rimuove i valori mancanti\n\ndf.head()  # Mostra le prime righe del dataframe\n\n\n\n\n\n\n\n\nUnemployment Rate\nCompensation per employee\nWage inflation rate\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n2001\n8.508061\n70.332653\n2.705440\n\n\n2002\n8.753020\n72.127430\n2.551840\n\n\n2003\n9.188271\n73.757671\n2.260224\n\n\n2004\n9.399281\n75.327356\n2.128164\n\n\n2005\n9.231180\n77.193789\n2.477762\n\n\n\n\n\n\n\n\n# Calcola il termine di aggiustamento per evitare problemi con il logaritmo\na = abs(df[\"Wage inflation rate\"].min()) + 0.9\n\n# Calcola il logaritmo del tasso di variazione percentuale delle retribuzioni, aggiungendo il termine 'a'\ndf[\"LOG_VAR_PERC\"] = np.log(df[\"Wage inflation rate\"] + a)\n\n# Calcola il logaritmo del tasso di disoccupazione\ndf[\"LOG_UNRATE\"] = np.log(df[\"Unemployment Rate\"])\n\n# Definisce le variabili per la regressione lineare (modello di Phillips)\nX = df[\"LOG_UNRATE\"]\nY = df[\"LOG_VAR_PERC\"]\nX = sm.add_constant(X)  # Aggiunge l'intercetta al modello di regressione\nmodel = sm.OLS(Y, X).fit()  # Stima il modello di regressione\nR2 = model.rsquared  # Ottiene il coefficiente di determinazione R^2\n\n# Calcola i valori predetti dalla regressione e li riconverte dalla scala logaritmica\ndf[\"Predicted log inflation rate\"] = model.predict(X)\ndf[\"Predicted inflation rate\"] = np.exp(df[\"Predicted log inflation rate\"]) - a\n\n# Ordina i dati per il tasso di disoccupazione per una migliore rappresentazione grafica\nsorted_data = df.sort_values(by='Unemployment Rate')\n\n# Crea una figura con un solo sottografico\nfig, ax = plt.subplots(1, 1)\n\n# Disegna una linea orizzontale per rappresentare l'asse y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea uno scatter plot con i dati osservati\nax.scatter(df['Unemployment Rate'], df['Wage inflation rate'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Disegna la curva di Phillips stimata dalla regressione\nax.plot(sorted_data['Unemployment Rate'], sorted_data['Predicted inflation rate'], \n        color=bmh_colors[1], label=\"Curva Di Phillips\")\n\n# Imposta il titolo del grafico\nax.set_title(f\"EU, 2001-2024\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Tasso Di Disoccupazione')\nax.set_ylabel('Tasso Di Inflazione Salariale')\n\n# Attiva la griglia\nax.grid(True)\n\n# Aggiunge la legenda\nax.legend()\n\n# Migliora la disposizione dei grafici per evitare sovrapposizioni\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/I mercati finanziari.html",
    "href": "notebooks/I mercati finanziari.html",
    "title": "I mercati finanziari",
    "section": "",
    "text": "import ipywidgets as widgets\nfrom IPython.display import display, HTML, Javascript\n\n# Output widget for rendering the D3.js visualization\noutput = widgets.Output()\n\n# Inject initial HTML & D3.js\nhtml_code = \"\"\"\n&lt;div id=\"d3-container\"&gt;&lt;/div&gt;\n&lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    var svg = d3.select(\"#d3-container\")\n                .append(\"svg\")\n                .attr(\"width\", 400)\n                .attr(\"height\", 200);\n\n    var circle = svg.append(\"circle\")\n                    .attr(\"cx\", 200)\n                    .attr(\"cy\", 100)\n                    .attr(\"r\", 50)\n                    .style(\"fill\", \"blue\");\n\n    // Define function to update the circle's radius\n    window.updateD3 = function(value) {\n        circle.transition().duration(300).attr(\"r\", value);\n    };\n&lt;/script&gt;\n\"\"\"\nwith output:\n    display(HTML(html_code))\n\n# Create slider widget\nslider = widgets.IntSlider(value=50, min=10, max=100, step=1, description=\"Radius\")\n\n# Define function to update JavaScript dynamically\ndef update_circle(change):\n    js_code = f\"window.updateD3({change['new']});\"\n    display(Javascript(js_code))\n\n# Attach slider event listener\nslider.observe(update_circle, names=\"value\")\n\n# Display components\ndisplay(output, slider)"
  },
  {
    "objectID": "notebooks/I mercati finanziari.html#introduzione",
    "href": "notebooks/I mercati finanziari.html#introduzione",
    "title": "I mercati finanziari",
    "section": "",
    "text": "import ipywidgets as widgets\nfrom IPython.display import display, HTML, Javascript\n\n# Output widget for rendering the D3.js visualization\noutput = widgets.Output()\n\n# Inject initial HTML & D3.js\nhtml_code = \"\"\"\n&lt;div id=\"d3-container\"&gt;&lt;/div&gt;\n&lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    var svg = d3.select(\"#d3-container\")\n                .append(\"svg\")\n                .attr(\"width\", 400)\n                .attr(\"height\", 200);\n\n    var circle = svg.append(\"circle\")\n                    .attr(\"cx\", 200)\n                    .attr(\"cy\", 100)\n                    .attr(\"r\", 50)\n                    .style(\"fill\", \"blue\");\n\n    // Define function to update the circle's radius\n    window.updateD3 = function(value) {\n        circle.transition().duration(300).attr(\"r\", value);\n    };\n&lt;/script&gt;\n\"\"\"\nwith output:\n    display(HTML(html_code))\n\n# Create slider widget\nslider = widgets.IntSlider(value=50, min=10, max=100, step=1, description=\"Radius\")\n\n# Define function to update JavaScript dynamically\ndef update_circle(change):\n    js_code = f\"window.updateD3({change['new']});\"\n    display(Javascript(js_code))\n\n# Attach slider event listener\nslider.observe(update_circle, names=\"value\")\n\n# Display components\ndisplay(output, slider)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marco Bernardini",
    "section": "",
    "text": "Linkedin\n  \n  \n    \n     Github\n  \n\n  \n  \nHi, I’m Marco, a macroeconomics researcher with a passion for data, math, and programming. I am currently working as a consultant, helping banks and insurance companies develop insightful ML and AI tools. When I’m not diving into data, you’ll find me enjoying endurance sports, mountaineering, or spending quality time with my two poodles, Joyce and Leyla.\n\n\n\n\nMPhil in Economics\n\n\n\nBSc in Economics\n\n\n\n\n\n\nFinancial Services Consultant\n\n\n\nData Analytics & BI Lead"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Marco Bernardini",
    "section": "",
    "text": "MPhil in Economics\n\n\n\nBSc in Economics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Marco Bernardini",
    "section": "",
    "text": "Financial Services Consultant\n\n\n\nData Analytics & BI Lead"
  },
  {
    "objectID": "D3/Mundell-Fleming.html",
    "href": "D3/Mundell-Fleming.html",
    "title": "Il modello Mundell-Fleming",
    "section": "",
    "text": "Il modello Mundell-Fleming fu introdotto per estendere i risultati del modello IS-TR ad una piccola economia aperta, ovvero un’economia in cui non ci siano limitazioni agli scambi commerciali e finanziari con l’estero. Inotrducendo il commercio internazionale di attività finanziare, infatti, i risultati visti fin’ora cambiano considerevolmente. Particolare importanza ha qui il regime di tasso di cambio adottato dall’economia oggetto di studio.\nIn particolare, verranno considerati:\n- Il regime di cambi fissi, dove la banca centrale si impegna a manetenere fisso un certo valore del tasso di cambio nominale, di solito nei confronti del Dollaro.\n- Il regime di cambi flessibili, dove invece i tassi di cambio sono liberi di variare.\nSi partirà dunque dall’analisi delle principali conseguenze dell’apertura ai flussi finanziari internazionali, per entrare poi nello specifico del modello per le due tipologie di economia elencate."
  },
  {
    "objectID": "D3/Mundell-Fleming.html#introduzione",
    "href": "D3/Mundell-Fleming.html#introduzione",
    "title": "Il modello Mundell-Fleming",
    "section": "",
    "text": "Il modello Mundell-Fleming fu introdotto per estendere i risultati del modello IS-TR ad una piccola economia aperta, ovvero un’economia in cui non ci siano limitazioni agli scambi commerciali e finanziari con l’estero. Inotrducendo il commercio internazionale di attività finanziare, infatti, i risultati visti fin’ora cambiano considerevolmente. Particolare importanza ha qui il regime di tasso di cambio adottato dall’economia oggetto di studio.\nIn particolare, verranno considerati:\n- Il regime di cambi fissi, dove la banca centrale si impegna a manetenere fisso un certo valore del tasso di cambio nominale, di solito nei confronti del Dollaro.\n- Il regime di cambi flessibili, dove invece i tassi di cambio sono liberi di variare.\nSi partirà dunque dall’analisi delle principali conseguenze dell’apertura ai flussi finanziari internazionali, per entrare poi nello specifico del modello per le due tipologie di economia elencate."
  },
  {
    "objectID": "D3/Mundell-Fleming.html#la-parità-dei-tassi-di-interesse-e-la-retta-ifm",
    "href": "D3/Mundell-Fleming.html#la-parità-dei-tassi-di-interesse-e-la-retta-ifm",
    "title": "Il modello Mundell-Fleming",
    "section": "La parità dei tassi di interesse e la retta IFM",
    "text": "La parità dei tassi di interesse e la retta IFM\nLa regola di parità dei tassi di interesse afferma che, in un mercato aperto e finanziariamente integrato, il livello di attività simili non possa divergere sistematicamente a livello internazionale, nemmeno nel breve periodo. Questa viene vista come una condizione di equilibrio dei mercati internazionali.\nPer capire come mai, si può far riferimento ad un semplice esercizio di arbitraggio. Esulando per un attimo dagli aspetti legati alle variazioni del tasso di cambio e ai bid-ask spread, si chiamino \\(i\\) il tasso di interesse interno e \\(i^*\\) il tasso di interesse internazionale. Si supponga che \\(i^*&gt;i\\): in questo caso prendendo a prestito nell’economia interna e investendo nell’economia internazionale si avrebbe un sicuro profitto. Si avrebbe quindi una istantaneo incremento della domanda di moneta in valuta nazionale, spingendo \\(i\\) verso \\(i^*\\). Il movimento opposto si avrebbe qualora \\(i&gt;i^*\\).\nLa condizione di parità dei tassi di interesse è rappresentata dalla retta dei mercati finanziari internazionali, la retta IFM.\n\n\n\nMercato Monetario\n\n\n\n\n\n\n\n\nVariabili:\n\n Δi Reset"
  },
  {
    "objectID": "D3/Mundell-Fleming.html#il-modello-in-un-regime-di-cambi-fissi",
    "href": "D3/Mundell-Fleming.html#il-modello-in-un-regime-di-cambi-fissi",
    "title": "Il modello Mundell-Fleming",
    "section": "Il modello in un regime di cambi fissi",
    "text": "Il modello in un regime di cambi fissi\nIn regime di cambi fissi, la banca centrale si impegna a mantenere ancorata la propria valuta in riferimento ad una valuta estera. Per raggiungere questo obiettivo, questa crea un mercato per la propria valuta, impiegando passività ed attività sul mercato finanziario. Per impedire l’apprezzamento della valuta nazionale, indebolirà la propria valuta, aumentando l’offerta di moneta. Al contrario, per rafforzare la propria valuta la banca centrale può vendere attività finanziarie denominate in valuta estera, per riacquistare la propria valuta.\nEmerge quindi come la banca centrale perda il controllo sul tasso di interesse, che viene quindi determinato dal tasso di interesse internazionale. Infatti, la banca deve variare l’offerta di moneta per mantenere fisso il tasso di cambio ed eventuali variazioni tese a modificare il tasso di interesse trascinerebbero con sè variazioni del tasso di cambio.\n\n\n\nMercato Monetario\n\n\n\n\nApprezzamento Valuta\n\n\n\n\nVariabili:\n\n Δi"
  },
  {
    "objectID": "D3/IS-TR.html",
    "href": "D3/IS-TR.html",
    "title": "Il modello IS-TR",
    "section": "",
    "text": "Il modello IS-TR viene utilizzato per rappresentare l’equilibrio economico di breve periodo dal punto di vista qualitativo.\nL’ipotesi di base, proveniente da Keynes, è che nel breve periodo il livello dei prezzi sia “vischioso” (costante). In periodi di normale attività economica, in assenza di squilibri dal punto di vista geopolitico, si tratta di un’ipotesi ragionevole e supportata dai dati. In questo modello sono presenti due mercati, quello dei beni (e servizi) e quello della moneta.\nLa seconda ipotesi del modello è che i mercati raggiungano l’equilibrio, e lo raggiungano simultaneamente.\nSi partirà quindi dall’analisi del mercato dei beni, presentando la curva IS, per passare successivamente al mercato della moneta e all’analisi della regola di Taylor."
  },
  {
    "objectID": "D3/IS-TR.html#introduzione",
    "href": "D3/IS-TR.html#introduzione",
    "title": "Il modello IS-TR",
    "section": "",
    "text": "Il modello IS-TR viene utilizzato per rappresentare l’equilibrio economico di breve periodo dal punto di vista qualitativo.\nL’ipotesi di base, proveniente da Keynes, è che nel breve periodo il livello dei prezzi sia “vischioso” (costante). In periodi di normale attività economica, in assenza di squilibri dal punto di vista geopolitico, si tratta di un’ipotesi ragionevole e supportata dai dati. In questo modello sono presenti due mercati, quello dei beni (e servizi) e quello della moneta.\nLa seconda ipotesi del modello è che i mercati raggiungano l’equilibrio, e lo raggiungano simultaneamente.\nSi partirà quindi dall’analisi del mercato dei beni, presentando la curva IS, per passare successivamente al mercato della moneta e all’analisi della regola di Taylor."
  },
  {
    "objectID": "D3/IS-TR.html#la-domanda-aggregata",
    "href": "D3/IS-TR.html#la-domanda-aggregata",
    "title": "Il modello IS-TR",
    "section": "La domanda aggregata",
    "text": "La domanda aggregata\nSi è vista nella prima sezione l’identità contabile fondamentale: \\[\n    Y=C+I+G+NX\n\\] dove il termine di sinistra rappresenza l’offerta aggregata, mentre il termine di destra riporta le componenti della domanda aggregata. Piuttosto che ad un concetto contabile, conviene pensare a questa uguaglianza come ad una condizione di equilibrio: perchè il mercato dei beni sia in equilibrio, è necessario che la domanda e l’offerta si incontrino. Nel framework Keynesiano di breve periodo, si può immaginare che l’offerta sia determinata dalla domanda, che a sua volta è determinata da forze esogene.\nSi vedrà ora rapidamente quali siano le determinanti della domanda in questo modello semplificato.\n\nIl consumo delle famiglie \\(C\\) è una funzione della ricchezza \\(\\Omega\\) e del reddito ante imposte, dove le imposte \\(T\\) vengono trattate come esogene per semplicità. L’idea qui è semplicemente che maggiori disponibilità economiche producano maggiori consumi. In simboli: \\(C=\\mathcal{C}\\left(Y-\\bar{T},\\Omega\\right)\\)\n\nL’investimento delle imprese dipende dal tasso di interesse reale \\(r\\) e dalla \\(q\\) di Tobin, un parametro che cattura le aspettative degli imprenditori. Chiaramente, quando il costo del capitale è maggiore, gli investimenti diminuiranno, in periodi di tassi bassi questi aumenteranno. In simboli: \\(I=\\mathcal{I}\\left(r,q\\right)\\)\n\nLe importazioni dipendono dalla domanda interna \\(C+I+\\bar{G}\\) e dal tasso di cambio reale \\(\\epsilon\\). Una spesa interna maggiore comporterà, in media, maggiori import, e un apprezzamento del tasso di cambio reale renderà l’acquisto di beni esteri meno oneroso, incoraggiando le importazioni (e sfavorendo le esportazioni). Si ha quindi \\(IM=\\mathcal{IM}\\left(C+I+\\bar{G},\\epsilon\\right)\\)\n\nIn maniera simile, le esportazioni sono funzione della domanda estera (\\(A^*=C^*+I^*+G^*\\)) e del tasso di cambio reale: \\(EX=\\mathcal{EX}\\left(A^*,\\epsilon\\right)\\)\n\nSemplificando le due equazioni precedenti, notando che \\(A \\propto Y\\) e \\(\\bar{A^*} \\propto Y^*\\), si ottiene la funzione per le esportazioni nette: \\(NX=\\mathcal{NX}\\left(Y,Y^*,\\epsilon\\right)\\). Come si può immaginare, un maggiore prodotto interno riduce le esportazioni nettem così come un apprezzamento del tasso di cambio reale.\n\nConcludendo, si riprende ora l’equazione iniziale, indicando le componenti della domanda: \\[\n    Y = ZZ = \\mathcal{C}\\left(Y-\\bar{T},\\Omega\\right) + \\mathcal{I}\\left(r,q\\right) + \\mathcal{NX}\\left(Y,Y^*,\\epsilon\\right)\n\\] dove il PIL che soddisfa la precedente equazione è detto PIL di equilibrio e \\(ZZ\\) indica la domanda desiderata.\nConviene concentrarsi su due aspetti prima di proseguire:\n\nC’è una apparente contraddizione nella precedente funzione, in quando \\(Y\\) interviene sia aumentando il consumo che riducendo le esportazioni nette. I dati hanno dimostrato che un incremento \\(\\Delta Y\\) produce degli incrementi \\(|\\Delta C| &gt; |\\Delta NX|\\) quindi \\(\\Delta Y &gt;0 \\Rightarrow \\Delta ZZ &gt;0\\)\n\nNel breve periodo variazioni delel variabili esogene producono variazioni della domanda tali per cui \\(ZZ_{t_1}&lt;ZZ_{t_0}\\) e \\(ZZ_{t_1}&lt;Y_{t_1}\\). In questo caso, dal momento che la produzione è maggiore della domanda, le aziende inizieranno ad accumulare merce invenduta come scorte di magazzino, per poi ridurre il livello della produzione fino a che questo non incontrerà la domanda desiderata. Il processo inverso si ottiene quando la domanda desiderata supererà la produzione attuale."
  },
  {
    "objectID": "D3/IS-TR.html#la-curva-is",
    "href": "D3/IS-TR.html#la-curva-is",
    "title": "Il modello IS-TR",
    "section": "La curva IS",
    "text": "La curva IS\nLa curva IS rappresenta le combinazioni del tasso di interesse nominale \\(i\\) (si ricorda che nel breve periodo \\(\\pi \\approx 0\\) e \\(r \\approx i\\)) e del PIL \\(Y\\) che sono coerenti con l’equilibrio nel mercato dei beni.\nNella seguente rappresentazione, è possibile interagire modificando i valori delle variabili esogene. Inoltre, è consentito variare \\(i\\) ma non \\(Y\\), in quanto variazioni del prodotto porterebbero temporaneamente nelle aree di eccesso di domanda o di offerta, riassorbite nel breve periodo. Inoltre, non è possibile interagire con le determinanti di \\(\\epsilon\\) - \\(P\\) e \\(P^*\\), in quanto ciò contraddirebbe l’ipotesi 1.\n\n\n\nCurva IS\n\n\n\n\nVariabili:\n\n Δy\n Δr\n ΔT\n ΔG\n ΔΩ\n Δq\n ΔY*\n Δε"
  },
  {
    "objectID": "D3/IS-TR.html#la-curva-tr",
    "href": "D3/IS-TR.html#la-curva-tr",
    "title": "Il modello IS-TR",
    "section": "La curva TR",
    "text": "La curva TR\nLa curva TR (Taylor Rule) individua i livelli di PIL e tasso di interesse che individuano l’equilibrio sul mercato monetario. Si è visto in particolare come la regola di Taylor sia in grado di approssimare fedelmente gli interventi della banca centrale, tramite la seguente equazione: \\[\n    i = \\bar{i} + \\alpha\\left(\\pi-\\bar{\\pi}\\right) + \\beta y\n\\] dove \\(\\alpha\\) e \\(\\beta\\) rappresentano l’importanza attribuita dalla banca centrale agli scostamenti dell’inflazione dall’ inflazione target \\(\\bar{\\pi}\\) e del PIL \\(Y\\) dal suo livello naturale \\(Y^n\\), relazione catturata dall’output gap \\(y\\). \\(\\bar{i}\\) rappresenta il tasso di interesse naturale che si otterrebbe qualora \\(y=0\\) e \\(\\pi=\\bar{\\pi}\\).\nImportante notare che nel breve periodo, per l’ipotesi iniziale la regola si riduce a: \\[\n    i = \\bar{i} + \\beta y\n\\] Da questa formula derivano due osservazioni fondamentali:\n\nA differenza del mercato dei beni, l’economia non può qui mai trovarsi fuori dalla curva TR. Per definizione, infatti, la banca centrale non sceglierà punti che non siano coerenti con la regola adottata.\n\nQuando \\(y=0\\), \\(i=\\bar{i}\\) per ovvi motivi.\n\n\n\n\nCurva TR\n\n\n\n\nVariabili:\n\n y\n Δβ\n Δibar"
  },
  {
    "objectID": "D3/IS-TR.html#il-modello-is-tr",
    "href": "D3/IS-TR.html#il-modello-is-tr",
    "title": "Il modello IS-TR",
    "section": "Il modello IS-TR",
    "text": "Il modello IS-TR\nNel seguente grafico è possibile replicare l’effetto di shock reali e monetari secondo il modello IS-TR. Attenzione però: i risultati sono validi unicamente dal punto di vista qualitativo.\n\n\n\nModello IS-TR\n\n\n\n\nVariabili:\n\n ΔT\n ΔG\n ΔΩ\n Δq\n ΔY*\n Δε\n Δβ\n Δibar"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "This section presents some of my original work, such as my dissertation (in Italian) and a selection of shorter pieces.\n- Dissertation\n- Behavioural Macro"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "MPhil in Economics - Graduation year: 2027\n\n\n\nBSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "MPhil in Economics - Graduation year: 2027\n\n\n\nBSc in Economics - Graduated with 110/110 cum laude"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\nSDG Group Italy\n\nFinancial Services Consultant\n\nServed as Project Manager for a high-impact data solution for a major banking institution, overseeing delivery of secure data entry systems, ETL pipelines for structured ingestion and executive dashboards tailored for bank leadership. Ensured full project delivery on time and within scope, coordinating a cross-functional team of engineers and stakeholders.\nDesigned and implemented a fully operational custom CRM platform to manage the business unit’s pipeline, client segmentation, and opportunity tracking (demo with anonymized data available here)\nLed the redefinition of the company’s insurance vertical value proposition, working across business units to align offerings and strategy.\nCo-developed client pitch decks and strategic proposals in collaboration with unit directors; actively participated in client meetings and pitch sessions, contributing to 3 successful deal closures.\n\n\n\n\nLeonardo Assicurazioni\nLeonardo Assicurazioni is a leading insurance distributor with a robust network of over 400 consultants based in Milan. After three years in technical roles, I founded and led the company’s data analytics office for five years. I scaled the function from a solo effort to a three-analyst team by 2023, providing direct management and training.\n\nData Analytics & BI Lead\n\nBuilt core reporting tools in Python and 50+ PBI dashboards used by 500+ salespeople and 100+ staff.\nProposed and implemented KPIs across multiple departments, standardizing performance tracking and enabling data-driven decision-making at scale.\nDeveloped predictive models (OLS, KNN, Logistic Regression, Decision Tree) for client segmentation, campaign optimization, churn prediction, and insolvency risk analysis.\nConsolidated messy data (SQL, JSON, legacy systems) into clean, scalable pipelines, dramatically reducing ad hoc reporting time.\n\n\n\nUnderwriter and Sales Support Specialist\n\nSupported policy underwriting, risk assessments, and sales processes.\nIdentified workflow inefficiencies that later shaped the company’s BI strategy.\nActed as liaison between sales team and underwriting team, building cross-functional knowledge.\nDelivered insight reports manually before automated systems were in place."
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "CV",
    "section": "Certificates",
    "text": "Certificates\n\nGRE\n\n\n\n\nOverall 336\n\n\n\n\nQuant 170\n\n\n\n\nVerbal 166\n\n\n\n\n\n\nIELTS\n\n\n\n\nOverall 8/9\n\n\n\n\nReading 9/9\n\n\n\n\nListening 9/9\n\n\n\n\nWriting 7.5/9\n\n\n\n\nSpeaking 7/9"
  },
  {
    "objectID": "D3/money.html",
    "href": "D3/money.html",
    "title": "Il mercato della moneta",
    "section": "",
    "text": "Moneta e tassi di interesse hanno un ruolo centrale per la loro capacità di influenzare l’economia reale. L’obiettivo di questa sezione sarà evidenziare le determinanti della domanda di moneta e del tasso di interesse, e derivare la relazione esistente tra offerta di moneta e tassi. In una lezione successiva verranno approfondite tematiche più strettamente legate ai titoli.\nInnanzitutto, occorre dividere la moneta in circolante, ovvero la moneta correntemente utilizzata per le transazioni, e depositi bancari, ovvero le passività delle banche commerciali. Si individuano poi quattro aggregati monetari:\n\nM0 (base monetaria) = circolante + riserve delle banche commerciali\n\nM1 = circolante + depositi a vista\n\nM2 = M1 + depositi a tempo non vincolati\n\nM3 = M2 + depositi vincolati + conti presso istituti non bancari\n\nNella presente sezione, conviene pensare alla moneta nella sua formulazione M0."
  },
  {
    "objectID": "D3/money.html#introduzione",
    "href": "D3/money.html#introduzione",
    "title": "Il mercato della moneta",
    "section": "",
    "text": "Moneta e tassi di interesse hanno un ruolo centrale per la loro capacità di influenzare l’economia reale. L’obiettivo di questa sezione sarà evidenziare le determinanti della domanda di moneta e del tasso di interesse, e derivare la relazione esistente tra offerta di moneta e tassi. In una lezione successiva verranno approfondite tematiche più strettamente legate ai titoli.\nInnanzitutto, occorre dividere la moneta in circolante, ovvero la moneta correntemente utilizzata per le transazioni, e depositi bancari, ovvero le passività delle banche commerciali. Si individuano poi quattro aggregati monetari:\n\nM0 (base monetaria) = circolante + riserve delle banche commerciali\n\nM1 = circolante + depositi a vista\n\nM2 = M1 + depositi a tempo non vincolati\n\nM3 = M2 + depositi vincolati + conti presso istituti non bancari\n\nNella presente sezione, conviene pensare alla moneta nella sua formulazione M0."
  },
  {
    "objectID": "D3/money.html#la-domanda-di-moneta",
    "href": "D3/money.html#la-domanda-di-moneta",
    "title": "Il mercato della moneta",
    "section": "La domanda di moneta",
    "text": "La domanda di moneta\nLa domanda di moneta dipende essenzialmente da due parametri:\n\nVolume e frequenza delle transazioni\n\nTasso di interesse sui titoli\n\nL’intuizione è che a volumi (livelli di prodotto) maggiori, corrisponderà una crescente domanda di moneta; a tassi di interesse maggiori corrisponderà una minore domanda, in quanto gli agenti economici 1. preferiranno detenere la propria ricchezza in titoli piuttosto che in moneta e 2. diminuiranno gli investimenti visto il maggior costo del capitale\nSi può quindi scrivere che la domanda di moneta \\(M^d\\) è data da:\n\\[\n    M^d = \\mathcal{f}\\left(i\\right)PY\n\\]\nChiamando \\(M\\) l’offerta di moneta, l’equilibrio nel mercato della moneta è dato da:\n\\[\n    M = M^d = \\mathcal{f}\\left(i\\right)PY\n\\]\nQuesto permette di individuare il tasso di interesse di equilibrio, ovvero il tasso in grado di eguagliare la domanda di moneta (che è funzione di \\(i\\)) e l’offerta di moneta, determinata dalla banca centrale, per valori costanti di \\(PY\\).\nSi può utilizzare il seguente grafico interattivo per valutare gli effetti di variazioni del prodotto nominale e dell’offerta di moneta sul tasso di interesse.\n\n\n\nMercato della moneta\n\n\n\n\nVariabili:\n\n ΔPY\n ΔM"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This section provides a collection of resources on data analysis, including notebooks and practical insights drawn from my professional experience."
  },
  {
    "objectID": "data-analysis.html#marketing",
    "href": "data-analysis.html#marketing",
    "title": "Data Analysis",
    "section": "Marketing",
    "text": "Marketing\n\nUsers Cohorts Analysis\nUnderstanding user behavior over time is crucial for any business, especially in fast-growing startups where data-driven decisions can define success. One of the most effective ways to analyze retention engagement, and growth is through cohort analysis."
  },
  {
    "objectID": "macroeconomics.html",
    "href": "macroeconomics.html",
    "title": "Macroeconomics",
    "section": "",
    "text": "This section contains various resources related to macroeconomics and econometrics, including articles, research, interactive notebooks and other useful resources."
  },
  {
    "objectID": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "href": "macroeconomics.html#corso-introduttivo-di-macroeconomia-con-python",
    "title": "Macroeconomics",
    "section": "Corso introduttivo di Macroeconomia con Python",
    "text": "Corso introduttivo di Macroeconomia con Python\n\nA series of lectures on the principles of macroeconomics in italian\n\nLe variabili macroeconomiche\nA notebook exploring macroeconomic variables and their applications.\nLa curva di Phillips\nA notebook exploring the relationship between inflation and unemployment\nLa legge di Okun\nA notebook diving into the relationship between unemployment and output\nIl mercato della moneta\nFinancial markets 1 - money\nLa regola di Taylor\nA notebook showing a practical application of the Taylor Rule\nIl modello IS-TR\nTools to experiment and interact with the IS-TR model\nIl modello Mundell-Fleming (work in progress)\nTools to experiment and interact with the Mundell-Fleming model"
  },
  {
    "objectID": "notebooks/Il modello AS-AD.html",
    "href": "notebooks/Il modello AS-AD.html",
    "title": "Il modello AS-AD",
    "section": "",
    "text": "from config import *\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Il modello AS-AD.html#la-curva-as",
    "href": "notebooks/Il modello AS-AD.html#la-curva-as",
    "title": "Il modello AS-AD",
    "section": "La curva AS",
    "text": "La curva AS\nLa determinazione dei salari viene descritta dalla seguente equazione: \\[\n    W = \\mathbb{E}\\left(P\\right) \\mathcal{F}\\left(u,z\\right)\n\\] dove il salario nominale \\(W\\) dipende dal livello atteso dei prezzi \\(\\mathbb{E}\\left(P\\right)\\), dal tasso di disoccupazione \\(u\\) e dalla variabile \\(z\\), che cattura l’effetto di tutti i fattori istituzionali in grado di influenzare la contrattazione dei salari (si pensi alla presenza di un salario minimo, alla forza contrattuale dei sindacati, ecc..).\nIn altri termini, si può pensare che, in tempi favorevoli, i lavoratori riescano ad ottenere un markup elevato su una quota costante di divisione del reddito in tempi “normali” \\(S_N\\), tale per cui si può determinare il costo nominale del lavoro per unità di prodotto come: \\[\n    \\frac{WN}{Y} = \\mathbb{E}\\left(P\\right)\\left(1+\\gamma\\right)S_N\n\\] dove con \\(\\gamma\\) si è indicato il markup. Chiaramente, in tempi particolarmente svantaggiosi si potrebbe avere \\(\\gamma&lt;0\\).\nA loro volta, i prezzi per unità di prodotto vengono così determinati: \\[\n    P = \\left(1+\\mu\\right)\\frac{WN}{Y}\n\\] ovvero il livello dei prezzi \\(P\\) fissato dalle imprese è uguale al costo unitario nominale \\(\\frac{WN}{Y}\\), moltiplicato per 1 più il markup \\(\\mu\\).\nSostituendo, si ottiene: \\[\n    P = \\mathbb{E}\\left(P\\right)\\left(1+\\mu\\right)\\left(1+\\gamma\\right)S_N\n\\] da cui emerge il ruolo centrale che hanno le aspettative circa i prezzi futuri nella determinazione dei prezzi effettivi. Analizzando l’equazione, anche senza ricorrere ai calcoli si può notare come il tasso di variazione dei prezzi dipende dalla variazione congiunta dei markup e dal tasso di inflazione, ovvero: \\[\n    \\pi = \\mathbb{E}\\left(\\pi\\right) + \\Delta markup\n\\] Conviene a questo punto soffermarsi sul comportamento di \\(\\Delta markup\\). Ci si può aspettare infatti che, normalmente, \\(\\mu\\) aumenti quando la domanda è sostenuta (sebbene l’aumento possa essere stemperato dall’ingresso di nuove imprese nel mercato) e che \\(\\gamma\\) aumenti quando la disoccupazione è ridotta (e, secondo la legge di Okun, il prodotto è sopra al prodotto naturale).\nAlla luce di queste considerazioni, si può immaginare l’esistenza di una relazione tra le variazioni dei markup e l’output gap, ovvero la differenza tra il PIL corrente e il PIL potenziale, calcolato come trend di lungo periodo. In simboli: \\[\n    \\pi = \\mathbb{E}\\left(\\pi\\right) + ay\n\\] Dove con \\(y\\) si è indicato l’output gap, e \\(a\\) cattura la relazione fra \\(\\Delta markup\\) e \\(y\\). In altri termini, sfruttando la relazione inversa tra \\(u\\) e \\(y\\) calcolata da Okun, si ha che: \\[\n    \\pi = \\mathbb{E}\\left(\\pi\\right) -bu\n\\] dove con \\(u\\) si vogliono indicare variazioni nel tasso di disoccupazione, piuttosto che livelli. Prima di concludere, conviene riflettere sulle equazioni scritte sopra: possibile che non ci siano altri fenomeni, pure in un modello semplificato, in grado di avere una forte influenza sul livello dei prezzi? Per catturare questi fenomeni “anomali”, gli economisti introducono spesso li concetto di shock esogeno, ovvero un evento improvviso ed esterno al modello, in grado di impattare sulle variabili. Normalmente gli shock vengono indicati con \\(\\varepsilon\\). Riscrivendo: \\[\n    (1) \\pi = \\mathbb{E}\\left(\\pi\\right) + ay + \\varepsilon \\\\\n    (2) \\pi = \\mathbb{E}\\left(\\pi\\right) -bu + \\varepsilon\n\\] laddove il segno dello shock è arbitrario, e in questo caso uno shock sfavorevole, o “negativo”, ha segno positivo.\nPuò essere interessante ora visualizzare la curva AS (1) e la Curva di Phillips aumentata per l’inflazione e gli shock di offerta (2).\n\ndef plot_ad_as(ε=0, y=0, u=0, EP1=0):\n    \n    x = np.linspace(.1, .9, 100)\n    xa = .5\n    xa_star_phillips = xa - y + u\n    xa_star_AS = xa + y - u\n    ya_phillips = .87 - .67 * xa\n    ya_AS = .2 + .67 * xa\n    \n    phillips = .87 - .67 * x + ε + EP1\n    phillips_ = .87 - .67 * x\n\n    AS = .2 + .67 * x + ε + EP1\n    AS_ = .2 + .67 * x\n\n    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n    \n    # --- Phillips Curve ---\n    axs[0].plot([xa, xa], [0, ya_phillips], color='silver', linestyle=\":\")\n    axs[0].plot([0, xa], [ya_phillips, ya_phillips], color='silver', linestyle=\":\")\n\n    axs[0].plot(x, phillips_, color='silver', zorder=1)\n    axs[0].plot(x, phillips, zorder=2)\n\n    axs[0].scatter(xa, ya_phillips, color='#525252', zorder=3)\n    axs[0].scatter(xa_star_phillips, .87 - .67 * xa_star_phillips + ε + EP1, color=bmh_colors[1], zorder=4)\n    \n    axs[0].annotate('P=E(P)', xy=(0, ya_phillips), xytext=(2, -10), textcoords='offset points', color='#525252')\n    axs[0].annotate('U=Un', xy=(xa, 0), xytext=(2, 2), textcoords='offset points', color='#525252')\n\n    axs[0].set_xlim(0, 1)\n    axs[0].set_ylim(0, 1)\n    axs[0].set_xticks([])\n    axs[0].set_yticks([])\n    axs[0].set_xlabel(\"Disoccupazione\")\n    axs[0].set_ylabel(\"Inflazione\")\n    axs[0].set_title(\"Curva di Phillips\", loc='left')\n    \n    # --- AS Curve ---\n    axs[1].plot([xa, xa], [0, ya_AS], color='silver', linestyle=\":\")\n    axs[1].plot([0, xa], [ya_AS, ya_AS], color='silver', linestyle=\":\")\n\n    axs[1].plot(x, AS_, color='silver', zorder=1)\n    axs[1].plot(x, AS, zorder=2)\n\n    axs[1].scatter(xa, ya_AS, color='#525252', zorder=3)\n    axs[1].scatter(xa_star_AS, .2 + .67 * xa_star_AS + ε + EP1, color=bmh_colors[1], zorder=4)\n    \n    axs[1].annotate('P=E(P)', xy=(0, ya_AS), xytext=(2, -10), textcoords='offset points', color='#525252')\n    axs[1].annotate('Y=Yn', xy=(xa, 0), xytext=(2, 2), textcoords='offset points', color='#525252')\n\n    axs[1].set_xlim(0, 1)\n    axs[1].set_ylim(0, 1)\n    axs[1].set_xticks([])\n    axs[1].set_yticks([])\n    axs[1].set_xlabel(\"Prodotto\")\n    axs[1].set_ylabel(\"Inflazione\")\n    axs[1].set_title(\"Curva AS\", loc='left')\n\n    plt.show()\n    return None\n\ninteract(plot_ad_as, ε=(-.5, .5, .02), y=(-.5, .5, .02), u=(-.5, .5, .02), EP1=(-.5, .5, .02))\n\n\n\n\n&lt;function __main__.plot_ad_as(ε=0, y=0, u=0, EP1=0)&gt;"
  },
  {
    "objectID": "notebooks/La legge di Okun.html",
    "href": "notebooks/La legge di Okun.html",
    "title": "La legge di Okun",
    "section": "",
    "text": "All’inizio degli anni ’60, la crescita economica e la riduzione della disoccupazione erano temi centrali per gli economisti americani. Arthur M. Okun si specializzò nello studio del prodotto potenziale, ovvero il livello di output raggiungibile in condizioni di disoccupazione ottimale (all’epoca stimata intorno al 4%) e di una domanda sufficiente ad assorbire l’offerta disponibile.\nDal punto di vista pratico, era fondamentale quantificare l’impatto di una variazione della disoccupazione sulla produzione. Ad esempio, una riduzione del tasso di disoccupazione dal 6% al 4,5% avrebbe avvicinato il prodotto al suo livello potenziale, riducendo il “gap” tra output effettivo e potenziale. Al contrario, un aumento della disoccupazione avrebbe avuto l’effetto opposto.\nL’obiettivo di Okun era individuare una relazione quantitativa tra il prodotto nazionale lordo (PNL) e il tasso di disoccupazione. In questa sezione analizzeremo le due metodologie con cui Okun ha stimato tale relazione.\n\nfrom config import *\nfrom config import make_df_ECB\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#introduzione",
    "href": "notebooks/La legge di Okun.html#introduzione",
    "title": "La legge di Okun",
    "section": "",
    "text": "All’inizio degli anni ’60, la crescita economica e la riduzione della disoccupazione erano temi centrali per gli economisti americani. Arthur M. Okun si specializzò nello studio del prodotto potenziale, ovvero il livello di output raggiungibile in condizioni di disoccupazione ottimale (all’epoca stimata intorno al 4%) e di una domanda sufficiente ad assorbire l’offerta disponibile.\nDal punto di vista pratico, era fondamentale quantificare l’impatto di una variazione della disoccupazione sulla produzione. Ad esempio, una riduzione del tasso di disoccupazione dal 6% al 4,5% avrebbe avvicinato il prodotto al suo livello potenziale, riducendo il “gap” tra output effettivo e potenziale. Al contrario, un aumento della disoccupazione avrebbe avuto l’effetto opposto.\nL’obiettivo di Okun era individuare una relazione quantitativa tra il prodotto nazionale lordo (PNL) e il tasso di disoccupazione. In questa sezione analizzeremo le due metodologie con cui Okun ha stimato tale relazione.\n\nfrom config import *\nfrom config import make_df_ECB\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#i.-differenze-percentuali",
    "href": "notebooks/La legge di Okun.html#i.-differenze-percentuali",
    "title": "La legge di Okun",
    "section": "I. Differenze percentuali",
    "text": "I. Differenze percentuali\nIn questo primo esercizio, viene utilizzato un modello di regressione lineare con intercetta, dove le variazioni nel tasso di disoccupazione Y vengono regredite sulle variazioni percentuali di PNL X. Tutti i dati sono rilevati trimestralmente. Le serie utilizzate sono: - https://fred.stlouisfed.org/series/UNRATE per il tasso di disoccupazione - https://www.nber.org/research/data/american-business-cycle-continuity-and-change-historic-data-tables per il PNL\n\n# Carica il dataset contenente il tasso di disoccupazione da un file CSV\nunemp = pd.read_csv('UNRATE_OKUN.csv')\n\n# Converte la colonna delle date in formato datetime per una gestione più efficace delle serie temporali\nunemp['observation_date'] = pd.to_datetime(unemp['observation_date'], format='%Y-%m-%d')\n\n# Imposta la colonna delle date come indice del DataFrame per lavorare con serie temporali\nunemp = unemp.set_index('observation_date')\n\n# Raggruppa i dati per trimestre calcolando la media del tasso di disoccupazione per ogni periodo\nunemp = unemp.groupby(pd.Grouper(freq='Q'))['UNRATE'].mean().to_frame()\n\n# Converte l'indice in un PeriodIndex con frequenza trimestrale per una gestione più accurata delle serie storiche\nunemp.index = pd.PeriodIndex(unemp.index, freq='Q')\n\n# Calcola la variazione trimestrale del tasso di disoccupazione\nunemp['change_unemp'] = unemp['UNRATE'].diff()\n\n# Rimuove eventuali valori NaN generati dalla differenziazione\nunemp = unemp.dropna()\n\n# Filtra i dati fino al quarto trimestre del 1960 per allinearsi al periodo di interesse\nunemp = unemp[unemp.index &lt;= '1960Q4']\n\n# Visualizza le prime righe del DataFrame risultante\nunemp.head()\n\n\n\n\n\n\n\n\nUNRATE\nchange_unemp\n\n\nobservation_date\n\n\n\n\n\n\n1948Q2\n3.666667\n-0.066667\n\n\n1948Q3\n3.766667\n0.100000\n\n\n1948Q4\n3.833333\n0.066667\n\n\n1949Q1\n4.666667\n0.833333\n\n\n1949Q2\n5.866667\n1.200000\n\n\n\n\n\n\n\n\n# Carica il dataset contenente il prodotto nazionale lordo (PNL) da un file CSV\ngnp = pd.read_csv('abcq.csv')\n\n# Crea una nuova colonna per la data, combinando l'anno e il trimestre\ngnp['observation_date'] = gnp['year'].astype(str) + 'Q' + gnp['quarter'].astype(str)\n\n# Imposta la colonna 'observation_date' come indice del DataFrame\ngnp = gnp.set_index('observation_date')\n\n# Converte l'indice in un PeriodIndex con frequenza trimestrale per una gestione più accurata delle serie storiche\ngnp.index = pd.PeriodIndex(gnp.index, freq='Q')\n\n# Seleziona solo la colonna 'GNP' e la converte in un DataFrame\ngnp = gnp['GNP'].to_frame()\n\n# Calcola la variazione percentuale trimestrale del prodotto nazionale lordo\ngnp['pct_change_gnp'] = gnp['GNP'].pct_change() * 100\n\n# Rimuove eventuali valori NaN generati dalla differenziazione\ngnp = gnp.dropna()\n\n# Filtra i dati per mantenere solo il periodo tra il secondo trimestre del 1948 e il quarto trimestre del 1960\ngnp = gnp[(gnp.index &lt;= '1960Q4') & (gnp.index &gt;= '1948Q2')]\n\n# Visualizza le prime righe del DataFrame risultante\ngnp.head()\n\n\n\n\n\n\n\n\nGNP\npct_change_gnp\n\n\nobservation_date\n\n\n\n\n\n\n1948Q2\n257.5\n3.000000\n\n\n1948Q3\n264.5\n2.718447\n\n\n1948Q4\n265.9\n0.529301\n\n\n1949Q1\n260.5\n-2.030839\n\n\n1949Q2\n257.0\n-1.343570\n\n\n\n\n\n\n\n\n# Unisce i DataFrame 'unemp' e 'gnp' utilizzando l'indice come chiave di unione\ndf = pd.merge(unemp, gnp, right_index=True, left_index=True)\n\n# Definisce la variabile indipendente X (le variazioni percentuali del PNL)\nX = df['pct_change_gnp']\n\n# Definisce la variabile dipendente Y (le variazioni del tasso di disoccupazione)\nY = df['change_unemp']\n\n# Aggiunge un termine costante al modello di regressione per tener conto dell'intercetta\nX = sm.add_constant(X)\n\n# Crea il modello di regressione lineare (OLS) con Y come dipendente e X come indipendente\nmodel = sm.OLS(Y, X)\n\n# Adatta il modello ai dati\nresults = model.fit()\n\n# Stampa i risultati della regressione\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.631\nModel:                            OLS   Adj. R-squared:                  0.623\nMethod:                 Least Squares   F-statistic:                     83.74\nDate:                Fri, 21 Mar 2025   Prob (F-statistic):           3.51e-12\nTime:                        18:54:20   Log-Likelihood:                -19.032\nNo. Observations:                  51   AIC:                             42.06\nDf Residuals:                      49   BIC:                             45.93\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              0.4238      0.065      6.546      0.000       0.294       0.554\npct_change_gnp    -0.2669      0.029     -9.151      0.000      -0.325      -0.208\n==============================================================================\nOmnibus:                        6.492   Durbin-Watson:                   1.585\nProb(Omnibus):                  0.039   Jarque-Bera (JB):                5.400\nSkew:                           0.713   Prob(JB):                       0.0672\nKurtosis:                       3.714   Cond. No.                         3.12\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nL’interpretazione dei coefficienti è immediata: tenendo fisso il PNL il tasso di disoccupazione aumenterà di .42 punti da un trimestre all’altro, in quanto la crescita della forza lavoro e i processi tecnologici spingeranno in basso il rapporto occupati/forza lavoro attiva. Per ogni aumento di 1 punto percentuale del PNL, il tasso di disoccupazione diminuirà di .27 punti. Di converso, un aumento del tasso di disoccupazione di 1 punto percentuale comporterà una riduzione del PNL del 3.7%.\nSi può notare che i valori ottenuti non sono dissimili da quelli dell’articolo originale, che riporta una approssimazione: \\[\n    Y = .3 - .3X\n\\] In questi casi, il metodo di rilevazione dei dati ed eventuali manipolazioni successive possono essere responsabili di leggere variazioni nei coefficienti.\n\n# Calcola le previsioni per la variazione del tasso di disoccupazione usando il modello di regressione\ndf['pred_unemp_change'] = results.predict(X)\n\n# Ordina i dati in base alla variazione percentuale del PNL (X)\nsorted_data = df.sort_values(by='pct_change_gnp')\n\n# Crea una figura con un solo subplot\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunge una linea orizzontale a y=0 per facilitare la lettura del grafico\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Crea un grafico a dispersione per visualizzare le osservazioni reali (variabile dipendente vs variabile indipendente)\nax.scatter(df['pct_change_gnp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Aggiunge la linea di regressione (legge di Okun) sul grafico\nax.plot(sorted_data['pct_change_gnp'], sorted_data['pred_unemp_change'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Imposta il titolo del grafico e lo allinea a sinistra\nax.set_title(f\"US, 1948-1960\", loc='left')\n\n# Imposta le etichette degli assi\nax.set_xlabel('Variazione Del PNL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunge la griglia al grafico per facilitare la lettura\nax.grid(True)\n\n# Aggiunge la legenda al grafico\nax.legend()\n\n# Ottimizza il layout del grafico per una visualizzazione migliore\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#ii.-elasticità",
    "href": "notebooks/La legge di Okun.html#ii.-elasticità",
    "title": "La legge di Okun",
    "section": "II. Elasticità",
    "text": "II. Elasticità\nQuesto metodo è molto importante dal punto di vista pratico, in quanto consente di associare dei coefficienti ai livelli di PNL e tasso di occupazione, piuttosto che alle loro variazioni percentuali. Questo servirà in seguito per stimare il PNL potenziale. In particolare, si ipotizza che:\n\nci sia elesticità costante (\\(a\\)) fra il rapporto tra prodotto osservato (\\(A\\)-actual) e potenziale (\\(P\\)) ed il rapporto fra tasso di occupazione (\\(N\\)) e tasso di occupazione potenziale (\\(N_F\\)). In simboli: \\(\\frac{N}{N_F} = \\left( \\frac{A}{P} \\right)^a\\)\nil prodotto potenziale cresca ad un tasso costante \\(r\\) in funzione del tempo: \\(P-t = P_0 e^{rt}\\)\n\nIl risultato restituisce l’equazione del modello, i cui parametri verranno stimati tramite OLS: \\[\\ln N_t = \\ln \\frac{N_F}{P_0^a} + a \\ln A_t - (ar)t\\]\n\n# Calcola il tasso di occupazione sottraendo il tasso di disoccupazione da 100\ndf['emprate'] = 100 - df['UNRATE']\n\n# Calcola il logaritmo naturale del tasso di occupazione\ndf['logN'] = np.log(df['emprate'])\n\n# Calcola il logaritmo naturale del Prodotto Nazionale Lordo (PNL)\ndf['logA'] = np.log(df['GNP'])\n\n# Crea una variabile trend che va da 1 fino al numero di osservazioni (usata per catturare l'effetto di trend nel modello)\ndf['trend'] = range(1, 1+len(df))\n\n# Mostra le prime righe del DataFrame risultante per una rapida verifica\ndf.head()\n\n\n\n\n\n\n\n\nUNRATE\nchange_unemp\nGNP\npct_change_gnp\npred_unemp_change\nemprate\nlogN\nlogA\ntrend\n\n\nobservation_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n1948Q2\n3.666667\n-0.066667\n257.5\n3.000000\n-0.376806\n96.333333\n4.567814\n5.551020\n1\n\n\n1948Q3\n3.766667\n0.100000\n264.5\n2.718447\n-0.301666\n96.233333\n4.566776\n5.577841\n2\n\n\n1948Q4\n3.833333\n0.066667\n265.9\n0.529301\n0.282564\n96.166667\n4.566083\n5.583120\n3\n\n\n1949Q1\n4.666667\n0.833333\n260.5\n-2.030839\n0.965804\n95.333333\n4.557380\n5.562603\n4\n\n\n1949Q2\n5.866667\n1.200000\n257.0\n-1.343570\n0.782388\n94.133333\n4.544712\n5.549076\n5\n\n\n\n\n\n\n\n\n# Definisce la variabile dipendente Y (logaritmo naturale del tasso di occupazione)\nY = df['logN']\n\n# Definisce la matrice delle variabili indipendenti X (logaritmo del PNL e trend)\nX = df[['logA', 'trend']]\n\n# Aggiunge una costante a X per includere l'intercetta nel modello\nX = sm.add_constant(X)\n\n# Crea un modello di regressione lineare ordinario (OLS) con Y come variabile dipendente e X come variabili indipendenti\nmodel = sm.OLS(Y,X)\n\n# Calcola i risultati del modello\nresults = model.fit()\n\n# Stampa il sommario dei risultati della regressione\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   logN   R-squared:                       0.854\nModel:                            OLS   Adj. R-squared:                  0.848\nMethod:                 Least Squares   F-statistic:                     140.6\nDate:                Fri, 21 Mar 2025   Prob (F-statistic):           8.53e-21\nTime:                        18:54:21   Log-Likelihood:                 196.88\nNo. Observations:                  51   AIC:                            -387.8\nDf Residuals:                      48   BIC:                            -382.0\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.9807      0.101     29.463      0.000       2.777       3.184\nlogA           0.2853      0.018     15.662      0.000       0.249       0.322\ntrend         -0.0043      0.000    -16.516      0.000      -0.005      -0.004\n==============================================================================\nOmnibus:                        3.874   Durbin-Watson:                   0.521\nProb(Omnibus):                  0.144   Jarque-Bera (JB):                3.453\nSkew:                          -0.636   Prob(JB):                        0.178\nKurtosis:                       2.930   Cond. No.                     4.24e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.24e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nDove il coefficiente di logA è l’elasticità dell’output al tasso di occupazione; il coefficiente del tempo è, per definizione, il prodotto tra il coefficiente di logA e il tasso di crescita costante del prodotto potenziale; l’intercetta restituisce il benchmark \\(P_0\\) per ogni livello di \\(N_F\\), che verrà preso pari a 96.\nIterate regressioni hanno portato Okun al seguente risultato: \\[\n    P = A \\left( 1+ .032 \\left( U-4 \\right) \\right)\n\\]\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Crea una nuova figura per il grafico 3D con dimensioni personalizzate\nfig = plt.figure(figsize=(10, 7))\n\n# Aggiunge un subplot 3D alla figura\nax = fig.add_subplot(111, projection='3d')\n\n# Crea un grafico scatter 3D con logA, trend e logN, etichettato come 'Osservazioni'\nax.scatter(df['logA'], df['trend'], df['logN'], label='Osservazioni', alpha=0.7)\n\n# Definisce i valori di intervallo per logA e trend (in modo da poter creare una superficie)\nlogA_range = np.linspace(df['logA'].min(), df['logA'].max(), 20)\ntrend_range = np.linspace(df['trend'].min(), df['trend'].max(), 20)\n\n# Crea una griglia di valori per logA e trend\nlogA_grid, trend_grid = np.meshgrid(logA_range, trend_range)\n\n# Calcola i valori adattati della superficie utilizzando i parametri stimati dal modello di regressione\nfitted_values = results.params['const'] + results.params['logA'] * logA_grid + results.params['trend'] * trend_grid\n\n# Aggiunge la superficie al grafico 3D, colorandola con un colore specifico e un'alpha di trasparenza\nax.plot_surface(logA_grid, trend_grid, fitted_values, color=bmh_colors[1], alpha=0.5)\n\n# Imposta le etichette degli assi\nax.set_xlabel('logA')\nax.set_ylabel('Trend')\nax.set_zlabel('logN')\n\n# Imposta il titolo del grafico\nax.set_title('Legge Di Okun, livelli')\n\n# Modifica l'angolazione della visualizzazione per rendere più chiara la superficie\nax.view_init(elev=20, azim=45)\n\n# Aggiunge la legenda al grafico\nplt.legend()\n\n# Mostra il grafico\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcola il PNL potenziale utilizzando la formula di Okun (assumendo un tasso di disoccupazione ottimale del 4%)\ndf['potential_gnp'] = df['GNP']*(1+.032*(df['UNRATE']-4))\n\n# Crea un grafico con un subplot\nfig, ax = plt.subplots(1,1)\n\n# Filtra i dati dal primo trimestre del 1954 e traccia il PNL osservato\ndf[df.index&gt;='1954Q1']['GNP'].plot(label='PNL', ax=ax)\n\n# Filtra i dati dal primo trimestre del 1954 e traccia il PNL potenziale calcolato\ndf[df.index&gt;='1954Q1']['potential_gnp'].plot(label='PNL potenziale', ax=ax)\n\n# Aggiunge etichette agli assi\nplt.ylabel('PNL (in miliardi di dollari)')\nplt.xlabel('Trimestre')\n\n# Aggiunge il titolo al grafico, posizionandolo a sinistra\nplt.title('Calcolo Del PNL Potenziale Secondo Okun', loc='left')\n\n# Mostra la legenda per distinguere le due linee\nplt.legend()\n\n# Regola la disposizione del layout per ottimizzare lo spazio\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/La legge di Okun.html#e-oggi",
    "href": "notebooks/La legge di Okun.html#e-oggi",
    "title": "La legge di Okun",
    "section": "E oggi?",
    "text": "E oggi?\n\n# Funzione per estrarre i dati dal datawarehouse della BCE\ndef make_df_ECB(key, obs_name):\n    \"\"\"Estrae i dati dal datawarehouse della BCE\"\"\"\n    url_ = 'https://sdw-wsrest.ecb.europa.eu/service/data/'  # URL di base per il servizio web della BCE\n    format_ = '?format=csvdata'  # Impostazione del formato dei dati in CSV\n    df = pd.read_csv(url_+key+format_)  # Legge i dati CSV dal servizio web BCE usando la chiave\n    df = df[['TIME_PERIOD', 'OBS_VALUE']]  # Seleziona le colonne di interesse (periodo e valore osservato)\n    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])  # Converte il periodo in formato datetime\n    df = df.set_index('TIME_PERIOD')  # Imposta il periodo come indice del dataframe\n    df.columns = [obs_name]  # Rinomina la colonna con il nome della variabile\n    return df\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp = make_df_ECB(unemp_rate_key, 'unemp')  # Ottieni i dati dal database ECB\nunemp['change_unemp'] = unemp['unemp'].diff()  # Calcola la variazione del tasso di disoccupazione\n\n# Chiave per il PIL (GDP)\ngdp_key = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'  # Codice identificativo del PIL\ngdp = make_df_ECB(gdp_key, 'gdp')  # Ottieni i dati dal database ECB\ngdp['pct_change_gdp'] = gdp['gdp'].pct_change()*100  # Calcola la variazione percentuale del PIL\n\n# Unisci i dati del tasso di disoccupazione e del PIL\ndf = pd.merge(unemp, gdp, right_index=True, left_index=True)\ndf = df.dropna()  # Rimuove le righe con valori NaN\n\n# Mostra le prime righe del dataframe risultante\ndf.head()\n\n\n\n\n\n\n\n\nunemp\nchange_unemp\ngdp\npct_change_gdp\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n2000-04-01\n9.128157\n-0.223128\n2.433026e+06\n0.896372\n\n\n2000-07-01\n8.982247\n-0.145910\n2.449339e+06\n0.670484\n\n\n2000-10-01\n8.751499\n-0.230748\n2.460466e+06\n0.454298\n\n\n2001-01-01\n8.532471\n-0.219028\n2.487753e+06\n1.109010\n\n\n2001-04-01\n8.472304\n-0.060167\n2.489155e+06\n0.056341\n\n\n\n\n\n\n\n\n# Definizione delle variabili indipendente (X) e dipendente (Y)\nX = df['pct_change_gdp']  # Variabile indipendente: variazione percentuale del PIL\nY = df['change_unemp']  # Variabile dipendente: variazione del tasso di disoccupazione\n\n# Aggiunta dell'intercetta (costante) al modello\nX = sm.add_constant(X)  # Aggiunge una colonna di 1 alla variabile indipendente per includere l'intercetta\n\n# Creazione del modello di regressione lineare (OLS)\nmodel = sm.OLS(Y, X)  # OLS: Ordinary Least Squares (minimi quadrati ordinari)\n\n# Fitting del modello (stima dei parametri)\nresults = model.fit()  # Calcola i risultati del modello di regressione\n\n# Stampa del sommario dei risultati\nprint(results.summary())  # Visualizza un riepilogo completo dei risultati della regressione\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                 -0.005\nMethod:                 Least Squares   F-statistic:                    0.5490\nDate:                Fri, 21 Mar 2025   Prob (F-statistic):              0.460\nTime:                        18:54:22   Log-Likelihood:                 2.0828\nNo. Observations:                  99   AIC:                           -0.1656\nDf Residuals:                      97   BIC:                             5.025\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst             -0.0286      0.024     -1.170      0.245      -0.077       0.020\npct_change_gdp    -0.0101      0.014     -0.741      0.460      -0.037       0.017\n==============================================================================\nOmnibus:                       42.392   Durbin-Watson:                   0.743\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              109.819\nSkew:                           1.560   Prob(JB):                     1.42e-24\nKurtosis:                       7.110   Cond. No.                         1.84\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIl modello non passa il test di zero-slopes, e nessun coefficiente è significativo. Il modello mostra autocorrelazione positiva (il limite inferiore di DW è intorno a 1.5). come si può constatare dal seguente grafico, la relazione tra variazione nel tasso di disoccupazione e variazione percentuale del PIL è molto debole in Europa, nel periodo considerato.\n\n# Calcolo delle previsioni della variazione del tasso di disoccupazione\ndf['pred_unemp_change'] = results.predict(X)\n\n# Ordinamento dei dati per la variabile indipendente (PIL)\nsorted_data = df.sort_values(by='pct_change_gdp')\n\n# Creazione della figura e dell'asse per il grafico\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunta di una linea orizzontale a y=0\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Creazione del grafico a dispersione (scatter plot) per le osservazioni effettive\nax.scatter(df['pct_change_gdp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Aggiunta della retta di regressione (Legge di Okun)\nax.plot(sorted_data['pct_change_gdp'], sorted_data['pred_unemp_change'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Titolo e etichette degli assi\nax.set_title(f\"EU, 2000-2024\", loc='left')\nax.set_xlabel('Variazione Del PIL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunta di una griglia al grafico\nax.grid(True)\n\n# Aggiunta della legenda\nax.legend()\n\n# Ottimizzazione della disposizione del grafico\nplt.tight_layout()\n\n\n\n\n\n\n\n\nCome esercizio, ho replicato il modello aggiungendo i lag di GDP e disoccupazione (come suggerito dal test di DW) - il modello ora riporta un elevato \\(R^2\\) e coefficienti significativi, tuttavia ha perso il sapore caratteristico dell’originale curva di Okun.\n\n# Creazione delle variabili laggate per la variazione del PIL e per la disoccupazione\ndf['pct_change_gdp_lag1'] = df['pct_change_gdp'].shift(1)\ndf['unemp_lag1'] = df['change_unemp'].shift(1)\n\n# Rimozione dei valori nulli generati dalla creazione dei lag\ndf = df.dropna()\n\n# Creazione delle variabili indipendenti (incluso il termine costante)\nX = df[['pct_change_gdp', 'pct_change_gdp_lag1', 'unemp_lag1']]\nX = sm.add_constant(X)\n\n# Variabile dipendente (cambio del tasso di disoccupazione)\nY = df['change_unemp']\n\n# Creazione del modello di regressione dinamica con stima robusta degli errori (HAC)\nmodel_dyn = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 4})\n\n# Visualizzazione dei risultati del modello\nprint(model_dyn.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           change_unemp   R-squared:                       0.760\nModel:                            OLS   Adj. R-squared:                  0.752\nMethod:                 Least Squares   F-statistic:                     272.3\nDate:                Fri, 21 Mar 2025   Prob (F-statistic):           3.23e-46\nTime:                        18:54:22   Log-Likelihood:                 71.457\nNo. Observations:                  98   AIC:                            -134.9\nDf Residuals:                      94   BIC:                            -124.6\nDf Model:                           3                                         \nCovariance Type:                  HAC                                         \n=======================================================================================\n                          coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.0219      0.015      1.476      0.140      -0.007       0.051\npct_change_gdp         -0.0260      0.012     -2.166      0.030      -0.050      -0.002\npct_change_gdp_lag1    -0.0766      0.005    -16.759      0.000      -0.086      -0.068\nunemp_lag1              0.6233      0.042     14.905      0.000       0.541       0.705\n==============================================================================\nOmnibus:                        5.478   Durbin-Watson:                   2.021\nProb(Omnibus):                  0.065   Jarque-Bera (JB):                6.170\nSkew:                           0.289   Prob(JB):                       0.0457\nKurtosis:                       4.085   Cond. No.                         8.21\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 4 lags and without small sample correction\n\n\n\n# Previsione dei cambiamenti nel tasso di disoccupazione usando il modello dinamico\ndf['pred_unemp_change_2'] = model_dyn.predict(X)\n\n# Ordinamento dei dati in base alla variazione del PIL\nsorted_data = df.sort_values(by='pct_change_gdp')\n\n# Creazione del grafico\nfig, ax = plt.subplots(1, 1)\n\n# Aggiunta della linea orizzontale per il valore zero\nax.axhline(y=0, color='silver', linewidth=2)\n\n# Creazione dello scatter plot per le osservazioni\nax.scatter(df['pct_change_gdp'], df['change_unemp'], alpha=0.7, label=\"Osservazioni\", color=bmh_colors[0])\n\n# Creazione della linea della Legge di Okun basata sulle previsioni\nax.scatter(sorted_data['pct_change_gdp'], sorted_data['pred_unemp_change_2'], \n        color=bmh_colors[1], label=\"Legge Di Okun\")\n\n# Titolo del grafico\nax.set_title(f\"EU, 2000-2024\", loc='left')\n\n# Etichette degli assi\nax.set_xlabel('Variazione Del PIL')\nax.set_ylabel('Variazione Del Tasso Di Disoccupazione')\n\n# Aggiunta della griglia\nax.grid(True)\n\n# Aggiunta della legenda\nax.legend()\n\n# Ottimizzazione del layout\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html",
    "href": "notebooks/Le variabili macroeconomiche.html",
    "title": "Le variabili macroeconomiche",
    "section": "",
    "text": "In questa prima sezione verranno presentate alcune delle principali variabili macroeconomiche, e si mostrerà come estrarre le serie storiche dai database di Eurostat e BCE utilizzando Python. In ordine, verranno presentate:\n1. PIL, ovvero la produzione aggregata dell’economia, indicativo del grado di salute dell’economia 2. tasso di disoccupazione, che riveste fondamentale importanza soprattutto per la FED 3. inflazione, di cruciale rilevanza in Europa, in quanto mantenere la stabilità dei prezzi è il principale compito della BCE\nfrom config import *\nfrom config import make_df_ECB\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "href": "notebooks/Le variabili macroeconomiche.html#produzione-aggregata",
    "title": "Le variabili macroeconomiche",
    "section": "Produzione Aggregata",
    "text": "Produzione Aggregata\n\nDefinizione\nLa misura della produzione aggregata nella contabilità nazionale è chiamata prodotto interno lordo, o PIL. Il PIL può essere definito in tre modi equivalenti:\n\n\nIl PIL è il valore dei beni e dei servizi finali prodotti nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma del valore aggiunto nell’economia in un dato periodo di tempo\n\n\nIl PIL è la somma dei redditi dell’economia in un dato periodo di tempo\n\n\nDalla definizione (3) deriva che, in una economia, produzione aggregata e reddito aggregato siano sempre uguali\n\n\nCome si misura il PIL?\nNormalmente, le informazioni sono raccolte dalle autorità fiscali di un paese:\n\n\nLe imprese registrano le proprie vendite\n\n\nLe imprese pagano tasse sul valore aggiunto\n\n\nGli individui dichiarano il reddito percepito\n\n\n\n\nPIL nominale e PIL reale\nIl PIL nominale è la somma del valore dei beni finali valutati al loro prezzo corrente, dunque la sua crescita è influenzata da I. l’aumento della produzione II. l’aumento dei prezzi. Il PIl reale è la somma del valore dei beni finali valutati a prezzi costanti: di solito si utilizzano i prezzi di un anno di riferimento\n\n\nPIL pro capite\nSpesso, per misurare il tenore di vita in un paese, si utilizza il PIL pro capite, ovvero il PIL reale diviso per la popolazione del paese\n\n# Creazione del DataFrame con i dati del PIL dell'Area Euro\n# La chiave specificata corrisponde al PIL reale trimestrale in euro a prezzi concatenati\nPIL_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.N'\npil = make_df_ECB(PIL_KEY, 'PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie utilizzata rappresenta il PIL reale dell'Area Euro con dati trimestrali (Q).\nI valori sono espressi in miliardi di euro (EUR) a prezzi concatenati (LR), il che significa\nche sono corretti per l'inflazione e consentono un confronto diretto nel tempo.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse: \n  - Crisi del debito europeo (2009) con una significativa contrazione del PIL.\n  - Pandemia di Covid-19 (2020) con una forte caduta seguita da una ripresa.\n\"\"\"\n\n# Creazione del grafico del PIL\npil.plot()\nplt.title('PIL Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('PIL (Miliardi di euro, prezzi concatenati)')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', pil.loc['2009-01-01']), \n             xytext=(-20, 60),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', pil.loc['2020-04-01']), \n             xytext=(-40, -40),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTasso di crescita del PIL\nSpesso, si utilizza il concetto di crescita del PIL per indicare espansioni (periodi di crescita positiva) e recessioni (almeno due trimestri consecutivi di crescita negativa). Il tasso di crescita del PIL è semplicemente definito come: $ Y_t - Y_{t-1} $\n\n# Creazione del DataFrame con i dati della crescita del PIL reale\n# La chiave specificata corrisponde alla crescita del PIL reale trimestrale in termini percentuali\nREAL_GDP_GROWTH_KEY = 'MNA/Q.Y.I9.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.GY'\nreal_gdp_growth = make_df_ECB(REAL_GDP_GROWTH_KEY, 'Crescita PIL')\n\n\"\"\"\nDescrizione della serie:\nLa serie rappresenta la crescita percentuale del PIL reale dell'Area Euro con dati trimestrali (Q).\nMisura il tasso di variazione del PIL rispetto al trimestre precedente, esprimendolo in termini percentuali.\n\nCaratteristiche principali:\n- Frequenza: Trimestrale (Q)\n- Valuta: Euro (EUR) a prezzi concatenati (LR)\n- Indicatore: Crescita percentuale del PIL reale (GY)\n- Area geografica: Area Euro (I9)\n- Settore: Settore istituzionale totale (S1)\n- Fonte: BCE (Banca Centrale Europea)\n- Periodi di interesse:\n  - Crisi del debito europeo (2009) con crescita negativa e recessione prolungata.\n  - Pandemia di Covid-19 (2020) con una caduta storica seguita da un rimbalzo.\n\"\"\"\n\n# Creazione del grafico della crescita del PIL\nreal_gdp_growth.plot()\nplt.title('Crescita del PIL nell\\'Area Euro', loc='left')  # Titolo del grafico posizionato a sinistra\nplt.xlabel('Trimestre')  # Etichetta dell'asse x\nplt.ylabel('Crescita %')  # Etichetta dell'asse y\nplt.legend().set_visible(False)  # Nasconde la legenda per migliorare la leggibilità\n\n# Aggiunta di una linea orizzontale per evidenziare il valore zero (assenza di crescita)\nplt.axhline(y=0, color='#525252', linestyle='--', linewidth=1)\n\n# Annotazione della crisi del debito europeo (2009)\nplt.annotate('Crisi del debito', \n             xy=('2009Q1', real_gdp_growth.loc['2009-01-01']), \n             xytext=(-100, -30),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=0.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\n# Annotazione della pandemia di Covid-19 (2020)\nplt.annotate('Pandemia di Covid', \n             xy=('2020Q2', real_gdp_growth.loc['2020-04-01']), \n             xytext=(-150, 10),  # Offset del testo\n             textcoords='offset points',  # Specifica l'offset in punti\n             arrowprops=dict(facecolor='#525252', lw=1.5),  # Stile della freccia\n             color='#525252')  # Colore del testo\n\nplt.tight_layout()  # Ottimizzazione del layout per evitare sovrapposizioni\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLa composizione del PIL\nPer comprendere cosa determini la domanda di beni, si può scomporre la produzione aggregata in base alla tipologia di acquirente dei beni. Si possono individuare cinque componenti:\n\n\nIl consumo (C), ovvero beni e servizi acquistati dai consumatori, ovvero le famiglie\n\n\nL’investimento (I) che include l’investimento non residenziale (impianti e macchinari) da parte delle aziende, e l’investimento residenziale da parte delle famiglie\n\n\nLa spesa pubblica in beni e servizi (G), ovvero i beni acquistati dallo stato e dagli enti pubblici. Si noti che non fanno parte di questa categoria pensioni, servizi assistenziali quali il reddito di cittadinanza, o gli interessi pagati sul debito.\n\n\nImportazioni (IM), che entrano nell’equazione con segno negativo\n\n\nEsportazioni (X), che entrano nell’equazione con segno positivo\n\n\nSi ha dunque che: \\(Y = C + I + G + (X - IM)\\) Dove spesso si usa parlare di esportazioni nette o saldo commerciale del termine \\(X - IM\\).\n\n# Definisco le chiavi per i vari indicatori economici relativi all'Italia\n# La struttura di ogni chiave segue il formato: \n# MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N\n\nIT_gdp_key = 'MNA/Q.Y.IT.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.V.N'  # PIL (Prodotto Interno Lordo)\n\nIT_inv_key = 'MNA/Q.Y.IT.W0.S1.S1.D.P51G.N11G._T._Z.EUR.V.N'  # Investimenti\n\nIT_cons_key = 'MNA/Q.Y.IT.W0.S1M.S1.D.P31._Z._Z._T.EUR.V.N'  # Consumi\n\nIT_gov_exp_key = 'MNA/Q.Y.IT.W0.S13.S1.D.P3._Z._Z._T.EUR.V.N'  # Spesa Pubblica\n\nIT_imp_key = 'MNA/Q.Y.IT.W1.S1.S1.C.P7._Z._Z._Z.EUR.V.N'  # Importazioni\n\nIT_exp_key = 'MNA/Q.Y.IT.W1.S1.S1.D.P6._Z._Z._Z.EUR.V.N'  # Esportazioni\n\n# Creo i DataFrame per ogni indicatore economico utilizzando le chiavi\nIT_gdp = make_df_ECB(IT_gdp_key, 'GDP')  # Prodotto Interno Lordo (PIL)\nIT_inv = make_df_ECB(IT_inv_key, 'INV')  # Investimenti\nIT_cons = make_df_ECB(IT_cons_key, 'CONS')  # Consumi\nIT_gov_exp = make_df_ECB(IT_gov_exp_key, 'GOV_EXP')  # Spesa Pubblica\nIT_imp = make_df_ECB(IT_imp_key, 'IMP')  # Importazioni\nIT_exp = make_df_ECB(IT_exp_key, 'EXP')  # Esportazioni\n\n# Unisco tutti i DataFrame in uno solo per l'analisi aggregata delle spese\nimport functools as ft\ndfs = [IT_gdp, IT_inv, IT_cons, IT_gov_exp, IT_imp, IT_exp]\nIT_expenditures = ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), dfs)\n\n# Calcolo la bilancia commerciale (BOT) come la differenza tra esportazioni e importazioni\nIT_expenditures['BOT'] = IT_expenditures['EXP'] - IT_expenditures['IMP']\n\n# Creo un grafico a barre impilate per visualizzare la spesa aggregata\nax = IT_expenditures[['INV', 'CONS', 'GOV_EXP', 'BOT']][-32:].plot.bar(stacked=True, figsize=(9, 5))\n\n# Modifico le etichette dell'asse x per visualizzare l'anno e il trimestre\nax.set_xticklabels([f'{x.year} Q{x.quarter}' for x in IT_expenditures.index[-32:]])\n\n# Aggiungo il titolo e le etichette degli assi\nplt.title('Spesa Aggregata - IT')\nplt.xlabel('Trimestre')\nplt.ylabel('Spesa')\n\n# Aggiungo una legenda con le categorie di spesa\nax.legend(['I', 'C', 'G', 'X-IM', 'Y'],\n          title='Categorie',\n          loc='upper left',      \n          frameon=True,\n          fontsize='x-small',\n          ncol=4\n          )\n\n# Adatto il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPIL e PNL\nIn alcuni casi, è utile distinguere tra: PIL, che misura il valore totale dei beni e servizi prodotti all’interno di un paese, indipendentemente dalla proprietà delle risorse e PNL (Prodotto Nazionale Lordo), che misura il valore totale dei beni e servizi prodotti da cittadini di un paese, indipendentemente da dove si trovino nel mondo. Sono emblematici il caso tedesco e quello irlandese.\n\n# Definiamo le colonne di interesse per i dati\ncols = ['geo\\TIME_PERIOD', '2020', '2021', '2022', '2023']\n\n# Otteniamo i dati del Prodotto Nazionale Lordo (PNL) pro capite\ngni_pc_tuples = eurostat.get_data('nama_10_pp')  # Recupera i dati dal database Eurostat\ngni_pc = pd.DataFrame(gni_pc_tuples[1:], columns=gni_pc_tuples[0])  # Crea il DataFrame\ngni_pc = gni_pc[cols]  # Seleziona solo le colonne di interesse\ngni_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne per chiarezza\n\n# Otteniamo i dati del Prodotto Interno Lordo (PIL) pro capite\ngdp_pc_tuples = eurostat.get_data('nama_10_pc')  # Recupera i dati del PIL pro capite\ngdp_pc = pd.DataFrame(gdp_pc_tuples[1:], columns=gdp_pc_tuples[0])  # Crea il DataFrame\ngdp_pc = gdp_pc.query(\"unit=='CP_PPS_EU27_2020_HAB' & na_item == 'B1GQ'\")  # Filtro per unità e tipo di dato\ngdp_pc = gdp_pc[cols]  # Seleziona solo le colonne di interesse\ngdp_pc.columns = ['COUNTRY', '2020', '2021', '2022', '2023']  # Rinomina le colonne\n\n# Definiamo la lista dei paesi per i quali vogliamo visualizzare i dati\ncountries = ['IT', 'DE', 'IE']\n\n# Creiamo un grafico a sottotrame per visualizzare i dati\nfig, axs = plt.subplots(1, 3, sharey=True, figsize=(9, 4))  # Crea 3 sottotrame affiancate\naxs = axs.flatten()  # Appiattisce l'array di assi per facilitare l'accesso\n\n# Ciclo sui paesi per plottare i dati di ciascuno\nfor n, country in enumerate(countries):\n    # Selezioniamo i dati del paese corrente per PNL e PIL\n    gni_pc_country = gni_pc[gni_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PNL\n    gdp_pc_country = gdp_pc[gdp_pc['COUNTRY'] == country].T[1:]  # Trasposta dei dati di PIL\n    \n    # Plottiamo i dati sulla stessa sottotrama\n    axs[n].plot(gni_pc_country, label='PNL pro capite')  # PNL\n    axs[n].plot(gdp_pc_country, label='PIL pro capite')  # PIL\n    \n    # Aggiungiamo il titolo e le etichette\n    axs[n].set_title(country, loc='left')  # Titolo a sinistra per ogni paese\n    axs[n].set_xlabel('Anno')  # Etichetta asse X\n    axs[n].set_ylabel('Valore')  # Etichetta asse Y\n\n    # Aggiungiamo la legenda\n    axs[n].legend(\n        loc='upper left',  # Posizione della legenda\n        frameon=True,  # Abilita il frame della legenda\n        fontsize='x-small'  # Font di dimensioni piccole\n    )\n\n# Titolo principale del grafico\nfig.suptitle('PIL e PNL pro capite in Italia, Germania, Irlanda')\n\n# Miglioriamo il layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Mostriamo il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-disoccupazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di disoccupazione",
    "text": "Tasso di disoccupazione\nIl tasso di disoccupazione si calcola come rapporto tra disoccupati (ovvero coloro i quali non hanno un lavoro, ma lo stanno cercando) e la forza lavoro (data dalla somma di occupati e disoccupati). A questo proposito, è importante sottolineare come chi non ha un lavoro, ma non ne sta nemmeno cercando uno, sia considerato fuori dalla forza lavoro, e dunque non incluso nel conteggio dei disoccupati. Si può quindi calcolare il tasso di partecipazione come forza lavoro divisa per il totale della popolazione.\n\nCome si misura il tasso di disoccupazione?\nGeneralmente, si effettuano indagini a campione sulle famiglie (Labour Force Surveys)\n\n# Chiave per il tasso di disoccupazione\nunemp_rate_key = 'LFSI/Q.I9.S.UNEHRT.TOTAL0.15_74.T'  # Codice identificativo del tasso di disoccupazione\nunemp_rate = make_df_ECB(unemp_rate_key, 'Unemployment Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo un grafico con due assi\nfig, ax1 = plt.subplots()  # Crea una figura e un asse\n\n# Plot del tasso di disoccupazione\nax1.plot(unemp_rate)  # Traccia il grafico del tasso di disoccupazione\nax1.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax1.set_ylabel('Tasso di Disoccupazione', color=bmh_colors[0])  # Etichetta per l'asse Y con colore personalizzato\n\n# Creiamo un secondo asse per la crescita del PIL\nax2 = ax1.twinx()  # Crea un secondo asse Y condiviso sull'asse X\nax2.plot(real_gdp_growth[real_gdp_growth.index &gt;= unemp_rate.index.min()], color='#525252', linestyle=':')  # Traccia il grafico della crescita del PIL con linea tratteggiata\nax2.set_ylabel('Crescita del PIL')  # Etichetta per l'asse Y della crescita del PIL\nax2.grid(False)  # Disabilita la griglia per il secondo asse\n\n# Titolo del grafico\nplt.title('Disoccupazione e crescita del PIL nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  },
  {
    "objectID": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "href": "notebooks/Le variabili macroeconomiche.html#tasso-di-inflazione",
    "title": "Le variabili macroeconomiche",
    "section": "Tasso di Inflazione",
    "text": "Tasso di Inflazione\nL’inflazione rappresenta un aumento sostenuto del livello generale dei prezzi. Il tasso di inflazione è il tasso a cui il livello dei prezzi aumenta nel tempo.\n\nCome si misura l’inflazione?\nIl livello dei prezzi si può calcolare in due modi:\n\n\nUtilizzando il deflatore del PIL, ovvero il rapporto tra il PIL nominale e il PIL reale nell’anno di interesse\n\n\nLindice armonizzato dei prezzi al consumo, ovvero un numero indice che fissa un prezzo pari a 100 nell’anno base\n\n\nChiamando \\(P_t\\) il livello generale dei prezzi nell’anno \\(t\\), segue che il tasso di inflazione \\(\\pi\\) viene definito come \\(\\pi =\n\\frac{P_t - P_{t-1}}{P_{t-1}} \\approx \\ln P_t - \\ln P_{t-1}\\)\n\n# Chiave per l'Inflation Rate\nHICP_key = 'ICP/M.U2.N.000000.4.ANR'  # Codice identificativo dell'Inflation Rate\nHICP = make_df_ECB(HICP_key, 'Inflation Rate')  # Ottieni i dati dal database ECB\n\n# Creiamo il grafico\nfig, ax = plt.subplots()  # Crea una figura e un asse\n\n# Traccia l'inflazione\nax.plot(HICP)  # Traccia il grafico dell'inflazione\n\n# Imposta etichetta per l'asse X e Y\nax.set_xlabel('Trimestre')  # Etichetta per l'asse X\nax.set_ylabel('Tasso di Inflazione')  # Etichetta per l'asse Y\n\n# Titolo del grafico\nplt.title('Tasso di Inflazione nell\\'Area Euro', loc='left')  # Aggiungi il titolo a sinistra\n\n# Ottimizza il layout per evitare sovrapposizioni\nfig.tight_layout()\n\n# Mostra il grafico\nplt.show()"
  }
]